---
output:
  pdf_document: default
  html_document: default
---
# Working Text

```{r, include=FALSE}
library(tidyverse)
library(kableExtra)
library(broom)
library(ggrepel)
knitr::opts_chunk$set(echo = TRUE, fig.width=5, fig.height=3, fig.align = "center",
                      message=FALSE, warning=FALSE)
doc_theme <- ggplot2::theme_bw()
```

### Analytical Optimization

In the case of finding the line that minimizes the RMS of the residuals, we have an easy analytical solution. We don't need a grid search and we don't need to optimize numerically.

#### Scalar Form

Remember that we simply need to minimize the function 

$f(\alpha, \beta) = \displaystyle \sqrt{\frac{\sum_{i = 1}^n [y_i - (\alpha + \beta x_i)]^2}{n}}$.

This is equalivent to minimizing $h(\alpha, \beta) = \sum_{i = 1}^n(y_i - \alpha - \beta x_i)^2$. We sometimes refer to this quantity as the SSR or "sum of squared residuals."

To minimize $h(\alpha, \beta)$, remember that we need to solve for $\frac{\partial h}{\partial \alpha} = 0$ and $\frac{\partial h}{\partial \beta} = 0$ (i.e., the first-order conditions).

Using the chain rule, we have the partial derivatives

$\frac{\partial h}{\partial \alpha} = \sum_{i = 1}^n [2 \times (y_i - \alpha - \beta x_i) \times (-1)] = -2 \sum_{i = 1}^n(y_i - \alpha + \beta x_i)$

and

$\frac{\partial h}{\partial \beta} = \sum_{i = 1}^n 2 \times (y_i - \alpha - \beta x_i) \times (-x_i) = -2 \sum_{i = 1}^n(y_i - \alpha - \beta x_i)x_i$

and the two first-order conditions

$-2 \sum_{i = 1}^n(y_i - \hat{\alpha} + \hat{\beta} x_i) = 0$

and

$-2 \sum_{i = 1}^n(y_i - \hat{\alpha} + \hat{\beta} x_i)x_i = 0$

##### The 1st First-Order Condition

\begin{align}
-2 \sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i) &= 0 \\
\sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i) &= 0 \text{   (divide both sizes by $-2$)} \\
\sum_{i = 1}^n y_i - \sum_{i = 1}^n \hat{\alpha}  - \sum_{i = 1}^n \hat{\beta} x_i &= 0 \text{   (distribute the sum)} \\
\sum_{i = 1}^n y_i -  n \hat{\alpha}  - \hat{\beta}\sum_{i = 1}^n  x_i &= 0 \text{   (move constant $\beta$ in front and realize that $\sum_{i = 1}^n \hat{\alpha} = n\hat{\alpha}$)} \\
\sum_{i = 1}^n y_i & = n \hat{\alpha}  + \hat{\beta}\sum_{i = 1}^n  x_i \text{   (rearrange)} \\
\frac{\sum_{i = 1}^n y_i}{n} & = \hat{\alpha}  + \hat{\beta} \frac{\sum_{i = 1}^n  x_i}{n} \text{   (divide both sides by $n$)} \\
\overline{y} & = \hat{\alpha}  + \hat{\beta} \overline{x} \text{   (recognize the average of $y$ and of $x$)} \\
\end{align}

```{theorem, label = "pt-of-avgs"}
The 1st first-order condition implies that the regression line $\hat{y} = \hat{\alpha} + \hat{\beta}x$ equals $\overline{y}$ when $x = \overline{x}$. Thus, the regression line must go through the point $(\overline{x}, \overline{y})$ or "the point of averages".  
```

The figure below shows a regression line that goes through the point of averages.

```{r echo = FALSE}
set.seed(1234)
x <- rnorm(10)
y <- -x + rnorm(10)
df <- tibble(x, y)

fit <- lm(y ~ x, data = df)
block_df <- tibble(x = seq(min(df$x), max(df$x), by = 0.1)) %>%
  mutate(y = predict(fit, newdata = .)) %>%
  bind_rows(crossing(x = seq(min(df$x), max(df$x), by = 0.1), y = mean(y))) %>%
  bind_rows(crossing(x = mean(x), y = seq(min(df$y), max(df$y), by = 0.1))) %>%
  mutate(label = "") 

ann_df <- tribble(
  ~label, ~x, ~y,
  "average of x",   mean(x), -1.5,
  "average of y",   -1.5, mean(y),
  "point of averages",   mean(x), mean(y),
  "regression line", -1.5, sum(coef(fit)*c(1, -1.5))
) %>% 
  bind_rows(block_df) %>%
  bind_rows(df %>% mutate(label = ""))

library(ggrepel)
ggplot(df, aes(x, y)) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 1, color = 1) +
  geom_vline(xintercept = mean(x), linetype = 3) + 
  geom_hline(yintercept = mean(y), linetype = 3) + 
  geom_label_repel(data = ann_df, aes(x = x, y = y, label = label), 
                   box.padding = 1,  
                   segment.color = "grey70") + 
  geom_point() + 
  theme_minimal() + 
  geom_point(x = mean(x), y = mean(y), 
             shape = 21, fill = "white", 
             size = 3)
```

```{lemma, label = "1st-order"}
We can rearrange the identity $\hat{y} = \hat{\alpha} + \hat{\beta}x$ from Theorem \@ref(thm:pt-of-avgs) to obtain the identity $\hat{\alpha} = \overline{y} - \hat{\beta}\overline{x}$
```

**A personal perspective on proofs:** In my experience, proofs are not intuitive. Sometimes I have a sneaking suspicion about a result, but sometimes that sneaking suspicion is *wildly* wrong. When I investigate a the suspicion analytically, the path to the result is unclear. A maze analogy works quite well. To obtain the result, just move things around, sometimes getting further from the result. Eventually, you just happen upon the correct sequence of movements. 

##### The 2nd First-Order Condition

\begin{align}
-2 \sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i)x_i &= 0 \\
\sum_{i = 1}^n(y_i x_i - \hat{\alpha}x_i - \hat{\beta} x_i^2) &= 0 \text{   (distribute the $x_i$)} \\
\sum_{i = 1}^n(y_i x_i - (\overline{y} - \hat{\beta}\overline{x})x_i - \hat{\beta} x_i^2) &= 0 \text{   (use the identity $\hat{\alpha} = \overline{y} - \hat{\beta}\overline{x}$)} \\
\sum_{i = 1}^n( x_i y_i - \overline{y} x_i + \hat{\beta}\overline{x} x_i - \hat{\beta} x_i^2) &= 0 \text{   (expand the middle term)} \\
\sum_{i = 1}^n x_i y_i  - \overline{y} \sum_{i = 1}^n x_i + \hat{\beta}\overline{x} \sum_{i = 1}^n x_i - \hat{\beta} \sum_{i = 1}^n x_i^2 &= 0 \text{   (distribute the sum)} \\
\sum_{i = 1}^n  x_i y_i - n \overline{y} \overline{x} + \hat{\beta}n \overline{x}^2 - \hat{\beta} \sum_{i = 1}^n x_i^2 &= 0 \text{   ($\overline{y} = \frac{\sum_{i = i}^n y_i}{n}$, so $\sum_{i = i}^n y_i = n\overline{y}$)}\\
\sum_{i = 1}^n x_i y_i  - n \overline{x} \overline{y}  &= \hat{\beta} \left(\sum_{i = 1}^n x_i^2 - n \overline{x}^2 \right) \text{   (rearrange)}\\
\hat{\beta} &=\dfrac{\sum_{i = 1}^n x_i y_i - n \overline{x} \overline{y}}{\sum_{i = 1}^n x_i^2 - n \overline{x}^2} \text{   (rearrange)}\\
\end{align}

This final result is correct, but unfamiliar. In order to make sense of this solution, we need to connect this identity to previous results.

```{lemma, label = "sum-of-dev-prods"}
$\sum_{i = 1}^n x_i y_i - n \overline{x} \overline{y} = \sum_{i = 1}^n (x_i = \overline{x})(y_i = \overline{y})$.
```
```{exercise}
Prove Lemma \@ref(lem:sum-of-dev-prods).
```
```{lemma, label = "sum-of-squared-devs"}
$\sum_{i = 1}^n x_i ^2 - n \overline{x}^2 = \sum_{i = 1}^n (x_i = \overline{x})^2$.
```
```{exercise}
Prove Lemma \@ref(lem:sum-of-squared-devs).
```





