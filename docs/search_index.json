[
["index.html", "Concepts and Computation: An Introduction to Political Methodology Chapter 1 Overview", " Concepts and Computation: An Introduction to Political Methodology Carlisle Rainey 2019-10-16 Chapter 1 Overview "],
["location-and-scale.html", "Chapter 2 Location and Scale 2.1 The Intuition 2.2 The Usual Measures 2.3 Robust Alternatives 2.4 Computation in R", " Chapter 2 Location and Scale 2.1 The Intuition If we took a histogram and tried to describe it to someone else without showing it to them, the most most important pieces of information are usually the location and scale.1 We might describe the variable this way: “The values are about __________, give or take ________ or so.” We can think of the first blank as the location and the second blank as the scale. The location describes where the histogram is positioned along the left-right axis. The scale describes the width (or “spread” or “dispersion”) of the histogram. Inspect the histogram of a hypothethical variable to the right. Notice the location and the scale. If we had to describe these data, we might say that our variable is “about zero give or take one or so.” While this variable has a particular location (about zero), we can imagine shifting it left or right. The figure below shows some possible shifts. We could shift it way to the left, so that it’s “about -6” or a little bit to the right so that it’s “about two.” We can also imagine increasing the scale (more spread) or decreasing the scale (less spread). The figure below shows some possible changes in scale. In each case, the “give or take” number is changing. 2.2 The Usual Measures 2.2.1 The Average The most common measure of the location of a variable is the average.2 Suppose we have a variable (a list of numbers) \\(X = \\{x_1, x_2, ..., x_n\\}\\). \\[\\begin{equation} \\text{average} = \\dfrac{\\text{the sum of the list}}{\\text{the number of entries in the list}} = \\dfrac{\\sum_{i = 1}^n x_i}{n} \\nonumber \\end{equation}\\] The average is easy to compute and easy to work with mathematically.3 Unfortunately, the average doesn’t have an easy interpretation. The best interpretation, in my mind, is as the balance-point for the data. If we imagine the left-right axis as a teeter-totter and stack the data along the beam according to their values, then the average is the position of the fulcrum that would balance the data-filled beam. 2.2.2 The Standard Deviation The most common measure of scale is the standard deviation (SD). The intuition is subtle, so let’s look a a simple example. Rember, our goal is a \"give-or-take number. Suppose we have a list of numbers \\(X = \\{1, 2, 3, 4, 5\\}\\). The average of this list is 3, so we can compute the deviation from average for each value. \\[\\begin{equation} \\text{deviation from average} = d = \\text{value} - \\text{average} \\nonumber \\end{equation}\\] In this case, \\(d = \\{-2, -1, 0, 1, 2\\}\\). We want to use these deviations to find a give-or-take number. Here’s an initial idea. Just take the absolute values \\(|d| = \\{2, 1, 0, 1, 2\\}\\). These tell us how far each entry falls away from the average. Then we could average the absolute devations to find how far a typical entries falls away from the average of the list. In this case, we get 1.2. This is reasonable approach and we’ll refer to it as the average absolute devation or a.a.d. (It turns out that the a.a.d. isn’t a common quantity, so I don’t elevate it with an all-caps acronym.) The a.a.d. has one big problem–it uses an absolute value. This introduces some computational and mathematical difficulties.4 So let’s do something similar. Rather than take the absolute value, let’s square the deviations, take the average, and then undo the square at the end, so that \\(\\text{SD} = \\sqrt{\\text{avg}(d^2)}\\). Sometimes taking the (3) square root of (2) the average of (1) the squares is called the RMS. In this case, the RMS of the deviations from the average is the SD, so that \\[\\begin{equation} \\text{SD} = \\sqrt{\\text{avg}(d^2)} = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}} = \\text{RMS of deviations from average}. \\nonumber \\end{equation}\\] The SD moves smoothly as you move around the entries in the list. To calculate the SD, first make this little table, with the list of values, the deviations from the average, and the squares of the deviations. \\(X\\) \\(d\\) \\(d^2\\) 1 -2 4 2 -1 1 3 0 0 4 1 1 5 2 4 Then compute the average of the squares of the deviations, which in this case is 2. Then take the square root of that average, which in this case is about 1.4. Notice that 1.4 is about 1.2 (the a.a.d.). The SD is bounded (weakly) below by the a.a.s., but they’ll usually be close, so we can think of the SD as how far a typical point falls away from the average. 2.3 Robust Alternatives The average and the SD are mathematically nice. But they are not robust. Seemingly innocuous changes in the variable can lead to large changes in the average and SD.5 We can definite robustness more concretely: How many observations do I need to corrupt to make the summary arbitrarily large? Suppose the toy variable \\(X = \\{0.1, -0.6, 1.1, 1.3, 0.2\\}\\). If I replace the first entry (0.1) with 1, 5, 10, 50, and so on, what happens to the average and SD? The table below shows that we can easily manipulate the average and SD by changing only one data point. In this sense, the average and SD are fragile. Summary Average SD Actual Data Set 0.42 0.70 First entry of \\(X\\) replaced with 1 0.60 0.71 …with 5 1.40 1.92 …with 10 2.40 3.86 …with 50 10.40 19.81 …with 100 20.40 39.81 …with 500 100.40 199.80 …with 1,000 200.40 399.80 If corrupted data present a problem, then what do we mean by “corrupt”? There are (at least) three ways to imagine corrupting a measurement. First, perhaps we have a data entry error. While entering data in a spreadsheet, you entered the number 50,000 into the “hours spent watching the news per day” variable instead of the “income” variable. Second, perhaps our measurement procedure is noisy. Suppose we are coding Twitter posts by their support or opposition to President Trump. Our algorithm might interpret a sarcastic take as support when it actually presented intense opposition. Third, the substantive model might not apply to a particular observation. Take Clark and Golder’s project as an eplxame. They suggest that SMD systems should only have two parties. Indeed, this is a strong theoretical equilibirum. However, it might take several elections to reach this equilibrium. Parties might take several years to coordinate and consolodate. If we include a new democracy in the data set, then we might consider these data “corrupted” since the conceptual model doesn’t apply (yet). The average and SD respond to even a small amount of corrupt data. As an alternative to the average, we might use the median, which is more robust. The median is the/a number which splits the values in half, so that equal numbers of entries lie above and below the median. We have two common robust alternatives to the SD. The interquartile range (IQR) is the difference between the 25th and 75th quantiles. The median absolute deviation (MAD) is the median of the absolute values of the deviations from the median (almost the a.a.d., but using the medians in place of averages). It turns out that multiplying the MAD by 1.4826 makes it similar to the SD in many dataset, so it’s common to rescale it. To illustrate the robustness of each of our measures of location and scale, let’s imagine a variable with 10 observations \\(X = \\{-1.1, 1.5, -1, -0.1, -1.1, 0, -0.4, 0, 0.8, 0.4\\}\\). Let’s see how the measures change as we corrupt more and more of the observations. Summary % Corrupted Average SD Median IQR MAD Actual Data Set 0% -0.10 0.81 -0.05 1.15 0.96 First entry of \\(X\\) replaced with 100 10% 10.01 30.01 0.00 1.03 0.89 First two entries… 20% 19.86 40.07 0.00 1.03 0.89 First three entries… 30% 29.96 45.85 0.20 75.28 0.89 First four entries… 40% 39.97 49.02 0.60 100.00 2.00 First five entries… 50% 50.08 49.92 50.40 99.90 73.54 First six entries… 60% 60.08 48.89 100.00 99.50 0.00 This table illustrates that while the average and SD respond to any corruption, the median, IQR, and MAD remain reasonable summaries of the uncorrupted variable with 40%, 20%, and 30% of the data corrupted, respectively. T The percent of the data that one can corrupt before they can make the measure arbitrarily large is called the breakdown point. Here are the breakdown points for our measures: Measure Breakdown Point Average 0% SD 0% Median 50% IQR 25% MAD 50% As you can see, the median and the MAD are highly robust–they achieve the theoretical maximum breakdown point. 2.4 Computation in R We can easily calculate all these measures of location and scale in R.6 # create variable x = {1, 2, 3, 4, 5} x &lt;- 1:5 # compute measures of location and scale mean(x) # average ## [1] 3 sd(x) # SD; see sidenote ## [1] 1.581139 median(x) # median ## [1] 3 IQR(x) # IQR ## [1] 2 mad(x) # MAD, rescaled by 1.4826 ## [1] 1.4826 mad(x, constant = 1) # MAD, not rescaled ## [1] 1 The functions above work nicely for computing on whole variables. But in most cases, we are interested in comparing the summaries across groups. Take the nominate data set for example. # load packages library(tidyverse) # load nominate data df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% glimpse() ## Observations: 7,080 ## Variables: 7 ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;… ## $ district &lt;int&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, … ## $ party &lt;fct&gt; Republican, Democrat, Democrat, Democrat, Democrat, Rep… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, Wi… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0… For these data, we might want to know the average ideology for Republicans and Democrats. We could do it the hard way. # create a data frame with only republicans rep_df &lt;- df %&gt;% filter(party == &quot;Republican&quot;) # compute average mean(rep_df$ideology, na.rm = TRUE) ## [1] 0.4213385 But this is tedious, especially if we wanted to do it by party and Congress. To compute these summaries for lots of subsets of the data, we have the group_by()/summarize() workflow. group_by() defines several groups in the data frame. The first argument is the data frame to group (but we’ll %&gt;% it in). The remaining arguments are the grouping variables. You can think if the groups as a footnote at the bottom of the data set that just mentions the variables that define the groups of interest. Whenever we act (in the wrangling sense) on the data set and the action makes sense in the context of groups, the action will happen by group. After grouping, we use summarize() to create summaries for each group. The first argument is the data frame to summarize (but we’ll %&gt;% it in). The remaining arguments are the summarizes to compute. The names of the remaining arguments become variables in the resulting data frame. smry_df &lt;- df %&gt;% # group by party and congress group_by(party, congress) %&gt;% # compute all of our measures of location and scale summarize(average = mean(ideology, na.rm = TRUE), sd = sd(ideology, na.rm = TRUE), median = median(ideology, na.rm = TRUE), iqr = IQR(ideology, na.rm = TRUE), mad = mad(ideology, na.rm = TRUE), mad1 = mad(ideology, constant = 1, na.rm = TRUE)) %&gt;% # quick look at our work glimpse() ## Observations: 32 ## Variables: 8 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Democ… ## $ congress &lt;int&gt; 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, … ## $ average &lt;dbl&gt; -0.3092901, -0.3130075, -0.3142407, -0.3333065, -0.3615… ## $ sd &lt;dbl&gt; 0.1653092, 0.1664293, 0.1658089, 0.1609726, 0.1524251, … ## $ median &lt;dbl&gt; -0.3200, -0.3200, -0.3200, -0.3360, -0.3815, -0.3835, -… ## $ iqr &lt;dbl&gt; 0.22750, 0.22600, 0.23025, 0.24000, 0.20550, 0.17850, 0… ## $ mad &lt;dbl&gt; 0.1675338, 0.1719816, 0.1690164, 0.1793946, 0.1490013, … ## $ mad1 &lt;dbl&gt; 0.1130, 0.1160, 0.1140, 0.1210, 0.1005, 0.0895, 0.0870,… We can plot this measures to get a sense of how they change over time. Notice that mad (rescaled by multiplying by 1.4826) closely corresponds to the SD, but mad1 (not rescaled) is much smaller. # wrangle the data for plotting gg_df &lt;- smry_df %&gt;% pivot_longer(average:mad1, names_to = &quot;measure&quot;) %&gt;% mutate(measure_of = ifelse(measure %in% c(&quot;average&quot;, &quot;median&quot;), &quot;location&quot;, &quot;scale&quot;)) %&gt;% glimpse() ## Observations: 192 ## Variables: 5 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Dem… ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101… ## $ measure &lt;chr&gt; &quot;average&quot;, &quot;sd&quot;, &quot;median&quot;, &quot;iqr&quot;, &quot;mad&quot;, &quot;mad1&quot;, &quot;ave… ## $ value &lt;dbl&gt; -0.3092901, 0.1653092, -0.3200000, 0.2275000, 0.16753… ## $ measure_of &lt;chr&gt; &quot;location&quot;, &quot;scale&quot;, &quot;location&quot;, &quot;scale&quot;, &quot;scale&quot;, &quot;s… # plot the measures of location and scale ggplot(gg_df, aes(x = congress, y = value, color = measure)) + geom_line() + facet_grid(cols = vars(party), rows = vars(measure_of), scales = &quot;free_y&quot;) I use these terms intentionally. Later, when we discuss random variables, the terms “location” and “scale” will return (with similar meanings). Indeed, we paramaterize many distributions according to their location and scale. For example, the normal distribution has a location parameter \\(\\mu\\) and a scale parameter \\(\\sigma\\).↩ Some people refer to the “average” as the “mean”. I prefer to avoid this because the “mean” might also refer to the expectation of a random variable. I use “average” and “expected value” to differentiate these two meanings.↩ The median, alternatively, is not easy to compute and quite difficult to work with mathematically.↩ Here’s the gist: If you take an entry and slide it up and down (i.e., make it larger or smaller), then the a.a.d. moves up and down as well. This is fine, except the a.a.s. doesn’t respond smoothly. The figure to the right shows what happens as we move the first entry on the list above around–notice the kink! The derivative of the a.a.d. isn’t define here (i.e., there are lots of tangents). This makes things hard mathematically.↩ The mathmatical ease and the substantive fragility are related.↩ For reasons I don’t want to deal with now, R uses the formula \\(SD = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n - 1}}\\) rather than \\(\\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}}\\). This means that R’s SD will be slightly larger than the SD with my formula. This difference will be tiny in data sets with a typical number of observations.↩ "],
["the-normal-model.html", "Chapter 3 The Normal Model 3.1 The Intuition 3.2 The Normal Curve(s) 3.3 The Empirical Rule 3.4 The Normal Approximation 3.5 Review Exercises", " Chapter 3 The Normal Model 3.1 The Intuition Last week, we used the average and SD to reduce and entire variable to two summaries. We use the average and SD to fill in the following sentence: “The values are about ________, give or take ________ or so.” This week, we add an additional assumption. This week, we also say that the histogram of the variable follows the normal curve. The normal curve is a bell-shaped curve with a particular equation. There are two varieties. There is a general, parameterized normal distibution that can move left and right (i.e., change location) and grow wider or taller (i.e., change scale) 3.2 The Normal Curve(s) There are two particular normal curves that we care about the normal curve, which has a location and scale parameter that we can specify: \\(f(x | \\mu, \\sigma) = \\phi(x | \\mu, \\sigma) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^\\frac{{ - \\left( {x - \\mu } \\right)^2 }}{2\\sigma ^2 }\\) the standard normal curve, with the location and scale parameters fixed: \\(f(x | \\mu = 0, \\sigma = 1) = \\phi(x | \\mu = 0, \\sigma = 1) = \\frac{1}{{\\sqrt {2\\pi } }}e^\\frac{{ - x ^2 }}{2}\\) These equations are complicated. Instead of memorizing them or working carefully through the math, just understand (for now) that the normal curve has an equation that exactly characterizes it. The figure below shows the standard normal curve (\\(\\mu = 0\\) and \\(\\sigma = 1\\)) and several other paramaterizations. 3.3 The Empirical Rule It turns out that many variable’s have a histogram that resembles the normal curve. Because of this, the normal curve can sometimes serve as an effective model for these variables. For example, NOMINATE ideology scores for Republicans in the 115th Congress roughly follow the normal curve. df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% filter(party == &quot;Democrat&quot;, congress == 115) ggplot(df, aes(x = ideology)) + geom_histogram() However, the ideology scores for both Republicans and Democrats together does not follow a normal curve. df &lt;- read_rds(&quot;data/nominate.rds&quot;) ggplot(df, aes(x = ideology)) + geom_histogram() The histograms of ENEP by electoral system and social heterogeneity deviate slightly from the normal curve. ## Observations: 1,161 ## Variables: 4 ## $ x &lt;dbl&gt; 1.23, 1.33, 1.43, 1.53, 1.63, 1.73, 1.83, 1… ## $ density &lt;dbl&gt; 0.02616495, 0.02960818, 0.03338541, 0.03751… ## $ electoral_system &lt;fct&gt; Large-Magnitude PR, Large-Magnitude PR, Lar… ## $ social_heterogeneity &lt;fct&gt; Bottom 3rd of ENEG, Bottom 3rd of ENEG, Bot… If the variable seems to follow the normal curve, then we have the following rules: About 68% of the data (i.e., “most”) fall within 1 SD of the average. About 95% of the data (i.e., “almost all”) fall within 2 SDs of the average. We can evaluate this rule with the parties data above. Some of the nine hisgrams follow the normal curve quite well (e.g., lower-left). Other’s seem to meaningfully deviate from the normal curve (e.g., middle-left). The table below shows the actual percent of the variable that falls within one and two SDs of the average for each histogram. As you can see, for the lower-left panel (SMD, Top 3rd), the empircal rule of 68% and 95% matches the actual values of 74% and 98% fairly well. For the middle-left panel (SMD, Middle 3rd), the empirical rule matches the actual values of 87% and 93% less well. Across all histograms, it seems fair that the empirical rule works as a rough approximation, even for histograms that meaningfully deviate from the normal curve. Electoral System Social Heterogeneity within 1 SD within 2 SDs Single-Member District Bottom 3rd of ENEG 87% 96% Single-Member District Middle 3rd of ENEG 87% 93% Single-Member District Top 3rd of ENEG 74% 98% Small-Magnitude PR Bottom 3rd of ENEG 68% 97% Small-Magnitude PR Middle 3rd of ENEG 73% 96% Small-Magnitude PR Top 3rd of ENEG 76% 93% Large-Magnitude PR Bottom 3rd of ENEG 80% 98% Large-Magnitude PR Middle 3rd of ENEG 77% 96% Large-Magnitude PR Top 3rd of ENEG 65% 97% 3.4 The Normal Approximation If our normal model summarizes a histogram well, then we can use the model to estimate the percent of the observations that fall in a given range. There are two approaches: Just like we add up the area of the bars to compute percentages with a histogram, we add up the area under the normal curve to approximate percentages. Use a normal table from a textbook. Because the table is for the standard normal curve, we need to re-locate and re-scale the data to fit the standard normal curve. Use the pnorm() function in R. Because this function is parameterized with location and scale, we can simple re-locate and re-scale the curve to fit the data. 3.4.1 Normal Table Normal tables offer an antiquated method to use the normal distribution to approximate percentages. Because we cannot have a normal table for all possible locations and scales, we have one: the standard normal table, which works for a variable with an average of zero and an SD of one. This seems limiting, but it turns out that we can easily re-locate and re-scale any value to match the standard normal curve. We simply subtract the average and divide by the SD. We call this new value a z-score. \\(z\\text{-score} = \\dfrac{\\text{value} - \\text{average}}{\\text{SD}}\\) FSuppose we have the list \\(X = \\{1, 2, 3, 4, 5\\}\\). Then the average is 3, and the SD is about 1.26. We can compute the zscore for the first entry 1 as \\(\\frac{1 - 3}{1.25} \\approx -1.58\\). Similarly, we can convert the entire list to z-scores and get \\(Z = \\{1.59, -0.79, 0.00, 0.79, 1.59\\}\\). If you compute the average and SD of the list \\(Z\\), you will find zero and one, respectively. We can then use a normal table to compute areas under the normal curve between (or above or below) these values of \\(z\\). There are two types of normal tables. Some tables report the percent (or proportion) of the normal curve below a particular value \\(z\\). Other tables report the percent (or proportion) of the normal curve between a particular value \\(z\\) and \\(-z\\). (The normal table on p. A-104 of FPP works this way.) Either table works, but you must know what type of table you are working with. Depending on the question, one type might offer a more direct solution. Here’s a small normal table for a few values of \\(z\\) that uses both approaches. z % less than z % between -z and z Status 0.00 50% 0% 0.10 54% 8% 0.20 58% 16% 0.30 62% 24% 0.40 66% 31% 0.50 69% 38% 0.75 77% 55% 1.00 84% 68% Important 1.50 93% 87% 1.64 95% 90% Important 1.96 98% 95% Important 2.00 98% 95% Important 3.00 100% 100% In order to use the table to find the area between any two values, you need to use the following three rules in combination. The normal table gives the area (i) below \\(z\\) or (ii) between \\(-z\\) and \\(z\\). The area under the entire normal curve is 1 or 100%. The normal curve is symetric, so that the area to the right of \\(z\\) equals the area to the left of \\(-z\\). 3.4.2 pnorm() The pnorm() function in R return the area under the normal curve less than \\(z\\). By default, it uses the standard normal curve, but you can specify a mean and sd if you prefer to re-locate and/or re-scale the curve to fit your values. # area under the std. normal curve less than 1 pnorm(1) ## [1] 0.8413447 # area under the a normal curve (with average of 1 and SD of 4) less than 1 pnorm(1, mean = 1, sd = 4) ## [1] 0.5 # area between -1.64 and 1.64 pnorm(1.64) - pnorm(-1.64) ## [1] 0.8989948 3.4.3 Exactly Percentages To actually compute percentages, we can create a function that works just like pnorm(), but it returns the percent of the data that fall below a particular value. The most convenient method is to create an \"empirical cumulative distribution function*. This function is somewhat confusing. The ecdf() function does not return the proportion below its argument. Instead, it creates a function that returns the percent below its argument. If we have a numeric vector x, then ecdf(x) is a function! Let that settle in… both ecdf and ecdf(x) are function. The function ecdf (I’m dropping the () for clarity) is a function that creates a function, and ecdf(x)() (I’m including the (), as usual, for clarity) is a function that returns the percent below. df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% filter(party == &quot;Democrat&quot;, congress == 115) # normal approximation for % of Democrats less than -0.05 avg &lt;- mean(df$ideology) sd &lt;- sd(df$ideology) pnorm(-0.5, mean = avg, sd = sd) ## [1] 0.1731597 # exact % of Democrats less than -0.05 ecdf(df$ideology)(-0.5) ## [1] 0.1479592 We can also plot the ECDF with ggplot2. 3.5 Review Exercises The plot below show the histograms for the ideology of legislators in the U.S. House by party. We can compute the average and SD by party. Party Average SD Democrat -0.39 0.12 Republican 0.49 0.15 The table below lists some of the leaders of each party and their ideology score. For each leader, use our three approaches to compute the percent of the party that is “more extreme” than their leader: inspect the histogram, use the normal approximation, and use R to compute the answer exactly. Name Party Position Ideology Score Inspect Histogram Normal Approximation Actual RYAN, Paul D. Republican Speaker of the House 0.56 MCCARTHY, Kevin Republican Majority Leader 0.46 SCALISE, Steve Republican Majority Whip 0.56 McMORRIS RODGERS, Cathy Republican Conference Chair 0.43 PELOSI, Nancy Democrat Minority Leader -0.49 HOYER, Steny Hamilton Democrat Minority Whip -0.38 CLYBURN, James Enos Democrat Assistant Democratic Leader -0.46 LEWIS, John R. Democrat Senior Chief Deputy Minority Whip -0.59 "],
["the-x-y-space.html", "Chapter 4 The X-Y Space 4.1 Points 4.2 Lines", " Chapter 4 The X-Y Space 4.1 Points The scatterplot has two key aesthetics: the horizontal and vertical location of points (and lines). We refer to the horizontal location as “x” and the vertical location as “y.” We sometimes refer to this two-dimmensional space (of horizontal-vertical or x-y locations) as the “Cartisian coordinate system.” The table below contains five observations. Each observation has values for variables x and y. (In the context of a data analysis, we typically think of x as the key explanatory variable and y as the outcome varaible.) Observation x y #1 1 1 #2 4 4 #3 -4 -3 #4 2 -2 #5 -2 4 The plot below shows the location of each point in the x-y space. Exercise 4.1 Recreate the x-y space below on a sheet of paper. Add the following points (0, 0), (1, 2), (-3, 4), (2, -3), and (-4, -2). Hint The first number in the (x, y) pair represents the “x” or horizontal location. The second number represents the “y” or vertical location. The location of the second point is… Solution 4.2 Lines We can also draw lines in the x-y space. Remember, the formula for a line is \\(y = mx + b\\). Here, \\(y\\) and \\(x\\) represent variables (i.e., locations in the x-y space), \\(m\\) represents the slope of the line, and \\(b\\) represents the intercept. Consider the following four examples: Example Equation Intercept Slope Example 1 \\(y = x + 0\\) 0 1.0 Example 2 \\(y = -2x + 2\\) 2 -2.0 Example 3 \\(y = 0.5x - 1\\) -1 0.5 Example 4 \\(y = 3x - 2\\) -2 3.0 4.2.1 The Intercept The intercept \\(b\\) tells us where the line crosses the vertical slice of the space where \\(x = 0\\). Exercise 4.2 For the lines below, identify the intercept visually, if possible. Hint All the lines look like 45-degree lines (or 315-degree) because the ranges of the axes are rescaled. This makes the problem a little trickier. For each line, identify where the line cross the slide of the space where \\(x = 0\\). When \\(x = 0\\), then \\(y = m \\times 0 + b = b\\). Remember that \\(b\\) represents the intercept. Solution The intercepts are 0, -2, 1, 1.5. You cannot see the third intercept visually, because the slice of the space where \\(x = 0\\) is not included in the plot. 4.2.2 Slope The slope \\(m\\) tells us how fast the line rises or falls as we move from left-to-right. If \\(m\\) is positive, then the line rises. If \\(m\\) is negative, then the line falls. If \\(m\\) is zero, then the line neither rises nor falls (stays constant at the same height). As \\(m\\) gets larger in magnitude, the line rises or falls faster. The best way to think about slope is as the “rise over run.” \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Take Example 2 from the table above or \\(y = -2x + 2\\). Consider two scenaarios: one where \\(x = 0\\) and another where \\(x = 3\\). When \\(x = 0\\), we know that \\(y = 2\\) because the intercept \\(b\\) equals 2. When \\(x = 3\\), we have “run” 3 units to the right (i.e., \\(\\text{run} = \\text{2nd value} - \\text{1st value} = 3 - 0 = 3\\)) and \\(y = -2 \\times 3 + 2 = -6 + 2 = -4\\). When we run 3 units, we rise \\(-4 - 2 = -6\\) units (or fall 6 units). The table below summarizes our work. Scenario x y Scenario 1 0 2 Scenario 2 3 -4 \\(\\text{run} = x \\text{ in Scenario 2} - x \\text{ in Scenario 1} = 3 - 0 = 3\\) \\(\\text{rise} = y \\text{ in Scenario 2} - y \\text{ in Scenario 1} = -4 - 2 = -6\\) \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}} = \\dfrac{-6}{3} = -2\\) These calculations match the slope we find by inspecting the original equation \\(y = -2x + 2\\). The figure below shows the rise-over-run logic for each of the four example equations. Exercise 4.3 Add the lines \\(y = 2x - 1\\) and \\(y = -0.5x + 1\\) to the figured you sketched in Exercise 4.1. Hint Remember that a line is completely defined by only two points. Use this rule to draw the line. Choose two values of \\(x\\) and find the corresponding value of \\(y\\). Ideally, choose two values of \\(x\\) that are separated by some distance (so long as the resulting x-y pairs remain on our plot). Let’s try the first line using \\(x = -1\\) and \\(x = 2\\). For the first equation, we have \\(y = 2 \\times -1 -1 = -2 - 1 = -3\\) and \\(y = 2 \\times 2 -1 = 4 - 1 = 3\\), respectively. Then our two points are (-1, -3) and (2, 3), respectively. Just add lightly add these two points to the plot and draw a line that goes through both. Solution Exercise 4.4 What is the slope and intercept of the line below? Hint To find the intercept, find where the line crosses the vertical slide at \\(x = 0\\). This gives the intercept directly. To find the slope, simple choose two points along the line and find the rise (i.e., the vertical distance between the two points) and the run (i.e., the horizontal distance between the two points). The slope is the rise over the run or \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Solution The intercept is 1 and the slope is 1.5, so the equation for the line is \\(y = 1.5x + 1\\). Exercise 4.5 What is the slope and intercept of the line below? Solution The intercept is 1 and the slope is -0.5, so the equation for the line is \\(y = -0.5x + 1\\). "],
["the-scatterplot.html", "Chapter 5 The Scatterplot 5.1 geom_point() 5.2 Example: gapminder 5.3 Example: voteincome 5.4 Resources", " Chapter 5 The Scatterplot The scatterplot is the most powerful tool in statistics. The following comes as close to any rote procedure that I would recommend following: Always plot your data using a scatterplot. For some combinations of unordered, qualitative variables with a large number of categories, the scatterplot might not offer useful information. However, the plot itself will not mislead the researcher. Therefore, the scatterplot offers a safe, likely useful starting point for almost all data analysis. As an example, here’s Sarah’s data for the research project. She cares about the difference in ideology self-reports across different study designs. Although this isn’t an ideal application for a scatterplot (i.e., two fine-grained measures of x and y), the scatterplot is (1) at least somewhat helpful and (2) certainly not harmful. 5.1 geom_point() To create scatterplots, we simply use geom_point() as the geometry combined with our same approach to data and aesthetics. Here’s a simple example with hypothetical data. # create a hypothetical dataset with tribble() df &lt;- tribble( ~x, ~ y, 1, 1, 2, 2, 3, 6, 1, 3, 2.5, 5) %&gt;% glimpse() Observations: 5 Variables: 2 $ x &lt;dbl&gt; 1.0, 2.0, 3.0, 1.0, 2.5 $ y &lt;dbl&gt; 1, 2, 6, 3, 5 ggplot(df, aes(x = x, y = y)) + geom_point() Here’s a more realistic example. gamson_df &lt;- read_rds(&quot;data/gamson.rds&quot;) %&gt;% glimpse() Observations: 826 Variables: 2 $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, … $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, … ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Because the data are so dense, especially in the lower-left corner of the plot, we might use alpha transparency to make the density easier to see. ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point(alpha = 0.3) 5.2 Example: gapminder For a dataset with more variables, we can represent a few other variables using aesthetics other than location in space. For this example, we use country-level data from the gapminder package. # load gapminder dataset from gapminder package data(gapminder, package = &quot;gapminder&quot;) glimpse(gapminder) Observations: 1,704 Variables: 6 $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Af… $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 148803… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.… ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) Because GDP per capita is skewed so heavily to the right, we might transform the x-axis from a linear scale (the default) to a log (base-10) scale. ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + scale_x_log10() Because countries are evolving over time, we can connect the subsequent points using geom_path(). Note that geom_path() connects points as they are arranged in the dataset, so make sure your dataset is arranged properly. Because we want one path per country, we should include the aesthetic aes(group = country) as an argument to geom_path(). # arrange data by year gapminder2 &lt;- gapminder %&gt;% arrange(year) ggplot(gapminder2, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + geom_path(aes(group = country)) + scale_x_log10() This is a little hard to see, so let’s clean it up a bit. ggplot(gapminder2, aes(x = gdpPercap, y = lifeExp, size = pop, color = year)) + geom_path(aes(group = country), size = 0.5, alpha = 0.2) + geom_point(alpha = 0.3) + scale_x_log10() + facet_wrap(vars(continent)) 5.3 Example: voteincome # load voteincome data from Zelig package data(voteincome, package = &quot;Zelig&quot;) glimpse(voteincome) Observations: 1,500 Variables: 7 $ state &lt;fct&gt; AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR… $ year &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, … $ vote &lt;int&gt; 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ income &lt;int&gt; 9, 11, 12, 16, 10, 12, 14, 10, 17, 8, 15, 13, 10, 9, 5… $ education &lt;int&gt; 2, 2, 2, 4, 4, 3, 4, 1, 2, 1, 3, 3, 2, 2, 2, 3, 2, 1, … $ age &lt;int&gt; 73, 24, 24, 40, 85, 78, 31, 75, 54, 78, 71, 40, 46, 71… $ female &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, … ggplot(voteincome, aes(x = education, y = income)) + geom_point() Notice three things: The variable education is not under control. To see the codings, use help(voteincome, package = \"Zelig\"). Ideally, this variable (i) use qualitative labels rather than numeric placeholders and (ii) be a factor with reasonably ordered levels. There’s substantial over-plotting. Dozens of points are right on top of each other, so we cannot tell how many points are at each coordiate. Let’s fix the first issue for education, so you can see how. (income has many more levels, so let’s just get on with the plotting). voteincome2 &lt;- voteincome %&gt;% mutate(education = fct_recode(as.character(education), &quot;Less than High School Education&quot; = &quot;1&quot;, &quot;High School Education&quot; = &quot;2&quot;, &quot;College Education&quot; = &quot;3&quot;, &quot;More than a College Education&quot; = &quot;4&quot;)) %&gt;% glimpse() Observations: 1,500 Variables: 7 $ state &lt;fct&gt; AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR, AR… $ year &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, … $ vote &lt;int&gt; 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ income &lt;int&gt; 9, 11, 12, 16, 10, 12, 14, 10, 17, 8, 15, 13, 10, 9, 5… $ education &lt;fct&gt; High School Education, High School Education, High Sch… $ age &lt;int&gt; 73, 24, 24, 40, 85, 78, 31, 75, 54, 78, 71, 40, 46, 71… $ female &lt;int&gt; 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, … Now let’s deal with the overplotting. In general, we have two strategies for dealing with overplotting. alpha transparency jittering First, let’s try to adjust the alpha transparency. ggplot(voteincome2, aes(x = education, y = income)) + geom_point(alpha = 0.2) This helps, but only a little. We can we wher we have many points, where we have just a few, and where we have none. But overall, we still don’t have a good sense of the density at each coordinate. Let’s try jittering. To jitter the data, we add a small amount of noise to each point. We add enough noise to separate it from the other points, but not so much noise to distort the position along in the space. ggplot(voteincome2, aes(x = education, y = income)) + geom_point(position = &quot;jitter&quot;) Exercise 5.1 Write an R script that uses the parties dataset to create a scatterplot that allows you to evaluate Clark and Golder’s (2006) claim: The number of political parties increases as social heterogeity increases, but only under permissive electoral rules. Hint Perhaps use the following aesthetics: x = eneg and y = enep. Create individual facets for each electoral_system. Solution # load packages library(tidyverse) # load data parties_df &lt;- read_rds(&quot;data/parties.rds&quot;) # make scatterplot ggplot(parties_df, aes(x = eneg, y = enep)) + geom_point() + facet_wrap(vars(electoral_system)) Exercise 5.2 Go to the Dataverse repository for Barrilleaux and Rainey (2014) and download the dataset politics_and_need.csv. Plot the variable percent_uninsured (the percent of each state’s population without health insurance) along the horizontal axis and the variable percent_favorable_aca (the percent of each state with a favorable attitude toward Obamacare) along the vertical axis. Interpret and speculate about any pattern. I encourage you to represent other variables with other aesthetics. Exercise 5.3 Continuing the exercise above, label each point with the state’s two-letter abbreviation. Experiment with the following strategies. geom_text() instead of geom_point() geom_label() instead of geom_point() geom_text_repel() in the ggrepel package in addition to geom_point() geom_label_repel() in the ggrepel package in addition to geom_point() Hint: Review the help files (e.g., ?geom_text()) and the contained examples to understand how to use each geom. The variable state_abbr contains the two-letter abbreviation, so you’ll need to include the aesthetic label = state_abbr in the aes() function. 5.4 Resources Tufte. 2001. The Visual Display of Quantitative Information. Healy. 2018. Data Visualization: A Practical Introduction. [html] Wickham. ggplot2: Elegant Graphics for Data Analysis. [html for in-progress 3rd e.] RStudio’s ggplot2 cheat sheet [GitHub] The help file for geom_point() has some examples. The help file for geom_path() might be helpful, especially if you have the same country observed for multiple years and want to connect the subsequent points. The help file for geom_text() might be helpful, especially if you have only a few observations and your readers know something about some of them. "],
["correlation-coefficient.html", "Chapter 6 Correlation Coefficient 6.1 Intuition 6.2 Computing 6.3 Interpreting 6.4 Example: Clark and Golder (2006) 6.5 Example: Feeling Thermometers", " Chapter 6 Correlation Coefficient We’ve discussed several ways to reduce data–to summarize the key features of many observations using a single (or a few) numbers. A histogram visually shows the density in chosen bins. The average tells us the location of a set of observations. Remember the seesaw analogy. The SD tells us the scale (or spread or disperson) of a set of observations. We can describe a list of numbers as being “about the average give or take [the SD].” The correlation coefficient allows us to describe the relationship between two variables. Before, we compared variables by comparing their histograms, averages, or SDs. The correlation coefficient is our first summary that compares two variables directly (rather than summarizing just one). 6.1 Intuition The correlation coefficient measures how well two variables “go together.” “Go together” means “as one goes up, the other goes up [or down].” “Go together” has linearity built into the meaning. The correlation coefficient does not describe curved relationships. The figure below shows some scatterplots and how well I might say these variables go together. However, I am firmly opposed to any rules that link particular correlation coefficients to strength of relationship. Imagine the following studies: A study comparing two measures of the same concept. A study comparing the effect of a dose of vitamin D in the first hour after birth on lifespan. A “weak” or “small” correlation in the first study would be impossibly large in the second. The interpretation of the strength of a relationship must be made by a substantive expert in a particular substantive context. I use two guidelines to interpret a correlation coefficient: 0.9 seems a lot stronger than 0.7, but 0.4 seems barely stronger than 0.2. Around 0.4 [-0.4], the a correlation becomes “easily noticeable” without studying the plot carefully. For smaller datasets, this threshold increases toward 1 [-1]; for larger datasets, the threshold shrinks toward 0. Exercise 6.1 Guess the correlation coefficient for each scatterplot below. Solution dataset r Dataset 1 -0.60 Dataset 2 0.45 Dataset 3 0.90 Dataset 4 0.45 Dataset 5 0.55 Dataset 6 0.55 Dataset 7 0.10 Dataset 8 0.85 Dataset 9 0.85 Dataset 10 0.35 Dataset 11 0.60 Dataset 12 0.80 6.2 Computing Suppose we have the dataset below. x y 1 10 3 15 2 12 4 13 5 18 6.2.1 By Hand We can compute the correlation coefficient \\(r\\) as follows: \\(r = \\text{average of} \\left[ (x \\text{ in standard units}) \\times (y \\text{ in standard units}) \\right]\\) Using \\(\\overline(x)\\) to represent the average of \\(x\\) and \\(n\\) to represent the number of observations (5, in this case), we have \\(r = \\dfrac{\\frac{(x - \\overline{x})}{\\sqrt{\\frac{(x - \\overline{x})^2}{n}}} \\times \\frac{(y - \\overline{y})}{\\sqrt{\\frac{(y - \\overline{y})^2}{n}}}}{n}\\). We can implement this formula by creating the little table below and then averaging the final column of products. x y x in SUs y in SUs product 1 10 -1.41 -1.32 1.87 3 15 0.00 0.51 0.00 2 12 -0.71 -0.59 0.41 4 13 0.71 -0.22 -0.16 5 18 1.41 1.61 2.28 The average of the final column is 0.88. 6.2.2 With R In R, we can compute the corrlation between x and y using cor(x, y). Note that dropping missing values is more complicated for pairs of data. If you want to drop missing values from the calculations, then cor(x, y, use = pairwise.complete.obs\") is a good choice. We can use the code below to find the correlation in the example above. x &lt;- c(1, 3, 2, 4, 5) y &lt;- c(10, 15, 12, 13, 18) cor(x, y) [1] 0.8814089 Exercise 6.2 Compute the correlation coefficient between each combination of the four variables below. Check your work with R. x y z 2 8 7 4 0 3 5 5 5 6 3 6 4 6 6 3 5 3 6.3 Interpreting In general, a correlation coefficient is NOT particularly useful. I introduce it for two reasons: Other people use it. We use it to obtain more useful quantities. However, the correlation coefficient \\(r\\) has a concrete interpretation: If \\(x\\) is one SD larger, then \\(y\\) is \\(r\\) SDs larger on average. We might also say that “a one SD increase in \\(x\\) leads to an \\(r\\) SD increase in \\(y\\) on average,” but we must take care that “leads to” describes a pattern in the data and does not describe a causal relationship. 6.4 Example: Clark and Golder (2006) For a substantive example, consider Clark and Golder’s data. # load parties dataset parties_df &lt;- read_rds(&quot;data/parties.rds&quot;) # compute correlation between enep and eneg for each electoral system cor_df &lt;- parties_df %&gt;% group_by(electoral_system) %&gt;% summarize(cor = cor(enep, eneg)) electoral_system cor Single-Member District 0.04 Small-Magnitude PR 0.45 Large-Magnitude PR -0.02 ggplot(parties_df, aes(x = eneg, y = enep)) + geom_point(alpha = 0.5) + facet_wrap(vars(electoral_system)) + geom_label(data = cor_df, aes(x = Inf, y = Inf, label = paste0(&quot;cor = &quot;, round(cor, 2))), hjust = 1.1, vjust = 1.1) + theme_bw() As Clark and Golder expect, we get a correlation coefficient near zero in SMD systems. But contrary to their expectation, we also get a correlation coefficient near zero in large-magnitude PR systems. Exercise 6.3 Interpret the correlation for small-magnitude PR systems above by filling in the following blanks: A one SD increase in ENEG leads to a _____ SD increase in ENEP, on average. A _____ unit increase in ENEG leads to a _____ unit increase in ENEP, on average. Hint How many units is one SD for ENEG? What about for ENEP? Going from SDs to the original units is like going from feet to yards: you just need to know how many feet are in a yard (or how many SDs are in each original unit). 6.5 Example: Feeling Thermometers Below, I compute the correlation between feelings toward the Democratic and Republican parties. It makes sense that this correlation should be negative. As respondents’ feelings toward the Democratic party grow warmer, their feelings toward the Republican party should grow cooler. We might also expect this correlation to be stronger among more educated respondents and change over time. The example below uses the therms dataset in the pos5737data package available on GitHub. # get pos5737data (if updated) devtools::install_github(&quot;pos5737/pos5737data&quot;) # load data data(therms, package = &quot;pos5737data&quot;) # quick look glimpse(therms) Observations: 38,100 Variables: 4 $ year &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 19… $ ft_democratic_party &lt;dbl&gt; 80, 50, 40, 60, 85, 50, 70, NA, 60, NA, NA, … $ ft_republican_party &lt;dbl&gt; 50, 50, 60, 60, 60, 50, 40, NA, 60, NA, NA, … $ education &lt;fct&gt; High School, 8th Grade or Less, High School,… # compute correlation between the two feelinging thermometers for # each year and education level smry_df &lt;- therms %&gt;% # drop observations where education is missing drop_na(education) %&gt;% # compute correlation for each year-education subset group_by(year, education) %&gt;% summarize(cor = cor(x = ft_democratic_party, y = ft_republican_party, use = &quot;pairwise.complete.obs&quot;)) %&gt;% # complete dataset by right-joining a dataset that has all years and all education levels combinations right_join(crossing(year = unique(therms$year), education = unique(therms$education))) %&gt;% # add a variable for presidential elections--if the year is evenly divisible by 4 mutate(election_type = ifelse(test = year %% 4 == 0, yes = &quot;Presidential Election&quot;, no = &quot;Congressional Election&quot;)) %&gt;% glimpse() Observations: 102 Variables: 4 Groups: year [17] $ year &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1980, 1980, 19… $ education &lt;fct&gt; 8th Grade or Less, Some High School, High School, … $ cor &lt;dbl&gt; -0.219932553, -0.153398674, -0.154494861, -0.05971… $ election_type &lt;chr&gt; &quot;Congressional Election&quot;, &quot;Congressional Election&quot;… # plot correlations ggplot(smry_df, aes(x = year, y = cor, color = election_type)) + geom_point() + geom_line() + facet_wrap(vars(education)) Exercise 6.4 Read the excerpt from Clark, Golder, and Golder on pp. 477-478. Download the gamson dataset from the data page. Compute the correlation coefficient \\(r\\) between seat and portfolio shares and create a scatterplot of the two. Comment briefly. Solution # load data gamson_df &lt;- read_rds(&quot;data/gamson.rds&quot;) # compute correlation coefficient cor(x = gamson_df$seat_share, gamson_df$portfolio_share) [1] 0.9423176 # create scatterplot ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Exercise 6.5 Use devtools::install_github(pos5737/pos5737data) to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737\"). Useglimpse(anscombe)` to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. Use a group_by() and summarize() workflow to compute a correlation coefficient for each of the four datasets. How do they compare? What do they suggest about the strength of the relationship between \\(x\\) and \\(y\\)? Create a scatterplot of \\(x\\) and \\(y\\) with separate panels for each dataset. How do they compare? How would you describe the strength of the relationship between \\(x\\) and \\(y\\) in each panel? Would you say that the correlation coefficient offered a good summary of each dataset? "],
["regression.html", "Chapter 7 Regression 7.1 Slope and Intercept", " Chapter 7 Regression 7.1 Slope and Intercept Take a look at the scatterplot below. It shows several potential lines to describe the relationship between x and y. Indeed, each of these lines seems reasonable. We can imagine many more reasonable lines. But what is the best line? Least squares offers one principle way to choose the best line. The least squares principles says to choose the line that minimizes the RMS of of the residuals. 7.1.1 Multiple Regression "],
["references.html", "References", " References "]
]
