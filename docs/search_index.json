[
["index.html", "Concepts and Computation: An Introduction to Political Methodology Chapter 1 Overview", " Concepts and Computation: An Introduction to Political Methodology Carlisle Rainey 2020-08-31 Chapter 1 Overview "],
["statistical-computing-with-r.html", "Chapter 2 Statistical Computing with R 2.1 R as a Calculator 2.2 Scripts 2.3 Object-Oriented Programming 2.4 Missing Values 2.5 Logical Operators 2.6 Packages", " Chapter 2 Statistical Computing with R R is a complex, powerful statistical programming language. It’s also free! I use R to do all my empirical and methodological work. I use R to wrangle data, fit statistical models, perform simulation studies, and draw graphics. R works by scripts. The user writes a program called a “script”\" and R executes the program. This might intimidate you a little. That’s okay. It’s easier than it sounds, and I’m here to help you. We’ll learn a lot about R this semester, but we’ll learn only some aspects of R. I have to include some features of R and exclude others. Just because I show you one way to tackle a problem doesn’t mean it’s the only (or the best) way. But in order to get you working with data ASAP, we have to exclude some important concepts and tools in R. Rather than use R directly, though, we’ll use RStudio to manage and run our R programs. RStudio is simply a way to organize our R code. I use RStudio for all my R programming. I even use RStudio to write documents and make presentations using RMarkdown. 2.1 R as a Calculator To get started with R, just open up RStudio and look around. If you want, you can use R like a calculator–just type directly into the console as you would a calculator. Fresh installation of RStudio 2 + 2 ## [1] 4 2*3 ## [1] 6 2^3 ## [1] 8 2/3 ## [1] 0.6666667 exp(3) ## [1] 20.08554 log(2) # this is the natural log ## [1] 0.6931472 sqrt(2) ## [1] 1.414214 Review Exercises Using R as a calculator, calculate the following (in the console): \\(32 + 17\\) \\(32 \\times 17\\) \\(\\frac{32}{17}\\) \\(32^2\\) \\(\\sqrt{32}\\) \\(\\log{32}\\) 2.2 Scripts Ultimately, we’ll want to write all of our code in scripts so that we can modify, reproduce, and check our work. From now on, almost everything we do will go in a script. The idea is not to do an analysis, but to write a script that can do an analysis for us. Scripts in RStudio To open a new R script, click File, New File, R Script. You can type lines of code directly into this script. In the upper-right corner of the script window, you’ll see a Run button. This runs the entire line that the cursor is currently on or all the highlighted lines. This is equivalent to Command + Enter (or Control + Enter on Windows). Unless the script takes a long time to run (and I don’t think any of ours will), I recommend hitting Command + A (or Control + A on Windows) to highlight the entire script and then Command + Enter (Control + Enter on Windows) to run the entire script. You need to get into the habit of running the entire script, because you want to entire script to work in one piece when you are done. It is much easier to do this if you’re running the entire script all along. ProTip: To run your code, press command + a (or control + a on Windows) and then press command + enter (or control + enter on Windows). To save this script, simply click File &gt; Save. I discuss where to save files a little later, but for now, just realize that R scripts will have a .R extension, such as my-script.R or important-analysis.R. 2.2.1 Importance Doing your work in a script is important. You might have done a statistical analysis before or at least manipulated data with Excel. Most likely, you went though several steps and perhaps ended with a graph. That’s fantastic, but there are several problems. If you want to re-do your analysis, you must go through the whole process again. You might forget what you did. (I shouldn’t say “might”–you will forget.) You cannot easily show others what you did. Instead, they must take your word for it. You cannot make small changes to your analysis without going through the whole process again. Scripting solves each of these problems. If you want to re-do your analysis, just open your script and click Run. If you forget what you did, just look at your script. If you want to show others exactly what you did, just show them your script. If you want to make a small change to your analysis, just make a small change to your script and click Run. Scripting might seem like a lot more work. At first, it will be more work. By the end of the semester, it will be less work. As part of the papers you’ll write for this class, you’ll write a script. 2.2.2 Comments You can also insert comments into R scripts. This is very important, especially when you are first learning to program. To insert a comment, simply type a pound or hash sign # (i.e., “hashtag” to me) anywhere in the code. Anything on the line after the hash will be ignored. I’ll always carefully comment my R code, and I’ll be even more careful about it for this class. Here’s an example of some commented code for the previous example. Comments in R ProTip: Use comments often to clearly describe the code to others and your future self. 2.3 Object-Oriented Programming But R is much more powerful than a simple calculator, partly because it allows object-oriented programming. You can store things as “objects” to reference later. Just about anything can be stored as an object, including variables, data sets, functions, numbers, and many others. 2.3.1 Scalars Let’s start with a single number, sometimes called a “scalar.” Let’s create an object b that holds or contains the number 2. To do this, we simply use the assignment operator &lt;-, which we read as “gets.” b &lt;- 2 # read as &quot;b gets 2&quot; We can be very creative with naming objects. Rather than b, we could have used myobject, myObject, my.object, or my_object. From a style perspective, I prefer my_object or important_variable. In general, you want to give objects descriptive names so that you code is easy to read, but short names so that the code is compact and easy to read and write. ProTip: Give objects short, descriptive names. We can now repeat some of the calculations from above, using b instead of two. Given that you know b equals two, check that the following calculations make sense. b + 3 ## [1] 5 b*3 ## [1] 6 b^3 ## [1] 8 3^b ## [1] 9 b/3 ## [1] 0.6666667 exp(b) ## [1] 7.389056 log(b) ## [1] 0.6931472 sqrt(b) ## [1] 1.414214 You probably realize that it would be easier to just use 2 rather than b. But we’ll be doing more complicated calculations. Rather than b holding scalars, it might hold thousands of survey responses. Rather than applying a simple function, we might apply many functions. 2.3.2 Functions So what is a function? In the above examples exp(), log(), and sqrt() are functions. Importantly, functions are followed immediately by parentheses (i.e., (), not [] or {}, which have different meanings). Arguments are supplied in the functions that tell the function what to do. You probably didn’t think about it at the time, but you can use many different bases when taking a logarithm. What base did we use when we ran log(b)? To see this, let’s open the help file. help(log) # or, equivalently ?log help file for log() The section “Usage” shows the typical function syntax. The log() function takes up to two arguments. The first argument x is a “numeric vector.” We’ll talk more specifically about numeric vectors below, but for now, we can consider a scalar as a numeric vector. If we provide the arguments in the same order that the appear in the functions in the “Usage” section, then we do not have to name the argument, but we still can. For example, log(b) and log(x = b) are equivalent. ProTip: If you need to know how to use a particular function such as exp(), then type help(exp) or ?exp into the console. You’ll also see from the help file that the default that default is base = exp(1), where exp(1) is just the number \\(e\\), the base of the natural log. This means that if you don’t specify base, it will use base = exp(1). log(b) # natural log ## [1] 0.6931472 log(b, base = exp(1)) # also natural log ## [1] 0.6931472 log(b, base = 10) # base-10 log ## [1] 0.30103 log(b, 10) # also a base-10 log ## [1] 0.30103 Notice that if we put the arguments in the proper order, we do not have to name the argument, so that log(b, base = 10) is equivalent to log(b, 10). However, the meaning of log(b, base = 10) is more clear, so I prefer that approach. ProTip: If arguments are supplied to functions in the correct order, then names are unnecessary. However, names should be included whenever there might be doubt about the meaning of the argument. In practice, this most often means leaving the first argument unnamed and naming the rest. Review Exercises Open a new script and give the object x the scalar 32. Repeat the first set of review exercises using x rather than the number 32. Add comments explaining what the code is doing. 2.3.3 Vectors But if we can only work with single numbers, we won’t get very far. When we do statistical computing, we’ll usually want to work with collections of numbers (or collections of character strings, like \"Republican\" or \"Male\"). In an actual problem, the collection might contain thousands or millions of numbers. Maybe these are survey respondents’ ages or hourly stock prices over the last few years. Maybe they are a respondent’s sex (i.e., \"Male\" or \"Female\") or party identification (i.e., \"Republican\", \"Democrat\", \"Independent\", or \"Other\"). We’ll call this collection of numbers or character strings a “vector” and we’ll refer to the number of elements in the vectors as the “length” of the vector. There are several types of vectors, classified by the sort of elements they contain. numeric: contain numbers, such as 1.1, 2.4, and 3.4. Sometimes numeric variables are subdivided into integer (whole numbers, e.g., 1, 2, 3, etc.) and double (fractions, e.g., 1.47, 3.35462, etc.). character: contain character strings, such as \"Republican\" or \"Argentina (2001)\". factor: contain character strings, such as \"Very Liberal\", \"Weak Republican\", or \"Female\". Similar to character, except the entire set of possible levels (and their ordering) is defined. logical: contain TRUE and/or FALSE. 2.3.3.1 Numeric Vectors Rather than the scalar 2, for example, we might want to work with the collection 2, 5, 9, 7, and 3. Let’s assign the collection above to the object a. We can create a vector using the “collect” function c(). a &lt;- c(2, 5, 9, 7, 3) ProTip: To create a vector, one tool we can use is the “collect” function c(). If we want to look at the object a, we need to enter a on a line by itself. This will print the object a for us to inspect. But since we only need to check this once, maybe we just type it in the console instead of including it in the script. a ## [1] 2 5 9 7 3 We can now apply functions to the vector a just like we did for the scalar b. In each case, the function is applied to each element of the vector. a + 3 ## [1] 5 8 12 10 6 a*3 ## [1] 6 15 27 21 9 a^3 ## [1] 8 125 729 343 27 3^a ## [1] 9 243 19683 2187 27 a/3 ## [1] 0.6666667 1.6666667 3.0000000 2.3333333 1.0000000 exp(a) ## [1] 7.389056 148.413159 8103.083928 1096.633158 20.085537 log(a) ## [1] 0.6931472 1.6094379 2.1972246 1.9459101 1.0986123 log(a, base = 10) ## [1] 0.3010300 0.6989700 0.9542425 0.8450980 0.4771213 sqrt(a) ## [1] 1.414214 2.236068 3.000000 2.645751 1.732051 # sum() adds all the elements together sum(a) ## [1] 26 # mean() finds the average--now we&#39;re doing statistics! mean(a) ## [1] 5.2 So far, we’ve only used numeric vectors–vectors that contain numbers. But we can create and work with other types of vectors as well. For now, let’s just illustrate two types: vectors of character strings, factors (and ordered factors), and logical vectors. Review Exercises In a script (perhaps the script you began in the exercises above), create a numeric vector assigning the collection 2, 6, 4, 3, 5, and 17 to the object my_vector. Create another numeric vector assigning the collection 64, 13, and 67 to the object myOtherVector. Use the sum() function to add the elements of my_vector together. Use the sqrt() function to take the square root of the elements of myOtherVector. Add 3 to the elements of my_vector. Add comments to the script explaining what this code is doing. 2.3.3.2 Character Vectors Character strings are simply letters (or numbers, I suppose) surrounded by quotes, such as \"Republican\" or \"Male\". If we put c() (i.e., “combine”) together multiple character strings, then we have a character vector. # create character vector x &lt;- c(&quot;Republican&quot;, &quot;Democrat&quot;, &quot;Republican&quot;, &quot;Independent&quot;) # print x x ## [1] &quot;Republican&quot; &quot;Democrat&quot; &quot;Republican&quot; &quot;Independent&quot; # for fun, try to multiply x times 3 x*3 # doesn&#39;t work ## Error in x * 3: non-numeric argument to binary operator A comment about escapes: Inside a string (i.e., text surrounded by quotes, e.g., \"Male\"), a backslash \\ is considered an “escape.” For example \\n represents new line and \\t represents tab. # new line cat(&quot;Repub\\nlican&quot;) ## Repub ## lican # tab cat(&quot;Demo\\tcrat&quot;) ## Demo crat This is important because filenames are sometimes represented with back-slashes, such as data\\nominate.csv. If entered this way, R will not read the data properly, because it treats \\n as an escape and tries to put a new line there. The solution is to use two back-slashes to represent one (e.g., data\\\\nominate.csv). Or you can use a forward-slash / instead (e.g., data/nominate.csv). Note that Mac uses forward-slashes in file names by default, so it might not come up. For Windows users, though, you need to use either double back-slashes or switch to forward slashes. Review Exercises Create a character vector containing the elements Male, Female, Male, Male, and Female. Assign this vector to the object sex. Create a character vector containing the elements Liberal, Moderate, Moderate, Conservative, and Liberal. Assign this vector to the object ideology. 2.3.3.3 Factor Vectors A factor vector is much like a character vector, but can only take on predefined values. While we might use a character vector to encode a variable that can have a variety of values (e.g., respondent’s name), we might use a factor to encode a variable that can take on just a few values, such as party identification (e.g., “Republican,” “Independent,” “Democrat,” “Other”). We refer to the possible values of a factor as the “levels.” Creating a factor is more tricky than creating a numeric or character vector. We might take several approaches, but I suggest the following two-step approach: Create a character vector containing the information using c(). Add the levels using the factor function. Factor vectors have two particular advantages over character vectors. It allows us to easily see when one category has zero observations. It allows us to control the order in which the categories appear. This will be useful, even for categorical variables that have no natural ordering (e.g., race, eye color). # create a character vector pid &lt;- c(&quot;Republican&quot;, &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Other&quot;) # check type class(pid) ## [1] &quot;character&quot; # table pid table(pid) # two problems: 1-weird order; 2-&quot;Indpendents&quot; missing ## pid ## Democrat Other Republican ## 1 1 2 We can fix these two problems by using a factor vector instead. # create a factor vector in two steps ## step 1: create a character vector pid &lt;- c(&quot;Republican&quot;, &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Other&quot;) ## step 2: add levels using factor() pid &lt;- factor(pid, levels = c(&quot;Republican&quot;, &quot;Independent&quot;, &quot;Democrat&quot;, &quot;Other&quot;)) # check type class(pid) ## [1] &quot;factor&quot; # table pid table(pid) # two problems fixed ## pid ## Republican Independent Democrat Other ## 2 0 1 1 You can see that by creating a factor variable that contains the level information, we can see that we have no Independents in our sample of four respondents. We can also control the ordering of the categories. Review Exercises Change the character vector sex created above to a factor vector. Be sure to explicitly add the levels. The order does not matter. Assign this new factor variable to the object sex_factor. Change the character vector ideology created above to a factor vector. Be sure to explicitly add the levels. Use an intuitive ordering. Assign this new factor variable to the object ideology_factor. 2.3.3.4 Logical Vectors Logical vectors contain elements that are true or false. R has a special way to represent true and false elements. R uses TRUE (without quotes) to represent true elements and FALSE (again, without quotes) to represent false elements. To create a logical vector, we can c() together a series of TRUE’s and/or FALSE’s. # create logical vector x &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE) # print x x ## [1] TRUE TRUE FALSE TRUE FALSE Review Excercises Create the logical vector containing. True, False, False, True, and True. Assign it to the object logic1. Multiply logic1 times 3. What do you get? Does that make sense? 2.3.4 More Information You should treat my notes as incomplete. I gloss over some potentially relevant distinction to get you up and going quickly. For a complete description of vectors, see chapter 4 of Advanced R Programming. You can find a similar description in chapter 20 of R4DS. I don’t mention dates and times at all, but R makes it easy to work with vectors of dates and date-times. We’ll discuss those when we get talk about data wrangling, because it’s a more advanced topic. For reference, you can see the lubridate page on the tidyverse website. 2.4 Missing Values Missing data are extremely common in statistics. For example, a survey respondent might refuse to reveal her age or income. Or we might not know the GDP or child mortality rate for a particular country in a particular year. In R, we can represent these values with NA (“not available”). Notice that NA does not have quotes. Different functions handle NA’s differently. Some function will drop missing values (e.g., compute the statistic using the non-missing data) and other functions will fail. Most of the simple functions that we’ll use at first will fail by default (e.g., sum(), mean()), but many of the more advanced functions we’ll use later (e.g., lm()) will drop missing values by default. x &lt;- c(1, 4, 3, NA, 2) log(x) # doesn&#39;t fail: computes the log for observed data, returns NA for missing data ## [1] 0.0000000 1.3862944 1.0986123 NA 0.6931472 sum(x) # fails: can&#39;t know the sum without know the value of the missing data ## [1] NA sum(x, na.rm = TRUE) # doesn&#39;t fail: setting na.rm = TRUE tell the function to drop the missing data ## [1] 10 Review Exercises Create the object x using x &lt;- c(1, 4, 3, NA, 2). Using mean() to find the mean of x with and without using the argument na.rm = TRUE. In a comment, explain why the results are different. Is na.rm = TRUE a reasonable choice? Repeat using sum() rather than mean(). 2.5 Logical Operators Occasionally, we’d like R to test whether a certain condition holds. We’ll use this most often to choose a subset of a data set. For example, we might need only the data from the 100th Congress (from a data set that contains all Congresses) or only data before 1990 (for a data set that contains all years from 1945 to 2000). The logical operators in R are &lt;, &lt;=, ==, &gt;=, &gt;, and !=. Notice that we must use ==, not =, to test for (exact) equality. We use != to test for inequality. We can use &amp; to represent “and” conditions and | to represent “or.” Logical operators return either TRUE or FALSE. Operator Syntax “less than” &lt; “less than or equal to” &lt;= “exactly equal to” == “greater than or equal to” &gt;= “greater than” &gt; “not equal to” != “or” | “and” &amp; Try running some of the following. Make sure you can anticipate the result. # less than 2 &lt; 1 2 &lt; 2 2 &lt; 3 # less than or equal to 2 &lt;= 1 2 &lt;= 2 2 &lt;= 3 # equal to 2 == 1 2 == 2 2 == 3 # greater than or equal to 2 &gt;= 1 2 &gt;= 2 2 &gt;= 3 # greater than 2 &gt; 1 2 &gt; 2 2 &gt; 3 # not equal to 2 != 1 2 != 2 2 != 3 # or (1 &gt; 2) | (3 &gt; 4) (1 &lt; 2) | (2 &gt; 4) (1 &lt; 2) | (3 &lt; 4) # and (1 &gt; 2) &amp; (3 &gt; 4) (1 &lt; 2) &amp; (2 &gt; 4) (1 &lt; 2) &amp; (3 &lt; 4) Review Exercises Use logical operators to test the whether each element of my_vector (created above) is… greater than 3. less than 3. equal to 3 greater than 3 or less than 3. less than or equal to 3 greater than or equal to 3 greater than 2 or less than 1 greater than 2 and less than 1 greater than 1 and less than 2 2.6 Packages Because R is an open-source program, it is easy to write extensions, and thousands of people have. These extensions come in the form of “packages” and these packages contain mostly functions that do stuff with data. For example, I’ve written an R package that does plotting. (I’ve since deferred to ggplot2). I’ve written another package that estimates statistical model. Hadley Wickam has written many R packages, some of which we’ll use. He’s written haven, which we can use to read proprietary data formats, such as Stata (.dta) files. He’s also written readr, which we can use to quickly read in better formats, such as comma-separated values (.csv), which I’ll encourage you to use throughout the course. He’s also written a package that is helpful for creating plots called ggplot2. We see that later. In order to use a package, it must be installed once and then loaded in each session (i.e., after each restart). Some packages come pre-installed in R (e.g., stats, MASS). Some of these pre-installed packages are automatically loaded in each session (e.g., stats), while others must be loaded manually in each session (e.g., MASS) if you want to use them. Other packages that do not come pre-installed with R need to be installed manually (but just once) and loaded in each session. 2.6.1 Installing Packages If you click the “Packages” tab in RStudio (positioned in the upper-right panel by default), it will show you a list packages that are currently installed. I’ve attached a screenshot of my installed packages below, but your’s might look slightly different–I’ve probably installed a lot more than you. List of packages in RStudio If you want to use a function from a package and it is not installed, you must first install it. Most R packages are available on CRAN, and can be installed with the install.packages() function. install.packages(&quot;ggrepel&quot;) install.packages(&quot;ggthemes&quot;) If you want to go ahead and install these packages, feel free–we’ll use them later. RStudio might ask you to choose a mirror. If so, just choose something close to you–it doesn’t really matter. If you look at your list of installed packages again, you should see ggrepel and ggthemes. You only need to install a package once. Once you’ve installed it, you have it on your hard drive. 2.6.2 Loading Packages In order to use function from a package, though, the package must be installed and loaded. In the packages list, the check box (beside the package name) indicates whether the package is loaded. In the screenshot above, you can see that I only had one package loaded at the time I took the screenshot–blme, a package useful for estimating Bayesian linear mixed-effects models. Let’s see how to access functions that are in packages. First, let’s create some data to plot. x &lt;- c(1, 3, 2, 5, 4) y &lt;- c(2, 1, 4, 3, 5) Now let’s try to use the qplot() function in the ggplot2 package to create a scatterplot of x and y. I’m assuming that you’ve already installed the ggplot2 package (it’s included in the tidyverse package). Now, let’s try to use qplot(). qplot(x, y) ## Error in qplot(x, y): could not find function &quot;qplot&quot; You’ll notice that the qplot() function cannot be found. That’s because while ggplot2 is installed (i.e., present in the library), it is not loaded (i.e., off the shelf). If you want to use a function from the ggplot2 package, you need to load the package using the library() function. You need to load the package each time you start a new session (e.g., restart RStudio), so be sure to include this in your script. Notice that you do not need to use quotes around the name of the package when using the library() function. library(ggplot2) qplot(x, y) When we use library(), it loads the entire set of functions in the package into our workspace, making them all accessible to us. There are literally hundreds of functions in the ggplot2 package. If we prefer to avoid loading all the functions, we can use the syntax package::function() to tell R where to find the function in the library without loading all the functions into the workspace. ggplot2::qplot(x, y) ggplot2::qplot(x, y) will work whether or not ggplot2 is loaded. I don’t have strong feelings about which approach is better–it depends on the context. If you only need to use one function from package one time, then perhaps it makes more sense to use the package::function() approach. If you’ll be using many functions many times, then it makes sense to use library(). Use whichever makes most sense to you. However, I’ll tend to use package::function() a lot, because it makes it clear where the function is coming from. Review Exercises Install and load the GGally package. Write a simple example of a function in the package. Use example code from the vignette. Install and load the ggdag package. Write a simple example of a function in the package. Use example code from the vignette. Install and load the texreg package. Write a simple example of a function in the package. Perhaps try the code below. If you are feeling ambitious, you might try compiling a LaTeX document with the LaTeX code output by texreg(). If you do this, be sure to start with the LaTeX template from 50-legs or the very minimal template from Homework 1. library(texreg) fit1 &lt;- lm(Fertility ~ . , data = swiss) fit2 &lt;- update(fit1, . ~ . -Examination) screenreg(list(fit1, fit2)) texreg(list(fit1, fit2)) "],
["loading-data-into-r.html", "Chapter 3 Loading Data into R 3.1 The Terms 3.2 Data Frames 3.3 How We’ll Always Use R", " Chapter 3 Loading Data into R Rather than manually entering data using c() or something else, we’ll want to load data in stored in a data file. 3.1 The Terms There are three important ingredients in loading a data set into R. The file type, usually indicated by the extension (.rds, .csv, .dta, .xlsx). The current working directory. The file path (relative to the working directory). 3.1.1 File Types In political science, our data sets are usually one of four types: R data or .rds files. This is the easiest format because it stores factors as factors and all the related information. Read with readr::read_rds(). Note that readr is part of the tidyverse, so library(tidyverse) loads readr. (I explain a “data frame” below, but realize that an .rds file can contain any R object, not just a data frame.) comma-separated value or .csv files. This is a common, sharable, robust data file type. You can open these files with any statistical software. GitHub renders these nicely. However, the csv format does not distinguish between factors as character strings, and treats variables as numbers or characters. Read with readr::read_csv(). Stata or .dta files. Another common data format because many political scientists use Stata. Read these files into R with haven::read_dta(). Excel or .xlsx files. Another common data format because data are easy to enter into spreadsheets like Excel. Read these files into R with readxl::read_excel(). 3.1.2 Comma-Separated Value Format (.csv) Data can be stored in a wide range of formats. One popular format, for example, is Stata’s proprietary .dta format. I typically use (and encourage you to use) the comma-separated values .csv format. The .csv format is excellent because it is open and simple. This means that anyone can use it without acess to proprietary software. It will also be useble by anyone into the foreseeable future. We can see why .csv files are easy to work with if we open it up the file nominate.csv with a text editor. You’ll see that you–with your eyes–can read the file directly. You don’t really need software at all! .csv in a Text Editor I tried the same thing for a similar .dta file. With your eyes, it looks like nonsense. You’ll definitely need Stata (or other speciallized software) to work with this file. .dta in a Text Editor Also, .csv files are easy to support, so they work in almost all data analysis software. For example, we can open up nominate.csv in Excel. You can see that we have six variables in the columns and many cases in the rows (we don’t know how many because they overflow the screen). In this case, each row represents a particular Congressperson from a particular Congress (with Presidents as well. The second row, for example, is for Rep. Callahan (R) from the 1st Congressional District of Alabama. During the 100th Congress, Rep. Calahan has a ideology score of 0.358, which means he’s conservative, but not as conservative as Pres. Reagan, who has a score of 0.747. We’ll work with these data a lot thoughout the semester, so we’ll have plenty of time for closer examination. .csv in Excel 3.1.3 The Working Directory Any time you work in R (or RStudio), you are working from a “working directory.” That is, whenever R needs to locate a file to load it or save it, it looks in the working directory. For example, I’m writing a paper now. I named the project directory wilks/ (which I placed in a folder called projects/ in my Dropbox folder.) If I open this project by double-clicking wilks.Rproj in the wilks/ project directory, I have opened that project in RStudio. If I run getwd() (“get the working directory”), R prints the following: &gt; getwd() [1] &quot;/Users/carlislerainey/Dropbox/projects/wilks&quot; Notice that the working directory is the project directory. This happens because with use .Rproj files to manage the way that RStudio interacts with our projects. If I did not have a project open in RStudio, then I get the following: &gt; getwd() [1] &quot;/Users/carlislerainey&quot; Of course, any files we want to access or save are in the project directory, which is three levels deeper in the file system than carlislerainey/. If I didn’t use an .Rproj file to manage the project, then I could set the working directory whereever I wanted with setwd(). See below: &gt; setwd(&quot;/Users/carlislerainey/Dropbox/projects/wilks&quot;) &gt; getwd() [1] &quot;/Users/carlislerainey/Dropbox/projects/wilks&quot; But we use an .Rproj file to manage our projects, so our working directory is always the project directory. I’ll say it louder for the people in the back. We use an .Rproj file to manage our projects, so our working directory is always the project directory. We don’t (or rarely) need to change or choose the working directory, just realize that it’s the project directory. 3.1.4 The Path Suppose we want to read the Stata data set ddrevisited_data_v1.dta. We know the following: The filetype is dta. We’ll use haven::read_dta(). The working directory is the project directory, as always. Now we just need the file path; that’s the only argument that haven::read_dta() requires. 3.1.4.1 Organizing Your Project Directories Any data set that you want to read into R should be in the project folder. You could put it in the main project directory, but you probably want to put it in a sub-directory (or perhaps even deeper). The diagram below outlines how I organized the files in cool-project/. I have an R/ subdirectory for R scripts. I have a data/ subdirectory for data sets. I put the raw data sets in data/raw/ (i.e., a subsubdirectory) to help protect them. I put the manuscript in doc/ and the subsubdirectories doc/fig/ for figures and doc/tab/ for tables. I have the output subdirectory for intermediate files I create along the way. cool-project/ ├── R/ │ ├── clean-data.R │ ├── fit-models.R │ ├── make-table.R │ └── plot-data.R ├── data/ │ ├── raw/ │ │ ├── ddrevisited_data_v1.dta │ │ └── p4v2018.xls │ ├── clean-data.csv │ └── clean-data.rds ├── doc/ │ ├── fig/ │ │ └── plot1.png │ ├── tab/ │ │ └── table.tex │ ├── cool-project.pdf │ └── cool-project.tex ├── output/ │ ├── model-fit.csv │ ├── model-fit.rds │ ├── model-pred.csv │ └── model-pred.rds ├── README.md └── cool-project.Rproj Below is a screenshot from Finder on macOS that shows the same organization. 3.1.4.2 An Aside on Formats If you’re reading carefully, you’ll notice that I keep both csv and rds versions of the data sets I create. I do this for the following reasons: rds data sets are the most convenient to compute with because preserve factors. I want this. csv data sets are the easiest to share and inspect, both locally (Finder even shows me the file!) and on GitHub (which renders a nice table). I want this. It’s easy to create both and keep them in sync. To create both, I just run write_rds() and write_csv() on consecutive lines. 3.1.5 Determining the Path Because we know the working directory is the project directory, we only need to provide the reading function the path relative to thet project directory. The path is just directions from the working/project directory to the file you want to read. For example, if we wanted to find ddrevisited_data_v1.dta, we would do the follwing (from the working/project directory): Go into data/. Go into raw/. Find ddrevisited_data_v1.dta. That’s it. To create the path, we just put these pieces together: data/raw/ddrevisited_data_v1.dta. Now we just give the path to haven::read_dta(). We get: read_dta(\"data/raw/ddrevisited_data_v1.dta\"). We need to assign the data set to an object, we want something the following: # load packages library(haven) # load raw data set raw_df &lt;- read_dta(&quot;data/raw/ddrevisited_data_v1.dta&quot;) This loads the data set into R. Review Exercises There are 8 total data sets in cool-project/ (all the files in data/ and output/). Answer the following questions for each data set: What function should you use to read that file? What’s the working directory? What’s the path (relative to the working directory)? Given the above, what’s the code to read the data (and store it as an object)? When you think you know, check your work by trying your commands. I put cool-project on Github. Clone it and see if you can load the data sets. Solution for ddrevisited_data_v1.dta The extension is .dta, so I know this is a Stata data set and that I need to use haven::read_dta(). The working directory is cool-project/ because the working directory is always the project directory since I use an .Rproj file to manage my projects. The path is data/raw/ddrevisited_data_v1.dta. I can see this in the directory tree above, or by inspecting the directory on my computer. Below: # load packages library(haven) # load raw data sets raw_df &lt;- read_dta(&quot;data/raw/ddrevisited_data_v1.dta&quot;) 3.1.6 rio Loading data into R is a little bit tricky and tedious. One reason is finding a function to handle the data format. If the data is .Rds, .csv, or .dta. formats, we already know what to do. But what if the data is in a format such as .tsv (tab separated), .xlsx (Microsoft Excel), .ods (OpenDocument spreadsheet), or any number of other formats? The R package rio contains the fuction import() that automatically adapts to the different formats according to the filename extension. It’s just one function—you simply need to point it to the data set. # load packages library(rio) # for generic import() function # read same data stored in three different formats nominate &lt;- import(&quot;data/nominate.csv&quot;) nominate &lt;- import(&quot;data/nominate.rds&quot;) nominate &lt;- import(&quot;data/nominate.dta&quot;) 3.2 Data Frames Almost the statistical computation we do in this class revolves around data sets. In R, it usually makes sense to store data sets as specific objects known as data frames. Data frames are simply a set of vectors that all contain the same number of elements. These might be numeric, character, factor, or logical vectors, or some mixture of types. When you read a data set into R using readr::read_csv, readr::read_rds(), haven::read_dta(), or some other method, it creates a data frame. A data frame is a special R object that holds a set of vectors that all have the name number of elements. If you think of the data set as an Excel spreadsheet, then you can think of the columns of the spreadsheet as the vectors held by the data frame. These vectors or variables can be numeric, character, factor, or logical. As a reminder, here are the variable types: numeric: numbers, such as 1.1, 2.4, and 3.4. Sometimes numeric variables are subdivided into integer (whole numbers, e.g., 1, 2, 3, etc.) and double (fractions, e.g., 1.47, 3.35462, etc.). character: text strings, such as \"Republican\" or \"Argentina (2001)\". factor: cateogories, such as \"Very Liberal\", \"Weak Republican\", or \"Female\". Similar to character, except the entire set of possible levels is defined. A factor variable may be ordered or unordered. logical: true or false, such as TRUE or FALSE. For the .csv files we will usually use, R cannot distinguish between character and factor variables. By default, readr::read_csv() will load these as character variables–there’s no way for R to know the entire set of levels from the .csv file anyway. Sometimes, though, it will be useful to work with factor variables. This is straightforward to change. 3.2.1 Working with Variables in Data Frames A data frame holds the variables, but it also hides the vectors. For example, the data frame nominate, which we loaded above, has a numeric variable ideology, but if we try to sum it, we get an error. sum(ideology) # fails because the variable ideology is hidden in a data frame ## Error in eval(expr, envir, enclos): object &#39;ideology&#39; not found We’ve loaded the data set, but R can’t seem to find the variable. That’s because the variable ideology is hidden in the data frame nominate. In order to access variables in data frames, we need to do one of two things. Use the $ operator. Use the data argument. Some functions, such as exp() are designed to work with vectors, not data frames. This will be the case for most functions we use (with the notable exceptions of plotting in with ggplot() and estimating liner model with lm()). To use the functions on variables stored in data frames, we need to use the $ operator. Suppose we have a data set loaded and given to the object my_data. If my_data contains the variable of interest my_variable, then we can access my_variable using the syntax my_data$my_varible. That is, the syntax data$var means “get the variable var from the data set data.” We’ll use this often, so make sure it’s clear. sum(nominate$ideology) # example of the $ operator ## [1] NA sum(nominate$ideology, na.rm = TRUE) # example of the $ operator ## [1] 164.457 But some functions are designed to work with data frames. For example, the qplot() function in the ggplot2 package is designed to work with data sets. If you open the help file for qplot() (i.e., help(qplot) after library(ggplot2)), you’ll see that one of the arguments is data. If you use this argument to point qplot() to the data frame, it will know where to find your variables. # load ggplot2 package, which contains the qplot function library(ggplot2) # example of a function with a data argument qplot(ideology, data = nominate) # using the data argument ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). Many of the functions we use take a data argument. If they do not, though, we’ll need to use the $ operator. Because we’ll almost always use data stored in data frames, you need to be sure to use one approach or the other. If the function has a data argument, use it. In other cases, use the $ operator. 3.3 How We’ll Always Use R Open RStudio by clicking the .Rproj file for the project. (If you haven’t created the .Rproj file yet, then open RStudio and click File &gt; New Project…). Open a new R script to do something new OR open a previously saved script to continue making progress. Review Exercises Download the nominate data sets from the data page and put them in the data/ subdirectory. Start a new R script that loads the tidyverse package. In the same script, load the each version of the nominate data set using the appropriate function (note that tidyverse automatically loads readr but not haven). Assign each data set to a different object name. Use the glimpse() (part of tidyverse) function to get a quick look at each data set. Repeat using import() (load the rio package first!). Try to use five different functions (mean(), sum(), etc.) on the data frames or variables in the data frames. Using # comments, explain what each function is doing. "],
["location-and-scale.html", "Chapter 4 Location and Scale 4.1 The Intuition 4.2 The Usual Measures 4.3 Robust Alternatives 4.4 Computation in R", " Chapter 4 Location and Scale 4.1 The Intuition If we took a histogram and tried to describe it to someone else without showing it to them, the most most important pieces of information are usually the location and scale.1 We might describe the variable this way: “The values are about __________, give or take ________ or so.” We can think of the first blank as the location and the second blank as the scale. The location describes where the histogram is positioned along the left-right axis. The scale describes the width (or “spread” or “dispersion”) of the histogram. Inspect the histogram of a hypothethical variable to the right. Notice the location and the scale. If we had to describe these data, we might say that our variable is “about zero give or take one or so.” While this variable has a particular location (about zero), we can imagine shifting it left or right. The figure below shows some possible shifts. We could shift it way to the left, so that it’s “about -6” or a little bit to the right so that it’s “about two.” We can also imagine increasing the scale (more spread) or decreasing the scale (less spread). The figure below shows some possible changes in scale. In each case, the “give or take” number is changing. 4.2 The Usual Measures 4.2.1 The Average The most common measure of the location of a variable is the average.2 Suppose we have a variable (a list of numbers) \\(X = \\{x_1, x_2, ..., x_n\\}\\). \\[\\begin{equation} \\text{average} = \\dfrac{\\text{the sum of the list}}{\\text{the number of entries in the list}} = \\dfrac{\\sum_{i = 1}^n x_i}{n} \\nonumber \\end{equation}\\] The average is easy to compute and easy to work with mathematically.3 Unfortunately, the average doesn’t have an easy interpretation. The best interpretation, in my mind, is as the balance-point for the data. If we imagine the left-right axis as a teeter-totter and stack the data along the beam according to their values, then the average is the position of the fulcrum that would balance the data-filled beam. 4.2.2 The Standard Deviation The most common measure of scale is the standard deviation (SD). The intuition is subtle, so let’s look a a simple example. Rember, our goal is a \"give-or-take number. Suppose we have a list of numbers \\(X = \\{1, 2, 3, 4, 5\\}\\). The average of this list is 3, so we can compute the deviation from average for each value. \\[\\begin{equation} \\text{deviation from average} = d = \\text{value} - \\text{average} \\nonumber \\end{equation}\\] In this case, \\(d = \\{-2, -1, 0, 1, 2\\}\\). We want to use these deviations to find a give-or-take number. Here’s an initial idea. Just take the absolute values \\(|d| = \\{2, 1, 0, 1, 2\\}\\). These tell us how far each entry falls away from the average. Then we could average the absolute devations to find how far a typical entries falls away from the average of the list. In this case, we get 1.2. This is reasonable approach and we’ll refer to it as the average absolute devation or a.a.d. (It turns out that the a.a.d. isn’t a common quantity, so I don’t elevate it with an all-caps acronym.) The a.a.d. has one big problem–it uses an absolute value. This introduces some computational and mathematical difficulties.4 So let’s do something similar. Rather than take the absolute value, let’s square the deviations, take the average, and then undo the square at the end, so that \\(\\text{SD} = \\sqrt{\\text{avg}(d^2)}\\). Sometimes taking the (3) square root of (2) the average of (1) the squares is called the RMS. In this case, the RMS of the deviations from the average is the SD, so that \\[\\begin{equation} \\text{SD} = \\sqrt{\\text{avg}(d^2)} = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}} = \\text{RMS of deviations from average}. \\nonumber \\end{equation}\\] The SD moves smoothly as you move around the entries in the list. To calculate the SD, first make this little table, with the list of values, the deviations from the average, and the squares of the deviations. \\(X\\) \\(d\\) \\(d^2\\) 1 -2 4 2 -1 1 3 0 0 4 1 1 5 2 4 Then compute the average of the squares of the deviations, which in this case is 2. Then take the square root of that average, which in this case is about 1.4. Notice that 1.4 is about 1.2 (the a.a.d.). The SD is bounded (weakly) below by the a.a.s., but they’ll usually be close, so we can think of the SD as how far a typical point falls away from the average. 4.3 Robust Alternatives The average and the SD are mathematically nice. But they are not robust. Seemingly innocuous changes in the variable can lead to large changes in the average and SD.5 We can definite robustness more concretely: How many observations do I need to corrupt to make the summary arbitrarily large? Suppose the toy variable \\(X = \\{0.1, -0.6, 1.1, 1.3, 0.2\\}\\). If I replace the first entry (0.1) with 1, 5, 10, 50, and so on, what happens to the average and SD? The table below shows that we can easily manipulate the average and SD by changing only one data point. In this sense, the average and SD are fragile. Summary Average SD Actual Data Set 0.42 0.70 First entry of \\(X\\) replaced with 1 0.60 0.71 …with 5 1.40 1.92 …with 10 2.40 3.86 …with 50 10.40 19.81 …with 100 20.40 39.81 …with 500 100.40 199.80 …with 1,000 200.40 399.80 If corrupted data present a problem, then what do we mean by “corrupt”? There are (at least) three ways to imagine corrupting a measurement. First, perhaps we have a data entry error. While entering data in a spreadsheet, you entered the number 50,000 into the “hours spent watching the news per day” variable instead of the “income” variable. Second, perhaps our measurement procedure is noisy. Suppose we are coding Twitter posts by their support or opposition to President Trump. Our algorithm might interpret a sarcastic take as support when it actually presented intense opposition. Third, the substantive model might not apply to a particular observation. Take Clark and Golder’s project as an eplxame. They suggest that SMD systems should only have two parties. Indeed, this is a strong theoretical equilibirum. However, it might take several elections to reach this equilibrium. Parties might take several years to coordinate and consolodate. If we include a new democracy in the data set, then we might consider these data “corrupted” since the conceptual model doesn’t apply (yet). The average and SD respond to even a small amount of corrupt data. As an alternative to the average, we might use the median, which is more robust. The median is the/a number which splits the values in half, so that equal numbers of entries lie above and below the median. We have two common robust alternatives to the SD. The interquartile range (IQR) is the difference between the 25th and 75th quantiles. The median absolute deviation (MAD) is the median of the absolute values of the deviations from the median (almost the a.a.d., but using the medians in place of averages). It turns out that multiplying the MAD by 1.4826 makes it similar to the SD in many dataset, so it’s common to rescale it. To illustrate the robustness of each of our measures of location and scale, let’s imagine a variable with 10 observations \\(X = \\{-1.1, 1.5, -1, -0.1, -1.1, 0, -0.4, 0, 0.8, 0.4\\}\\). Let’s see how the measures change as we corrupt more and more of the observations. Summary % Corrupted Average SD Median IQR MAD Actual Data Set 0% -0.10 0.81 -0.05 1.15 0.96 First entry of \\(X\\) replaced with 100 10% 10.01 30.01 0.00 1.03 0.89 First two entries… 20% 19.86 40.07 0.00 1.03 0.89 First three entries… 30% 29.96 45.85 0.20 75.28 0.89 First four entries… 40% 39.97 49.02 0.60 100.00 2.00 First five entries… 50% 50.08 49.92 50.40 99.90 73.54 First six entries… 60% 60.08 48.89 100.00 99.50 0.00 This table illustrates that while the average and SD respond to any corruption, the median, IQR, and MAD remain reasonable summaries of the uncorrupted variable with 40%, 20%, and 30% of the data corrupted, respectively. T The percent of the data that one can corrupt before they can make the measure arbitrarily large is called the breakdown point. Here are the breakdown points for our measures: Measure Breakdown Point Average 0% SD 0% Median 50% IQR 25% MAD 50% As you can see, the median and the MAD are highly robust–they achieve the theoretical maximum breakdown point. 4.4 Computation in R We can easily calculate all these measures of location and scale in R.6 # create variable x = {1, 2, 3, 4, 5} x &lt;- 1:5 # compute measures of location and scale mean(x) # average ## [1] 3 sd(x) # SD; see sidenote ## [1] 1.581139 median(x) # median ## [1] 3 IQR(x) # IQR ## [1] 2 mad(x) # MAD, rescaled by 1.4826 ## [1] 1.4826 mad(x, constant = 1) # MAD, not rescaled ## [1] 1 The functions above work nicely for computing on whole variables. But in most cases, we are interested in comparing the summaries across groups. Take the nominate data set for example. # load packages library(tidyverse) # load nominate data df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% glimpse() ## Rows: 7,080 ## Columns: 7 ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;int&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, 10, 2… ## $ party &lt;fct&gt; Republican, Democrat, Democrat, Democrat, Democrat, Republic… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, William… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0.279,… For these data, we might want to know the average ideology for Republicans and Democrats. We could do it the hard way. # create a data frame with only republicans rep_df &lt;- df %&gt;% filter(party == &quot;Republican&quot;) # compute average mean(rep_df$ideology, na.rm = TRUE) ## [1] 0.4213385 But this is tedious, especially if we wanted to do it by party and Congress. To compute these summaries for lots of subsets of the data, we have the group_by()/summarize() workflow. group_by() defines several groups in the data frame. The first argument is the data frame to group (but we’ll %&gt;% it in). The remaining arguments are the grouping variables. You can think if the groups as a footnote at the bottom of the data set that just mentions the variables that define the groups of interest. Whenever we act (in the wrangling sense) on the data set and the action makes sense in the context of groups, the action will happen by group. After grouping, we use summarize() to create summaries for each group. The first argument is the data frame to summarize (but we’ll %&gt;% it in). The remaining arguments are the summarizes to compute. The names of the remaining arguments become variables in the resulting data frame. smry_df &lt;- df %&gt;% # group by party and congress group_by(party, congress) %&gt;% # compute all of our measures of location and scale summarize(average = mean(ideology, na.rm = TRUE), sd = sd(ideology, na.rm = TRUE), median = median(ideology, na.rm = TRUE), iqr = IQR(ideology, na.rm = TRUE), mad = mad(ideology, na.rm = TRUE), mad1 = mad(ideology, constant = 1, na.rm = TRUE)) %&gt;% # quick look at our work glimpse() ## `summarise()` regrouping output by &#39;party&#39; (override with `.groups` argument) ## Rows: 32 ## Columns: 8 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Democrat, … ## $ congress &lt;int&gt; 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, … ## $ average &lt;dbl&gt; -0.3092901, -0.3130075, -0.3142407, -0.3333065, -0.3615000, … ## $ sd &lt;dbl&gt; 0.1653092, 0.1664293, 0.1658089, 0.1609726, 0.1524251, 0.137… ## $ median &lt;dbl&gt; -0.3200, -0.3200, -0.3200, -0.3360, -0.3815, -0.3835, -0.384… ## $ iqr &lt;dbl&gt; 0.22750, 0.22600, 0.23025, 0.24000, 0.20550, 0.17850, 0.1730… ## $ mad &lt;dbl&gt; 0.1675338, 0.1719816, 0.1690164, 0.1793946, 0.1490013, 0.132… ## $ mad1 &lt;dbl&gt; 0.1130, 0.1160, 0.1140, 0.1210, 0.1005, 0.0895, 0.0870, 0.08… We can plot this measures to get a sense of how they change over time. Notice that mad (rescaled by multiplying by 1.4826) closely corresponds to the SD, but mad1 (not rescaled) is much smaller. # wrangle the data for plotting gg_df &lt;- smry_df %&gt;% pivot_longer(average:mad1, names_to = &quot;measure&quot;) %&gt;% mutate(measure_of = ifelse(measure %in% c(&quot;average&quot;, &quot;median&quot;), &quot;location&quot;, &quot;scale&quot;)) %&gt;% glimpse() ## Rows: 192 ## Columns: 5 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Democrat… ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101… ## $ measure &lt;chr&gt; &quot;average&quot;, &quot;sd&quot;, &quot;median&quot;, &quot;iqr&quot;, &quot;mad&quot;, &quot;mad1&quot;, &quot;average&quot;… ## $ value &lt;dbl&gt; -0.3092901, 0.1653092, -0.3200000, 0.2275000, 0.1675338, 0… ## $ measure_of &lt;chr&gt; &quot;location&quot;, &quot;scale&quot;, &quot;location&quot;, &quot;scale&quot;, &quot;scale&quot;, &quot;scale&quot;… # plot the measures of location and scale ggplot(gg_df, aes(x = congress, y = value, color = measure)) + geom_line() + facet_grid(cols = vars(party), rows = vars(measure_of), scales = &quot;free_y&quot;) I use these terms intentionally. Later, when we discuss random variables, the terms “location” and “scale” will return (with similar meanings). Indeed, we paramaterize many distributions according to their location and scale. For example, the normal distribution has a location parameter \\(\\mu\\) and a scale parameter \\(\\sigma\\).↩︎ Some people refer to the “average” as the “mean”. I prefer to avoid this because the “mean” might also refer to the expectation of a random variable. I use “average” and “expected value” to differentiate these two meanings.↩︎ The median, alternatively, is not easy to compute and quite difficult to work with mathematically.↩︎ Here’s the gist: If you take an entry and slide it up and down (i.e., make it larger or smaller), then the a.a.d. moves up and down as well. This is fine, except the a.a.s. doesn’t respond smoothly. The figure to the right shows what happens as we move the first entry on the list above around–notice the kink! The derivative of the a.a.d. isn’t define here (i.e., there are lots of tangents). This makes things hard mathematically.↩︎ The mathmatical ease and the substantive fragility are related.↩︎ For reasons I don’t want to deal with now, R uses the formula \\(SD = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n - 1}}\\) rather than \\(\\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}}\\). This means that R’s SD will be slightly larger than the SD with my formula. This difference will be tiny in data sets with a typical number of observations.↩︎ "],
["the-normal-model.html", "Chapter 5 The Normal Model 5.1 The Intuition 5.2 The Normal Curve(s) 5.3 The Empirical Rule 5.4 The Normal Approximation 5.5 Review Exercises", " Chapter 5 The Normal Model 5.1 The Intuition Last week, we used the average and SD to reduce and entire variable to two summaries. We use the average and SD to fill in the following sentence: “The values are about ________, give or take ________ or so.” This week, we add an additional assumption. This week, we also say that the histogram of the variable follows the normal curve. The normal curve is a bell-shaped curve with a particular equation. There are two varieties. There is a general, parameterized normal distibution that can move left and right (i.e., change location) and grow wider or taller (i.e., change scale) 5.2 The Normal Curve(s) There are two particular normal curves that we care about the normal curve, which has a location and scale parameter that we can specify: \\(f(x | \\mu, \\sigma) = \\phi(x | \\mu, \\sigma) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^\\frac{{ - \\left( {x - \\mu } \\right)^2 }}{2\\sigma ^2 }\\) the standard normal curve, with the location and scale parameters fixed: \\(f(x | \\mu = 0, \\sigma = 1) = \\phi(x | \\mu = 0, \\sigma = 1) = \\frac{1}{{\\sqrt {2\\pi } }}e^\\frac{{ - x ^2 }}{2}\\) These equations are complicated. Instead of memorizing them or working carefully through the math, just understand (for now) that the normal curve has an equation that exactly characterizes it. The figure below shows the standard normal curve (\\(\\mu = 0\\) and \\(\\sigma = 1\\)) and several other paramaterizations. 5.3 The Empirical Rule It turns out that many variable’s have a histogram that resembles the normal curve. Because of this, the normal curve can sometimes serve as an effective model for these variables. For example, NOMINATE ideology scores for Republicans in the 115th Congress roughly follow the normal curve. df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% filter(party == &quot;Democrat&quot;, congress == 115) ggplot(df, aes(x = ideology)) + geom_histogram() However, the ideology scores for both Republicans and Democrats together does not follow a normal curve. df &lt;- read_rds(&quot;data/nominate.rds&quot;) ggplot(df, aes(x = ideology)) + geom_histogram() The histograms of ENEP by electoral system and social heterogeneity deviate slightly from the normal curve. ## Rows: 1,161 ## Columns: 4 ## $ x &lt;dbl&gt; 1.23, 1.33, 1.43, 1.53, 1.63, 1.73, 1.83, 1.93, … ## $ density &lt;dbl&gt; 0.02616495, 0.02960818, 0.03338541, 0.03751070, … ## $ electoral_system &lt;fct&gt; Large-Magnitude PR, Large-Magnitude PR, Large-Ma… ## $ social_heterogeneity &lt;fct&gt; Bottom 3rd of ENEG, Bottom 3rd of ENEG, Bottom 3… If the variable seems to follow the normal curve, then we have the following rules: About 68% of the data (i.e., “most”) fall within 1 SD of the average. About 95% of the data (i.e., “almost all”) fall within 2 SDs of the average. We can evaluate this rule with the parties data above. Some of the nine hisgrams follow the normal curve quite well (e.g., lower-left). Other’s seem to meaningfully deviate from the normal curve (e.g., middle-left). The table below shows the actual percent of the variable that falls within one and two SDs of the average for each histogram. As you can see, for the lower-left panel (SMD, Top 3rd), the empircal rule of 68% and 95% matches the actual values of 74% and 98% fairly well. For the middle-left panel (SMD, Middle 3rd), the empirical rule matches the actual values of 87% and 93% less well. Across all histograms, it seems fair that the empirical rule works as a rough approximation, even for histograms that meaningfully deviate from the normal curve. Electoral System Social Heterogeneity within 1 SD within 2 SDs Single-Member District Bottom 3rd of ENEG 87% 96% Single-Member District Middle 3rd of ENEG 87% 93% Single-Member District Top 3rd of ENEG 74% 98% Small-Magnitude PR Bottom 3rd of ENEG 68% 97% Small-Magnitude PR Middle 3rd of ENEG 73% 96% Small-Magnitude PR Top 3rd of ENEG 76% 93% Large-Magnitude PR Bottom 3rd of ENEG 80% 98% Large-Magnitude PR Middle 3rd of ENEG 77% 96% Large-Magnitude PR Top 3rd of ENEG 65% 97% 5.4 The Normal Approximation If our normal model summarizes a histogram well, then we can use the model to estimate the percent of the observations that fall in a given range. There are two approaches: Just like we add up the area of the bars to compute percentages with a histogram, we add up the area under the normal curve to approximate percentages. Use a normal table from a textbook. Because the table is for the standard normal curve, we need to re-locate and re-scale the data to fit the standard normal curve. Use the pnorm() function in R. Because this function is parameterized with location and scale, we can simple re-locate and re-scale the curve to fit the data. 5.4.1 Normal Table Normal tables offer an antiquated method to use the normal distribution to approximate percentages. Because we cannot have a normal table for all possible locations and scales, we have one: the standard normal table, which works for a variable with an average of zero and an SD of one. This seems limiting, but it turns out that we can easily re-locate and re-scale any value to match the standard normal curve. We simply subtract the average and divide by the SD. We call this new value a z-score. \\(z\\text{-score} = \\dfrac{\\text{value} - \\text{average}}{\\text{SD}}\\) FSuppose we have the list \\(X = \\{1, 2, 3, 4, 5\\}\\). Then the average is 3, and the SD is about 1.26. We can compute the zscore for the first entry 1 as \\(\\frac{1 - 3}{1.25} \\approx -1.58\\). Similarly, we can convert the entire list to z-scores and get \\(Z = \\{1.59, -0.79, 0.00, 0.79, 1.59\\}\\). If you compute the average and SD of the list \\(Z\\), you will find zero and one, respectively. We can then use a normal table to compute areas under the normal curve between (or above or below) these values of \\(z\\). There are two types of normal tables. Some tables report the percent (or proportion) of the normal curve below a particular value \\(z\\). Other tables report the percent (or proportion) of the normal curve between a particular value \\(z\\) and \\(-z\\). (The normal table on p. A-104 of FPP works this way.) Either table works, but you must know what type of table you are working with. Depending on the question, one type might offer a more direct solution. Here’s a small normal table for a few values of \\(z\\) that uses both approaches. z % less than z % between -z and z Status 0.00 50% 0% 0.10 54% 8% 0.20 58% 16% 0.30 62% 24% 0.40 66% 31% 0.50 69% 38% 0.75 77% 55% 1.00 84% 68% Important 1.50 93% 87% 1.64 95% 90% Important 1.96 98% 95% Important 2.00 98% 95% Important 3.00 100% 100% In order to use the table to find the area between any two values, you need to use the following three rules in combination. The normal table gives the area (i) below \\(z\\) or (ii) between \\(-z\\) and \\(z\\). The area under the entire normal curve is 1 or 100%. The normal curve is symetric, so that the area to the right of \\(z\\) equals the area to the left of \\(-z\\). 5.4.2 pnorm() The pnorm() function in R return the area under the normal curve less than \\(z\\). By default, it uses the standard normal curve, but you can specify a mean and sd if you prefer to re-locate and/or re-scale the curve to fit your values. # area under the std. normal curve less than 1 pnorm(1) ## [1] 0.8413447 # area under the a normal curve (with average of 1 and SD of 4) less than 1 pnorm(1, mean = 1, sd = 4) ## [1] 0.5 # area between -1.64 and 1.64 pnorm(1.64) - pnorm(-1.64) ## [1] 0.8989948 5.4.3 Exactly Percentages To actually compute percentages, we can create a function that works just like pnorm(), but it returns the percent of the data that fall below a particular value. The most convenient method is to create an \"empirical cumulative distribution function*. This function is somewhat confusing. The ecdf() function does not return the proportion below its argument. Instead, it creates a function that returns the percent below its argument. If we have a numeric vector x, then ecdf(x) is a function! Let that settle in… both ecdf and ecdf(x) are function. The function ecdf (I’m dropping the () for clarity) is a function that creates a function, and ecdf(x)() (I’m including the (), as usual, for clarity) is a function that returns the percent below. df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% filter(party == &quot;Democrat&quot;, congress == 115) # normal approximation for % of Democrats less than -0.05 avg &lt;- mean(df$ideology) sd &lt;- sd(df$ideology) pnorm(-0.5, mean = avg, sd = sd) ## [1] 0.1731597 # exact % of Democrats less than -0.05 ecdf(df$ideology)(-0.5) ## [1] 0.1479592 We can also plot the ECDF with ggplot2. 5.5 Review Exercises The plot below show the histograms for the ideology of legislators in the U.S. House by party. We can compute the average and SD by party. Party Average SD Democrat -0.39 0.12 Republican 0.49 0.15 The table below lists some of the leaders of each party and their ideology score. For each leader, use our three approaches to compute the percent of the party that is “more extreme” than their leader: inspect the histogram, use the normal approximation, and use R to compute the answer exactly. Name Party Position Ideology Score Inspect Histogram Normal Approximation Actual RYAN, Paul D. Republican Speaker of the House 0.56 MCCARTHY, Kevin Republican Majority Leader 0.46 SCALISE, Steve Republican Majority Whip 0.56 McMORRIS RODGERS, Cathy Republican Conference Chair 0.43 PELOSI, Nancy Democrat Minority Leader -0.49 HOYER, Steny Hamilton Democrat Minority Whip -0.38 CLYBURN, James Enos Democrat Assistant Democratic Leader -0.46 LEWIS, John R. Democrat Senior Chief Deputy Minority Whip -0.59 "],
["the-x-y-space.html", "Chapter 6 The X-Y Space 6.1 Points 6.2 Lines", " Chapter 6 The X-Y Space 6.1 Points The scatterplot has two key aesthetics: the horizontal and vertical location of points (and lines). We refer to the horizontal location as “x” and the vertical location as “y.” We sometimes refer to this two-dimmensional space (of horizontal-vertical or x-y locations) as the “Cartisian coordinate system.” The table below contains five observations. Each observation has values for variables x and y. (In the context of a data analysis, we typically think of x as the key explanatory variable and y as the outcome varaible.) Observation x y #1 1 1 #2 4 4 #3 -4 -3 #4 2 -2 #5 -2 4 The plot below shows the location of each point in the x-y space. Exercise 6.1 Recreate the x-y space below on a sheet of paper. Add the following points (0, 0), (1, 2), (-3, 4), (2, -3), and (-4, -2). Hint The first number in the (x, y) pair represents the “x” or horizontal location. The second number represents the “y” or vertical location. The location of the second point is… Solution 6.2 Lines We can also draw lines in the x-y space. Remember, the formula for a line is \\(y = mx + b\\). Here, \\(y\\) and \\(x\\) represent variables (i.e., locations in the x-y space), \\(m\\) represents the slope of the line, and \\(b\\) represents the intercept. Consider the following four examples: Example Equation Intercept Slope Example 1 \\(y = x + 0\\) 0 1.0 Example 2 \\(y = -2x + 2\\) 2 -2.0 Example 3 \\(y = 0.5x - 1\\) -1 0.5 Example 4 \\(y = 3x - 2\\) -2 3.0 6.2.1 The Intercept The intercept \\(b\\) tells us where the line crosses the vertical slice of the space where \\(x = 0\\). Exercise 6.2 For the lines below, identify the intercept visually, if possible. Hint All the lines look like 45-degree lines (or 315-degree) because the ranges of the axes are rescaled. This makes the problem a little trickier. For each line, identify where the line cross the slide of the space where \\(x = 0\\). When \\(x = 0\\), then \\(y = m \\times 0 + b = b\\). Remember that \\(b\\) represents the intercept. Solution The intercepts are 0, -2, 1, 1.5. You cannot see the third intercept visually, because the slice of the space where \\(x = 0\\) is not included in the plot. 6.2.2 Slope The slope \\(m\\) tells us how fast the line rises or falls as we move from left-to-right. If \\(m\\) is positive, then the line rises. If \\(m\\) is negative, then the line falls. If \\(m\\) is zero, then the line neither rises nor falls (stays constant at the same height). As \\(m\\) gets larger in magnitude, the line rises or falls faster. The best way to think about slope is as the “rise over run.” \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Take Example 2 from the table above or \\(y = -2x + 2\\). Consider two scenaarios: one where \\(x = 0\\) and another where \\(x = 3\\). When \\(x = 0\\), we know that \\(y = 2\\) because the intercept \\(b\\) equals 2. When \\(x = 3\\), we have “run” 3 units to the right (i.e., \\(\\text{run} = \\text{2nd value} - \\text{1st value} = 3 - 0 = 3\\)) and \\(y = -2 \\times 3 + 2 = -6 + 2 = -4\\). When we run 3 units, we rise \\(-4 - 2 = -6\\) units (or fall 6 units). The table below summarizes our work. Scenario x y Scenario 1 0 2 Scenario 2 3 -4 \\(\\text{run} = x \\text{ in Scenario 2} - x \\text{ in Scenario 1} = 3 - 0 = 3\\) \\(\\text{rise} = y \\text{ in Scenario 2} - y \\text{ in Scenario 1} = -4 - 2 = -6\\) \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}} = \\dfrac{-6}{3} = -2\\) These calculations match the slope we find by inspecting the original equation \\(y = -2x + 2\\). The figure below shows the rise-over-run logic for each of the four example equations. Exercise 6.3 Add the lines \\(y = 2x - 1\\) and \\(y = -0.5x + 1\\) to the figured you sketched in Exercise 6.1. Hint Remember that a line is completely defined by only two points. Use this rule to draw the line. Choose two values of \\(x\\) and find the corresponding value of \\(y\\). Ideally, choose two values of \\(x\\) that are separated by some distance (so long as the resulting x-y pairs remain on our plot). Let’s try the first line using \\(x = -1\\) and \\(x = 2\\). For the first equation, we have \\(y = 2 \\times -1 -1 = -2 - 1 = -3\\) and \\(y = 2 \\times 2 -1 = 4 - 1 = 3\\), respectively. Then our two points are (-1, -3) and (2, 3), respectively. Just add lightly add these two points to the plot and draw a line that goes through both. Solution Exercise 6.4 What is the slope and intercept of the line below? Hint To find the intercept, find where the line crosses the vertical slide at \\(x = 0\\). This gives the intercept directly. To find the slope, simple choose two points along the line and find the rise (i.e., the vertical distance between the two points) and the run (i.e., the horizontal distance between the two points). The slope is the rise over the run or \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Solution The intercept is -1 and the slope is 1.5, so the equation for the line is \\(y = 1.5x - 1\\). Exercise 6.5 What is the slope and intercept of the line below? Solution The intercept is 1 and the slope is -0.5, so the equation for the line is \\(y = -0.5x + 1\\). "],
["the-scatterplot.html", "Chapter 7 The Scatterplot 7.1 geom_point() 7.2 Example: gapminder 7.3 Example: voteincome 7.4 Resources", " Chapter 7 The Scatterplot The scatterplot is the most powerful tool in statistics. The following comes as close to any rote procedure that I would recommend following: Always plot your data using a scatterplot. For some combinations of unordered, qualitative variables with a large number of categories, the scatterplot might not offer useful information. However, the plot itself will not mislead the researcher. Therefore, the scatterplot offers a safe, likely useful starting point for almost all data analysis. As an example, here’s Sarah’s data for the research project. She cares about the difference in ideology self-reports across different study designs. Although this isn’t an ideal application for a scatterplot (i.e., two fine-grained measures of x and y), the scatterplot is (1) at least somewhat helpful and (2) certainly not harmful. 7.1 geom_point() To create scatterplots, we simply use geom_point() as the geometry combined with our same approach to data and aesthetics. Here’s a simple example with hypothetical data. # create a hypothetical dataset with tribble() df &lt;- tribble( ~x, ~ y, 1, 1, 2, 2, 3, 6, 1, 3, 2.5, 5) %&gt;% glimpse() Rows: 5 Columns: 2 $ x &lt;dbl&gt; 1.0, 2.0, 3.0, 1.0, 2.5 $ y &lt;dbl&gt; 1, 2, 6, 3, 5 ggplot(df, aes(x = x, y = y)) + geom_point() Here’s a more realistic example. gamson_df &lt;- read_rds(&quot;data/gamson.rds&quot;) %&gt;% glimpse() Rows: 826 Columns: 2 $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Because the data are so dense, especially in the lower-left corner of the plot, we might use alpha transparency to make the density easier to see. ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point(alpha = 0.3) 7.2 Example: gapminder For a dataset with more variables, we can represent a few other variables using aesthetics other than location in space. For this example, we use country-level data from the gapminder package. # load gapminder dataset from gapminder package data(gapminder, package = &quot;gapminder&quot;) glimpse(gapminder) Rows: 1,704 Columns: 6 $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,… ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) Because GDP per capita is skewed so heavily to the right, we might transform the x-axis from a linear scale (the default) to a log (base-10) scale. ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + scale_x_log10() Because countries are evolving over time, we can connect the subsequent points using geom_path(). Note that geom_path() connects points as they are arranged in the dataset, so make sure your dataset is arranged properly. Because we want one path per country, we should include the aesthetic aes(group = country) as an argument to geom_path(). # arrange data by year gapminder2 &lt;- gapminder %&gt;% arrange(year) ggplot(gapminder2, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + geom_path(aes(group = country)) + scale_x_log10() This is a little hard to see, so let’s clean it up a bit. ggplot(gapminder2, aes(x = gdpPercap, y = lifeExp, size = pop, color = year)) + geom_path(aes(group = country), size = 0.5, alpha = 0.2) + geom_point(alpha = 0.3) + scale_x_log10() + facet_wrap(vars(continent)) 7.3 Example: voteincome [This block is currently not working because the Zelig package is out of commission at the moment.] # load voteincome data from Zelig package # data(voteincome, package = &quot;Zelig&quot;) # # glimpse(voteincome) # # ggplot(voteincome, aes(x = education, # y = income)) + # geom_point() Notice three things: The variable education is not under control. To see the codings, use help(voteincome, package = \"Zelig\"). Ideally, this variable (i) use qualitative labels rather than numeric placeholders and (ii) be a factor with reasonably ordered levels. There’s substantial over-plotting. Dozens of points are right on top of each other, so we cannot tell how many points are at each coordiate. Let’s fix the first issue for education, so you can see how. (income has many more levels, so let’s just get on with the plotting). # voteincome2 &lt;- voteincome %&gt;% # mutate(education = fct_recode(as.character(education), # &quot;Less than High School Education&quot; = &quot;1&quot;, # &quot;High School Education&quot; = &quot;2&quot;, # &quot;College Education&quot; = &quot;3&quot;, # &quot;More than a College Education&quot; = &quot;4&quot;)) %&gt;% # glimpse() Now let’s deal with the overplotting. In general, we have two strategies for dealing with overplotting. alpha transparency jittering First, let’s try to adjust the alpha transparency. # ggplot(voteincome2, aes(x = education, # y = income)) + # geom_point(alpha = 0.2) This helps, but only a little. We can we wher we have many points, where we have just a few, and where we have none. But overall, we still don’t have a good sense of the density at each coordinate. Let’s try jittering. To jitter the data, we add a small amount of noise to each point. We add enough noise to separate it from the other points, but not so much noise to distort the position along in the space. # ggplot(voteincome2, aes(x = education, # y = income)) + # geom_point(position = &quot;jitter&quot;) Exercise 7.1 Write an R script that uses the parties dataset to create a scatterplot that allows you to evaluate Clark and Golder’s (2006) claim: The number of political parties increases as social heterogeity increases, but only under permissive electoral rules. Hint Perhaps use the following aesthetics: x = eneg and y = enep. Create individual facets for each electoral_system. Solution # load packages library(tidyverse) # load data parties_df &lt;- read_rds(&quot;data/parties.rds&quot;) # make scatterplot ggplot(parties_df, aes(x = eneg, y = enep)) + geom_point() + facet_wrap(vars(electoral_system)) Exercise 7.2 Go to the Dataverse repository for Barrilleaux and Rainey (2014) and download the dataset politics_and_need.csv. Plot the variable percent_uninsured (the percent of each state’s population without health insurance) along the horizontal axis and the variable percent_favorable_aca (the percent of each state with a favorable attitude toward Obamacare) along the vertical axis. Interpret and speculate about any pattern. I encourage you to represent other variables with other aesthetics. Exercise 7.3 Continuing the exercise above, label each point with the state’s two-letter abbreviation. Experiment with the following strategies. geom_text() instead of geom_point() geom_label() instead of geom_point() geom_text_repel() in the ggrepel package in addition to geom_point() geom_label_repel() in the ggrepel package in addition to geom_point() Hint: Review the help files (e.g., ?geom_text()) and the contained examples to understand how to use each geom. The variable state_abbr contains the two-letter abbreviation, so you’ll need to include the aesthetic label = state_abbr in the aes() function. 7.4 Resources Tufte. 2001. The Visual Display of Quantitative Information. Healy. 2018. Data Visualization: A Practical Introduction. [html] Wickham. ggplot2: Elegant Graphics for Data Analysis. [html for in-progress 3rd e.] RStudio’s ggplot2 cheat sheet [GitHub] The help file for geom_point() has some examples. The help file for geom_path() might be helpful, especially if you have the same country observed for multiple years and want to connect the subsequent points. The help file for geom_text() might be helpful, especially if you have only a few observations and your readers know something about some of them. "],
["correlation-coefficient.html", "Chapter 8 Correlation Coefficient 8.1 Intuition 8.2 Computing 8.3 Interpreting 8.4 Example: Clark and Golder (2006) 8.5 Example: Feeling Thermometers", " Chapter 8 Correlation Coefficient We’ve discussed several ways to reduce data–to summarize the key features of many observations using a single (or a few) numbers. A histogram visually shows the density in chosen bins. The average tells us the location of a set of observations. Remember the seesaw analogy. The SD tells us the scale (or spread or disperson) of a set of observations. We can describe a list of numbers as being “about [the average] give or take [the SD].” The correlation coefficient allows us to describe the relationship between two variables. Before, we compared variables by comparing their histograms, averages, or SDs. The correlation coefficient is our first summary that compares two variables directly (rather than summarizing just one). 8.1 Intuition The correlation coefficient measures how well two variables “go together.” “Go together” means “as one goes up, the other goes up [or down].” “Go together” has linearity built into the meaning. The correlation coefficient does not describe curved relationships. The figure below shows some scatterplots and how well I might say these variables go together. However, I am firmly opposed to any rules that link particular correlation coefficients to strength of relationship. Imagine the following studies: A study comparing two measures of the same concept. A study comparing the effect of a dose of vitamin D in the first hour after birth on lifespan. A “weak” or “small” correlation in the first study would be impossibly large in the second. The interpretation of the strength of a relationship must be made by a substantive expert in a particular substantive context. I use two guidelines to interpret a correlation coefficient: 0.9 seems a lot stronger than 0.7, but 0.4 seems barely stronger than 0.2. Around 0.4 [-0.4], the a correlation becomes “easily noticeable” without studying the plot carefully. For smaller datasets, this threshold increases toward 1 [-1]; for larger datasets, the threshold shrinks toward 0. Exercise 8.1 Guess the correlation coefficient for each scatterplot below. Solution dataset r Dataset 1 -0.60 Dataset 2 0.45 Dataset 3 0.90 Dataset 4 0.45 Dataset 5 0.55 Dataset 6 0.55 Dataset 7 0.10 Dataset 8 0.85 Dataset 9 0.85 Dataset 10 0.35 Dataset 11 0.60 Dataset 12 0.80 8.2 Computing Suppose we have the dataset below. x y 1 10 3 15 2 12 4 13 5 18 8.2.1 By Hand We can compute the correlation coefficient \\(r\\) as follows: \\(r = \\text{average of} \\left[ (x \\text{ in standard units}) \\times (y \\text{ in standard units}) \\right]\\) Using \\(\\overline(x)\\) to represent the average of \\(x\\) and \\(n\\) to represent the number of observations (5, in this case), we have \\(r = \\dfrac{\\frac{(x - \\overline{x})}{\\sqrt{\\frac{(x - \\overline{x})^2}{n}}} \\times \\frac{(y - \\overline{y})}{\\sqrt{\\frac{(y - \\overline{y})^2}{n}}}}{n}\\). We can implement this formula by creating the little table below and then averaging the final column of products. x y x in SUs y in SUs product 1 10 -1.41 -1.32 1.87 3 15 0.00 0.51 0.00 2 12 -0.71 -0.59 0.41 4 13 0.71 -0.22 -0.16 5 18 1.41 1.61 2.28 The average of the final column is 0.88. 8.2.2 With R In R, we can compute the corrlation between x and y using cor(x, y). Note that dropping missing values is more complicated for pairs of data. If you want to drop missing values from the calculations, then cor(x, y, use = pairwise.complete.obs\") is a good choice. We can use the code below to find the correlation in the example above. x &lt;- c(1, 3, 2, 4, 5) y &lt;- c(10, 15, 12, 13, 18) cor(x, y) [1] 0.8814089 Exercise 8.2 Compute the correlation coefficient between each combination of the four variables below. Check your work with R. x y z 2 8 7 4 0 3 5 5 5 6 3 6 4 6 6 3 5 3 8.3 Interpreting In general, a correlation coefficient is NOT particularly useful. I introduce it for two reasons: Other people use it. We use it to obtain more useful quantities. However, the correlation coefficient \\(r\\) has a concrete interpretation: If \\(x\\) is one SD larger, then \\(y\\) is \\(r\\) SDs larger on average. We might also say that “a one SD increase in \\(x\\) leads to an \\(r\\) SD increase in \\(y\\) on average,” but we must take care that “leads to” describes a pattern in the data and does not describe a causal relationship. 8.4 Example: Clark and Golder (2006) For a substantive example, consider Clark and Golder’s data. # load parties dataset parties_df &lt;- read_rds(&quot;data/parties.rds&quot;) # compute correlation between enep and eneg for each electoral system cor_df &lt;- parties_df %&gt;% group_by(electoral_system) %&gt;% summarize(cor = cor(enep, eneg)) electoral_system cor Single-Member District 0.04 Small-Magnitude PR 0.45 Large-Magnitude PR -0.02 ggplot(parties_df, aes(x = eneg, y = enep)) + geom_point(alpha = 0.5) + facet_wrap(vars(electoral_system)) + geom_label(data = cor_df, aes(x = Inf, y = Inf, label = paste0(&quot;cor = &quot;, round(cor, 2))), hjust = 1.1, vjust = 1.1) + theme_bw() As Clark and Golder expect, we get a correlation coefficient near zero in SMD systems. But contrary to their expectation, we also get a correlation coefficient near zero in large-magnitude PR systems. Exercise 8.3 Interpret the correlation for small-magnitude PR systems above by filling in the following blanks: A one SD increase in ENEG leads to a _____ SD increase in ENEP, on average. A _____ unit increase in ENEG leads to a _____ unit increase in ENEP, on average. Hint How many units is one SD for ENEG? What about for ENEP? Going from SDs to the original units is like going from feet to yards: you just need to know how many feet are in a yard (or how many SDs are in each original unit). 8.5 Example: Feeling Thermometers Below, I compute the correlation between feelings toward the Democratic and Republican parties. It makes sense that this correlation should be negative. As respondents’ feelings toward the Democratic party grow warmer, their feelings toward the Republican party should grow cooler. We might also expect this correlation to be stronger among more educated respondents and change over time. The example below uses the therms dataset in the pos5737data package available on GitHub. # get pos5737data (if updated) devtools::install_github(&quot;pos5737/pos5737data&quot;) # load data data(therms, package = &quot;pos5737data&quot;) # quick look glimpse(therms) Rows: 38,100 Columns: 4 $ year &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1… $ ft_democratic_party &lt;dbl&gt; 80, 50, 40, 60, 85, 50, 70, NA, 60, NA, NA, 70, 8… $ ft_republican_party &lt;dbl&gt; 50, 50, 60, 60, 60, 50, 40, NA, 60, NA, NA, 40, 8… $ education &lt;fct&gt; High School, 8th Grade or Less, High School, High… # compute correlation between the two feelinging thermometers for # each year and education level smry_df &lt;- therms %&gt;% # drop observations where education is missing drop_na(education) %&gt;% # compute correlation for each year-education subset group_by(year, education) %&gt;% summarize(cor = cor(x = ft_democratic_party, y = ft_republican_party, use = &quot;pairwise.complete.obs&quot;)) %&gt;% # complete dataset by right-joining a dataset that has all years and all education levels combinations right_join(crossing(year = unique(therms$year), education = unique(therms$education))) %&gt;% # add a variable for presidential elections--if the year is evenly divisible by 4 mutate(election_type = ifelse(test = year %% 4 == 0, yes = &quot;Presidential Election&quot;, no = &quot;Congressional Election&quot;)) %&gt;% glimpse() Rows: 102 Columns: 4 Groups: year [17] $ year &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1980, 1980, 1980, 1… $ education &lt;fct&gt; 8th Grade or Less, Some High School, High School, High … $ cor &lt;dbl&gt; -0.219932553, -0.153398674, -0.154494861, -0.059715780,… $ election_type &lt;chr&gt; &quot;Congressional Election&quot;, &quot;Congressional Election&quot;, &quot;Co… # plot correlations ggplot(smry_df, aes(x = year, y = cor, color = election_type)) + geom_point() + geom_line() + facet_wrap(vars(education)) Exercise 8.4 Read the excerpt from Clark, Golder, and Golder on pp. 477-478. Download the gamson dataset from the data page. Compute the correlation coefficient \\(r\\) between seat and portfolio shares and create a scatterplot of the two. Comment briefly. Solution # load data gamson_df &lt;- read_rds(&quot;data/gamson.rds&quot;) # compute correlation coefficient cor(x = gamson_df$seat_share, gamson_df$portfolio_share) [1] 0.9423176 # create scatterplot ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Exercise 8.5 Use devtools::install_github(\"pos5737/pos5737data\") to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737data\"). Use glimpse(anscombe) to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. Use a group_by() and summarize() workflow to compute a correlation coefficient for each of the four datasets. How do they compare? What do they suggest about the strength of the relationship between \\(x\\) and \\(y\\)? Create a scatterplot of \\(x\\) and \\(y\\) with separate panels for each dataset. How do they compare? How would you describe the strength of the relationship between \\(x\\) and \\(y\\) in each panel? Would you say that the correlation coefficient offered a good summary of each dataset? "],
["regression.html", "Chapter 9 Regression 9.1 Review 9.2 The Equation 9.3 The Conditional Average 9.4 The Best Line 9.5 The RMS of the Residuals 9.6 \\(R^2\\) 9.7 Fitting Regression Models 9.8 Standard Errors and p-Values 9.9 A Warning 9.10 Review Exercises", " Chapter 9 Regression 9.1 Review So far, we’ve made comparison between sets of measurements by… Comparing the histograms of the sets of measurements (e.g., the ideologies of representatives from the 100th and 115th Congresses). Comparing the average or SD of sets of the measurements (e.g., the ideologies of representatives from the 100th and 115th Congresses). Computing the correlation coefficient between two sets of measurements (e.g., the portfolio share and the seat share for coalition governments) Regression allows us to extend the correlation coefficient–a measure of how well two variables “go together”–into a more intuitive, informative quantity. Regression allows us to answer the question “What is the average of \\(y\\)?” for many different values of \\(x\\). Rather than compare a small number of scenarios (e.g., the 100th and 115th Congresses), the regression line allows us to compare the average of \\(y\\) as \\(x\\) varies continuously. In short, we describe the average value of \\(y\\) as a linear function of \\(x\\), so that \\(\\text{the average of } Y = mX + b\\). Before we hop into regression, let’s review where we’ve been. Let’s do it with an exercise. Exercise 9.1 One of the simplest democratic mechanims goes like this: If things are going well, vote for the incumbent. If things are going poorly, vote against the incumbent. Normatively, we want to see voters punish incumbents for bad outcomes. Let’s see if incumbents get fewer votes when things go poorly. The dataset below shows incumbent presidents’ margins of victory (in the popular vote) and the percent change in the real disposable income from Q2 in the year before the election to Q2 in the year of the election. Do the following analyses with a pencil-and-paper. (You can use a computer for things like addition and multiplication if that makes it easier, but show your work on the paper.) Use the percent change in the RDI to break incumbents into “good performers” and “bad performers”. You may use more than two categories if you want. Draw a histogram of the margin of victory for the good performers and another for the bad performers. Compare and interpret. Compute the average and SD for each category. Compare and interpret. Compute the correlation between the percent change in the RDI and the margin of victory. Interpret. Now replicate the pencil-and-paper work with R. You can find the data here. Overall, do voters punish incumbents for bad outcomes? Year Incumbent % Change in RDI Margin of Victory 2016 H. Clinton 0.89 2.23 2012 Obama 2.83 3.93 2008 McCain 1.30 -7.38 2004 GWB 2.67 2.49 2000 Gore 4.13 0.54 1996 B. Clinton 2.21 9.47 1992 GHWB 2.93 -6.91 1988 GHWB 4.80 7.80 1984 Reagan 6.66 18.34 1980 Carter -1.08 -10.61 1976 Ford 0.10 -2.10 1972 Nixon 2.10 23.57 1968 Humphrey 4.04 -0.81 1964 Johnson 6.09 22.69 1960 Nixon 0.31 -0.17 1956 Ike 3.23 15.50 1952 Stevenson 0.44 -10.90 1948 Truman 4.49 4.74 Two warnings: Social scientists use the term “regression” imprecisely. In one case, they might use “regression” to refer to a broad class of models. In another case, they might use it to refer to the particular model discussed below. So watch out for inconsistent meanings, but the context should make the meaning clear. Methodologists often motivate regression from a random sampling perspective. This is not necessary but it’s common. In my view, it’s unfortunate because regression is a useful tool with datasets that are not random samples and it unnecessarily complicated the results. However, in the random sampling framework, one can motivate the methods below quite elegantly. 9.2 The Equation Let’s start by describing a scatterplot using a line. Indeed, we can think of the regression equation as an equation for a scatterplot. First, let’s agree that we won’t encounter a scatterplot where all the points \\((x_i, y_i)\\) fall exactly along a line. As such, we need a notation that allows us to distinguish the line from the observed values. We commonly refer to the values along the line as the “fitted values” (or “predicted values” or “predictions” and the observations themselves as the “observed values” or “observations.” We use \\(y_i\\) to denote the \\(i\\)th observation of \\(y\\) and use \\(\\hat{y}_i\\) to denote the fitted value (usually given \\(x_i\\)). We write the equation for the line as \\(\\hat{y} = \\alpha + \\beta x\\) and the fitted values as \\(\\hat{y}_i = \\alpha + \\beta x_i\\). We refer to the intercept \\(\\alpha\\) and the slope \\(\\beta\\) as coefficients. We refer to the difference between the observed value \\(y_i\\) and the fitted value \\(\\hat{y}_i = \\alpha + \\beta x_i\\) as the residual \\(r_i = y_i - \\hat{y}_i\\). Thus, for any \\(\\alpha\\) and \\(\\beta\\), we can write \\(y_i = \\alpha + \\beta x_i + r_i\\) for the observations \\(i = \\{1, 2, ..., n\\}\\). Notice that we can break each \\(y_i\\) into two pieces components the linear function of \\(x_i\\): \\(\\alpha + \\beta x_i\\) the residual \\(r_i\\). In short, we can describe any scatterplot using the model \\(y_i = \\alpha + \\beta x_i + r_i\\). The black points show the individual observations \\((x_i, y_i)\\), The green line shows the equation \\(\\hat{y} = \\alpha + \\beta x\\). The purple star shows the prediction \\(\\hat{y}_i\\) for \\(x = x_i\\). The orange vertical line shows the residual \\(r_i = y_i - \\hat{y}_i\\). Using this generic approach, we can describe any scatterplot using any line. Of course, the line above isn’t a very good line. How can we go about finding a good line? Exercise 9.2 Before we talk about a good line, let’s talk about a good point. Suppose you have a dataset \\(y = \\{y_1, y_2, ... , y_n\\}\\) and you want to predict these observations with a single point \\(\\theta\\). Use calculus to find the \\(\\theta\\) that minimizes the r.m.s. of the residuals \\(r_i = y_i - \\theta\\) or that minimizes \\(f(\\theta) = \\sqrt{\\dfrac{\\sum_{i = 1}^n(y_i - \\theta)^2}{n}}\\). Hint 1. Realize that the \\(\\theta\\) that minimizes \\(f(\\theta) = \\sqrt{\\dfrac{\\displaystyle \\sum_{i = 1}^n (y_i - \\theta)^2}{n}}\\) also minimizes \\(g(\\theta) = \\dfrac{\\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2}{n}\\). We know this because the square root function is monotonically increase (for positive values, which this must always be) and preserves the order of observations. In other words, the \\(\\theta\\) that produces the smallest RMS of the deviations also produces the smallest MS of the deviations. 1. Realize that the \\(\\theta\\) that minimizes \\(g(\\theta) = \\dfrac{\\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2}{n}\\) also minimizes \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2\\). Removing a constant just shifts the curve up or down, but it does not change the \\(\\theta\\) that minimizes the curve. So work with \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2\\). 1. To make things easier, expand \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2\\) to \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i^2 - 2\\theta y_i + \\theta^2)\\). 1. Distribute the summation operator to obtain \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n y_i^2 - 2 \\theta \\sum_{i = 1} y_i + n\\theta^2\\). 1. Now take the derivative of \\(h(\\theta)\\) w.r.t. \\(\\theta\\), set that derivative equal to zero, and solve for \\(\\theta\\). The result should be familiar. Exercise 9.3 See how other authors conceptualize the regression model. Read Lewis-Beck (1980), pp. 9-13 (up to “Least Squares Principle”) Read Wooldridge (2013), pp. 22-26 (Section 2.1). Do q. 1, p. 60. 9.3 The Conditional Average Have a look at the scatterplot below. What’s the portfolio share of a party in a coalition government with a seat share of 25%? Your eyes probably immediately begin examining a vertical strip above 25%. You probably estimate the average is a little more than 25%… call it 27%. You can see that the SD is about 10% because you’d need to go out about 10 percentage points above and below the average to grab about 2/3rds of the data. Now you’re informed by the data and ready to answer the question. Q: What’s the portfolio share of a party in a coalition government with a seat share of 25%? A: It’s about 28% give or take 10 percentage points or so. Notice that if we break the data into many small windows, we can visually create an average (and an SD) for each. Freedman, Pisani, and Purves (2008) refer to this as a “graph of averages.” Fox (2008) calls this “naive nonparametric regression.” It’s a conceptual tool to help us understand regression. For some datasets, these averages will fall roughly along a line. In that case, we can described the average value of \\(y\\) for each value of \\(x\\)–that is, the conditional average of \\(y\\)–with a line. Here’s the takeaway: a “good” line is the conditional average. Exercise 9.4 Other authors use this “graph of averages”. Read FPP, ch. 10. Set A, p. 161: 1-4. Set B, p. 163: 1, 3. Set C, p. 167: 1-3. Set D, p. 174: 1, 2. Set E, p. 175: 1, 2. Section 6, p. 176: 1-3, 5-7, 10. Read Read Fox (2008), pp. 17-21. (Optional: Section 2.3 describes how to create a smoothed average in a more principled manner.) 9.4 The Best Line So far, we have to results: The average is the point that minimizes the RMS of the deviations. We want a line that captures the conditional average. Just as the average minimizes the RMS of the deviations, perhaps we should choose the line that minimizes the RMS of the residuals… that’s exactly what we do. We want the pair of coefficients \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimizes the RMS of the residuals or \\(\\DeclareMathOperator*{\\argmin}{arg\\,min}\\) \\[\\begin{equation} (\\hat{\\alpha}, \\hat{\\beta}) = \\displaystyle \\argmin_{( \\alpha, \\, \\beta ) \\, \\in \\, \\mathbb{R}^2} \\sqrt{\\frac{r_i^2}{n}} \\end{equation}\\] Let’s explore three methods to find the coefficients that minimize the RMS of the residuals. grid search numerical optimization, to get additional intuition and preview more advanced methods analytical optimization 9.4.1 Grid Search Because we’re looking for the pair \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimize the sum of the RMS residuals, we could simply check lots of different values. In some applications, this is a reasonable optimization routine. (It is not reasonable in the context of regression, where we have much faster and accuracy tools.) The figure below shows the result of a grid search. In the top-left panel, we see the line and the residuals for each intercept-slope pair. The size of the points indicates the residual squared. Notice that some lines make big errors and other lines make small errors. In the top-right panel, we see a histogram of the squared residuals. In the lower-left panel, each point in the grid shows a intercept-slope pair. The label, color, and size indicate the RMS of the residuals. As we move around the grid, the RMS changes–we’re looking for the smallest. In the lower-right panel, the three lines show the evolution of the intercept, slope, and RMS of the residuals, respectively. Again, we’re looking for the pair that produces the smallest RMS of the residuals. For this search, the intercept -0.36 and the slope 0.89 produce the smallest RMS of the residuals (of the combinations we considered). 9.4.2 Numerical Optimization Remember that we simply need to minimize the function \\(f(\\alpha, \\beta) = \\displaystyle \\sqrt{\\frac{\\sum_{i = 1}^n r_i^2}{n}} = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{ \\sum_{i = 1}^n [y_i - (\\alpha + \\beta x_i)]^2}{n}}\\), shown below. Hill-climbing algorithms, such as Newton-Raphson, find the optimum numerically by investigating the shape of \\(f\\) at its current location, taking a step uphill, and then repeating. When no step leads uphill, the algorithm has found the optimum. Under meaningful restrictions (e.g., no local optima), these algorithms find the global optimum. First, let’s add the data from the grid search example above using tribble(). df &lt;- tribble( ~x, ~y, -1, -0.2, 1.1, -0.2, -1.1, -2.9, -0.2, -0.2, -0.5, -1.4, 1.9, 2.2, 1.2, 0.9, 0.1, -0.5, 0.7, -0.4, -1.3, -1.8, 2.3, 2.5, 0.4, -0.9, 1, 2, -0.9, -0.1, -0.1, 1.4, -1.2, -1.3, 0.9, 1, 0, -0.6, -0.6, -0.6, -0.3, -2.2, 1.4, 1.2, 1.8, 1.2, 1.6, -0.2, 2.2, 1.4, -0.7, -0.7, 0.6, -0.4, 0.8, 0.6 ) Now let’s create a function that takes the parameters (to be optimized over) as the first argument. f &lt;- function(par, data) { alpha &lt;- par[1] beta &lt;- par[2] y_hat &lt;- alpha + beta*data$x r &lt;- data$y - y_hat rms &lt;- sqrt(mean(r^2)) return(rms) } Now we can optimize this function f() using optim(). The default method is \"Nelder-Mead\", which works similarly to the Newton-Raphson algorithm you might have seen before. results &lt;- optim( par = c(0, 0), # initial slope and intercept fn = f, # function to optimize data = df # dataset for f() ) results$par [1] -0.3595557 0.9414289 results$value [1] 0.8448945 The Nelder-Mead optimization routine finds that intercept of -0.36 and a slope of 0.94 result in the smallest RMS of the residuals 0.84. This somewhat agrees with the results from the coarse grid search. If the grid search were more fine-grained, we could easily obtain solutions that agree to two decimal places. 9.4.3 Analytical Optimization In the case of finding the line that minimizes the RMS of the residuals, we have an easy analytical solution. We don’t need a grid search, and we don’t need to optimize numerically. 9.4.3.1 Scalar Form Remember that we simply need to minimize the function \\(f(\\alpha, \\beta) = \\displaystyle \\sqrt{\\frac{\\sum_{i = 1}^n [y_i - (\\alpha + \\beta x_i)]^2}{n}}\\). This is equivalent to minimizing \\(h(\\alpha, \\beta) = \\sum_{i = 1}^n(y_i - \\alpha - \\beta x_i)^2\\). We sometimes refer to this quantity as the SSR or “sum of squared residuals.” To minimize \\(h(\\alpha, \\beta)\\), remember that we need to solve for \\(\\frac{\\partial h}{\\partial \\alpha} = 0\\) and \\(\\frac{\\partial h}{\\partial \\beta} = 0\\) (i.e., the first-order conditions). Using the chain rule, we have the partial derivatives \\(\\frac{\\partial h}{\\partial \\alpha} = \\sum_{i = 1}^n [2 \\times (y_i - \\alpha - \\beta x_i) \\times (-1)] = -2 \\sum_{i = 1}^n(y_i - \\alpha + \\beta x_i)\\) and \\(\\frac{\\partial h}{\\partial \\beta} = \\sum_{i = 1}^n 2 \\times (y_i - \\alpha - \\beta x_i) \\times (-x_i) = -2 \\sum_{i = 1}^n(y_i - \\alpha - \\beta x_i)x_i\\) and the two first-order conditions \\(-2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i) = 0\\) and \\(-2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i)x_i = 0\\) 9.4.3.1.1 The 1st First-Order Condition \\[\\begin{align} -2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i) &amp;= 0 \\\\ \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i) &amp;= 0 \\text{ (divide both sizes by $-2$)} \\\\ \\sum_{i = 1}^n y_i - \\sum_{i = 1}^n \\hat{\\alpha} - \\sum_{i = 1}^n \\hat{\\beta} x_i &amp;= 0 \\text{ (distribute the sum)} \\\\ \\sum_{i = 1}^n y_i - n \\hat{\\alpha} - \\hat{\\beta}\\sum_{i = 1}^n x_i &amp;= 0 \\text{ (move constant $\\beta$ in front and realize that $\\sum_{i = 1}^n \\hat{\\alpha} = n\\hat{\\alpha}$)} \\\\ \\sum_{i = 1}^n y_i &amp; = n \\hat{\\alpha} + \\hat{\\beta}\\sum_{i = 1}^n x_i \\text{ (rearrange)} \\\\ \\frac{\\sum_{i = 1}^n y_i}{n} &amp; = \\hat{\\alpha} + \\hat{\\beta} \\frac{\\sum_{i = 1}^n x_i}{n} \\text{ (divide both sides by $n$)} \\\\ \\overline{y} &amp; = \\hat{\\alpha} + \\hat{\\beta} \\overline{x} \\text{ (recognize the average of $y$ and of $x$)} \\\\ \\end{align}\\] Theorem 9.1 The 1st first-order condition implies that the regression line \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) equals \\(\\overline{y}\\) when \\(x = \\overline{x}\\). Thus, the regression line must go through the point \\((\\overline{x}, \\overline{y})\\) or “the point of averages”. The figure below shows a regression line that goes through the point of averages. Theorem 9.2 We can rearrange the identity \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) from Theorem 9.1 to obtain the identity \\(\\hat{\\alpha} = \\overline{y} - \\hat{\\beta}\\overline{x}\\) 9.4.3.1.2 The 2nd First-Order Condition Sometimes, when writing proofs, you obtain a result that’s not particularly interesting, but true and useful later. We refer to these results as “lemmas.” We’ll need the following Lemmas in the subsequent steps. Lemma 9.1 \\(\\sum_{i = i}^n y_i = n\\overline{y}\\). Exercise 9.5 Prove Lemma 9.1. A personal perspective on proofs: In my experience, proofs are not intuitive. Sometimes I have a sneaking suspicion about a result, but sometimes that sneaking suspicion is wildly wrong. When I investigate a the suspicion analytically, the path to the result is unclear. A maze analogy works quite well. To obtain the result, just move things around, sometimes getting further from the result. Eventually, you just happen upon the correct sequence of movements. Lemma 9.2 \\(\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y} = \\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\). Exercise 9.6 Prove Lemma 9.2. Lemma 9.3 \\(\\sum_{i = 1}^n x_i ^2 - n \\overline{x}^2 = \\sum_{i = 1}^n (x_i - \\overline{x})^2\\). Exercise 9.7 Prove Lemma 9.3. \\[\\begin{align} -2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)x_i &amp;= 0 \\\\ \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)x_i &amp;= 0 \\text{ (divide both sides by -2)} \\\\ \\sum_{i = 1}^n(y_i x_i - \\hat{\\alpha}x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (distribute the $x_i$)} \\end{align}\\] Now we can use use Theorem 9.2 and replace \\(\\hat{\\alpha}\\) with \\(\\overline{y} - \\hat{\\beta}\\overline{x}\\). \\[\\begin{align} \\sum_{i = 1}^n(y_i x_i - (\\overline{y} - \\hat{\\beta}\\overline{x})x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (use the identity $\\hat{\\alpha} = \\overline{y} - \\hat{\\beta}\\overline{x}$)} \\\\ \\sum_{i = 1}^n( x_i y_i - \\overline{y} x_i + \\hat{\\beta}\\overline{x} x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (expand the middle term)} \\\\ \\sum_{i = 1}^n x_i y_i - \\overline{y} \\sum_{i = 1}^n x_i + \\hat{\\beta}\\overline{x} \\sum_{i = 1}^n x_i - \\hat{\\beta} \\sum_{i = 1}^n x_i^2 &amp;= 0 \\text{ (distribute the sum)} \\end{align}\\] Now we can use Lemma 9.1 to replace \\(\\sum_{i = i}^n y_i\\) with \\(n\\overline{y}\\). \\[\\begin{align} \\sum_{i = 1}^n x_i y_i - n \\overline{y} \\overline{x} + \\hat{\\beta}n \\overline{x}^2 - \\hat{\\beta} \\sum_{i = 1}^n x_i^2 &amp;= 0 \\text{ (use the identity $\\sum_{i = i}^n y_i = n\\overline{y}$)}\\\\ \\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y} &amp;= \\hat{\\beta} \\left(\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2 \\right) \\text{ (rearrange)}\\\\ \\hat{\\beta} &amp;=\\dfrac{\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2} \\text{ (rearrange)}\\\\ \\end{align}\\] This final result is correct, but unfamiliar. In order to make sense of this solution, we need to connect this identity to previous results. Now we can use Lemmas 9.2 and 9.3 to replace the numerator \\(\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}\\) with the more familiar expression \\(\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\) and replace the denominator \\(\\sum_{i = 1}^n x_i ^2 - n \\overline{x}^2\\) with the more familiar expression \\(\\sum_{i = 1}^n (x_i - \\overline{x})^2\\). \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2} \\\\ \\end{align}\\] Denote the SD of \\(x\\) as \\(\\text{SD}_x\\) and the the SD of \\(y\\) as \\(\\text{SD}_y\\). Multiply the top and bottom by \\(\\frac{1}{n \\times \\text{SD}_x^2 \\times \\text{SD}_y}\\) and rearrange strategically. \\[\\begin{align} \\hat{\\beta} &amp;=\\frac{\\frac{\\sum_{i = 1}^n \\left(\\frac{x_i - \\overline{x}}{\\text{SD}_x} \\right)\\left(\\frac{y_i - \\overline{y}}{\\text{SD}_y} \\right)}{n} \\times \\frac{1}{\\text{SD}_x}}{ \\frac{1}{n \\times \\text{SD}_x^2 \\times \\text{SD}_y} \\times \\sum_{i = 1}^n (x_i - \\overline{x})^2} \\\\ \\end{align}\\] Now we recognize that the left term \\(\\dfrac{\\sum_{i = 1}^n \\left(\\frac{x_i - \\overline{x}}{\\text{SD}_x} \\right)\\left(\\frac{y_i - \\overline{y}}{\\text{SD}_y} \\right)}{n}\\) in the numerator is simply the correlation coefficient \\(r\\) between \\(x\\) and \\(y\\). \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{r \\times \\frac{1}{\\text{SD}_x}}{\\frac{1}{\\text{SD}_x^2 \\times \\text{SD}_y}\\sum_{i = 1}^n \\frac{(x_i - \\overline{x})^2}{n}} \\\\ \\end{align}\\] Now we recognize that \\(\\sum_{i = 1}^n \\frac{(x_i - \\overline{x})^2}{n}\\) is almost the \\(\\text{SD}_x\\). Conveniently, it’s \\(\\text{SD}_x^2\\), which allows us to cancel those two terms. \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{r \\times \\frac{1}{\\text{SD}_x}}{\\frac{1}{\\text{SD}_y}} \\\\ &amp; r \\times \\frac{\\text{SD}_y}{\\text{SD}_x} \\end{align}\\] This final result clearly connects \\(\\hat{\\beta}\\) to previous results. Theorem 9.3 \\(\\hat{\\beta} = r \\times \\dfrac{\\text{SD of } y}{\\text{SD of }x} = \\dfrac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2} = \\dfrac{\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2}\\). In summary, we can obtain the smallest RMS of the residuals with results from Theorems 9.2 and 9.3. \\[\\begin{align} \\hat{\\beta} &amp;= r \\times \\dfrac{\\text{SD of } y}{\\text{SD of }x} \\\\ \\hat{\\alpha} &amp;= \\overline{y} - \\hat{\\beta}\\overline{x} \\end{align}\\] Exercise 9.8 Other authors develop this least-squares approach using slightly different language and notation. Read FPP, ch. 12. Set A, p. 207: 1, 2, 3, 4. Set B, p. 210: 1, 2. Section 4, p. 213: 1-5, 8. Read Fox (2008), pp. 77-86. Do p. 96, q. 5.2. Read Wooldridge (2013), pp. 27-35. Do pp. 60-61, q. 3 (i, ii, and iii) and q. 6. Read DeGroot and Schervish (2012), pp. 689-692. Exercise 9.9 [HARD] Do Fox (2008), p. 97, q. 5.4. 9.4.3.2 Matrix Form In some cases, a matrix approach might help analytically or numerically compared to the scalar approach. Rather than writing the model as \\(y_i = \\alpha + \\beta x_i + r_i\\), we can write the model in an equivalent matrix form \\[\\begin{align} y &amp;= X\\beta + r \\\\ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_1\\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix} \\times \\begin{bmatrix} \\beta_1 \\\\ \\beta_2\\\\ \\end{bmatrix} + \\begin{bmatrix} r_1\\\\ r_2\\\\ \\vdots\\\\ r_n \\end{bmatrix} . \\end{align}\\] In this case, our intercept \\(\\alpha\\) and slope \\(\\beta\\) are combined into a single vector \\(\\beta = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2\\\\ \\end{bmatrix}\\), where \\(\\beta_1\\) represents the intercept and \\(\\beta_2\\) represents the slope. Exercise 9.10 Show that the scalar representation \\(y_i = \\alpha + \\beta x_i + r_i\\) and the matrix formulation \\(y = X\\beta + r\\) are equivalent. Hint Do the matrix multiplication \\(X\\beta\\) and show that \\(y_i = \\beta_1 + \\beta_2x + r_i\\). Theorem 9.4 In matrix form we can combine Theorems 9.2 and 9.3 and compute the slope and intercept as \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). Proof See Fox (2008), pp. 192-193 9.5 The RMS of the Residuals Just like the SD offers a give-or-take number around the average, the RMS of the residuals offers a give-or-take number around the regression line. Indeed, the SD is the RMS of the deviations from the average and the RMS of the residuals is the RMS of the deviations from the regression line. The RMS of the residuals tells us how far typical points fall from the regression line. Sometimes the RMS of the residuals is called the “RMS error (of the regression),” the “standard error of the regression,” or denoted as \\(\\hat{\\sigma}\\). We can compute the RMS of the regression by computing each residual and then taking the root-mean-square. But we can also use the much simpler formula \\(\\sqrt{1 - r^2} \\times \\text{ SD of }y\\). This formula makes sense because \\(y\\) has an SD, but \\(x\\) explains some of that variation. As \\(r\\) increases, \\(x\\) explains more and more of the variation. As \\(x\\) explains more variation, then the RMS of the residuals shrinks away from SD of \\(y\\) toward zero. It turns out that the SD of \\(y\\) shrinks toward zero by a factor of \\(\\sqrt{1 - r^2}\\). Exercise 9.11 Read FPP, ch. 11. Do the following exercises. Set A, p. 184: 1-4, 6, 7. Set B, p. 187: 1, 2. Set C, p. 189: 1-3. Set D, p. 193: 1, 2, 4-6. Set E, p. 197: 1, 2. Section 6, p. 198: 1, 2, 4, 6, 7,10, 12. 9.6 \\(R^2\\) Some authors use the quantity \\(R^2\\) to assess the fit of the regression model. I prefer the RMS of the residuals because it’s on the same scale as \\(y\\). Also, \\(R^2\\) computes the what fraction of the variance of \\(y\\), which is the SD squared, is explained by \\(x\\). I have a hard time making sense of variances, because they are not on the original scale. However, \\(R^2\\) is a common quantity, so do the following exercises. Exercise 9.12 Read Lewis-Beck (1980), pp. 20-25. Read Wooldridge (2013), pp. 36-39. Do pp. 60-61, q. 3 (part iv) 9.6.1 Adequacy of a Line In some cases, a line can describe the average value \\(y\\) quite well. In other cases, a line describes the data poorly. Remember, the regression line describes the average value of \\(y\\) for different values of \\(x\\). In the figure below, the left panel shows a dataset in which a line does not (and cannot) adequately describe the average values of \\(y\\) for describe low, middle, and high values (at least ad the same time). The right panel shows a data in which a line can adequately describe how the average value of \\(y\\) changes with \\(x\\). We can see that when \\(x \\approx -2\\), then \\(y \\approx 0\\). Similarly, when \\(x \\approx 0\\), then \\(y \\approx 4\\). A line can describe the average value of \\(y\\) for varying values of \\(x\\) when the average of \\(y\\) changes linearly with \\(x\\). A line does a great job of describing the relationship between seat shares and portfolio shares in government coalitions. A line poorly describes the relationship between Polity IV’s DEMOC measure and GDP per capita. When we have variable that’s skewed heavily to the right, we can sometimes more easily describe the log of the variable. For this dataset, the line poorly describes the average logged GDP per capita for the various democracy scores. 9.7 Fitting Regression Models To fit a regression model in R, we can use the following approach: Use lm() to fit the model. Use coef(), arm::display(), texreg::screenreg(), or summary() to quickly inspect the slope and intercept. Use glance(), tidy(), and augment() functions in the broom package to process the fit more thoroughly. 9.7.1 geom_smooth() In the context of ggplot, we can show the fitted line with geom_smooth(). gamson &lt;- read_rds(&quot;data/gamson.rds&quot;) ggplot(gamson, aes(x = seat_share, y = portfolio_share)) + geom_point() + geom_smooth() By default, geom_smooth() fits a smoothed curve rather than a straight line. There’s nothing wrong with a smoothed curve—sometimes it’s preferable to a straight line. But we don’t understand how to fit a smoothed curve. To us the least-squares fit, we supply the argument method = \"lm\" to geom_smooth(). geom_smooth() also includes the uncertainty around the line by default. Notice the grey band around the line, especially in the top-right. We don’t have a clear since of how uncertainty enters the fit, nor do we understand a standard error, so we should not include the uncertainty in the plot (at least for now). To remove the grey band, we supply the argument se = FALSE to geom_smooth(). The line \\(y = x\\) is theoretically relevant–that’s the line that indicates a perfectly proportional portfolio distribution. To include it, we can use geom_abline(). ggplot(gamson, aes(x = seat_share, y = portfolio_share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;) 9.7.2 lm() The lm() function takes two key arguments. The first argument is a formula, which is a special type of object in R. It has a left-hand side and a right-hand side, separated by a ~. You put the name of the outcome variable \\(y\\) on the LHS and the name of the explanatory variable \\(x\\) on the RHS. The second argument is the dataset. fit &lt;- lm(portfolio_share ~ seat_share, data = gamson) 9.7.3 Quick Look at the Fit We have several ways to look at the fit. Experiment with coef(), arm::display(), texreg::screenreg(), and summary() to see the differences. For now, we only understand the slope and intercept, so coef() works perfectly. coef(fit) (Intercept) seat_share 0.06913558 0.79158398 The coef() function outputs a numeric vector with named entries. The intercept is named (Intercept) and the slope is named after its associated variable. 9.7.4 Post-Processing The intercept and slope have a nice, intuitive interpretation so it’s tempting to just examine those values and call it quits. That’s a mistake, and it’s an especially bad habit to carry into richer models. The broom package offers three useful tools to explore the fit in more detail. The core of broom contains three functions: glance(), tidy(), and augment(). broom works nicely in our usual workflow because it produces a data frame containing information about the fit. These functions work for a wide range of models, so you can use this workflow as models become richer. 9.7.4.1 glance() glance() produces a one-row data frame that contains summaries of the model fit. fit_summary &lt;- glance(fit) %&gt;% glimpse() Rows: 1 Columns: 12 $ r.squared &lt;dbl&gt; 0.8879625 $ adj.r.squared &lt;dbl&gt; 0.8878265 $ sigma &lt;dbl&gt; 0.06889308 $ statistic &lt;dbl&gt; 6530.681 $ p.value &lt;dbl&gt; 0 $ df &lt;dbl&gt; 1 $ logLik &lt;dbl&gt; 1038.673 $ AIC &lt;dbl&gt; -2071.346 $ BIC &lt;dbl&gt; -2057.196 $ deviance &lt;dbl&gt; 3.910916 $ df.residual &lt;int&gt; 824 $ nobs &lt;int&gt; 826 We don’t understand many of these (yet, see ?glance.lm for the definitions), so let’s select() only those we understand. fit_summary &lt;- glance(fit) %&gt;% select(r.squared, sigma) %&gt;% # this used to have nobs, but not any more glimpse() Rows: 1 Columns: 2 $ r.squared &lt;dbl&gt; 0.8879625 $ sigma &lt;dbl&gt; 0.06889308 r.squared is the \\(R^2\\) statistic. sigma is the RMS of the residuals. nobs is the number of observations (see note above). 9.7.4.2 tidy() tidy() produces a usually several-row data frame that contains summaries of the “components” of the model. The “components” are usually the main focus of the model. In the case of an lm() fit, it’s the intercept and slope(s). fit_components &lt;- tidy(fit) %&gt;% glimpse() Rows: 2 Columns: 5 $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;seat_share&quot; $ estimate &lt;dbl&gt; 0.06913558, 0.79158398 $ std.error &lt;dbl&gt; 0.004037815, 0.009795300 $ statistic &lt;dbl&gt; 17.12203, 80.81263 $ p.value &lt;dbl&gt; 1.865252e-56, 0.000000e+00 You can see that tidy gives us one row per coefficient (one for intercept and one for slope). Again, there are several columns we don’t understand (yet, see ?tidy.lm for the definitions), so let’s select() the rows we know. fit_components &lt;- tidy(fit) %&gt;% select(term, estimate) %&gt;% glimpse() Rows: 2 Columns: 2 $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;seat_share&quot; $ estimate &lt;dbl&gt; 0.06913558, 0.79158398 term contains the name of the coefficient. estimate contains the estimate for that coefficient. This gives us the same information as coef(fit), except in a data frame rather than a named vector. The data frame is more convenient for computing and plotting. For example, we can plot the coefficients. ggplot(fit_components, aes(x = estimate, y = term)) + geom_point() You can see that this sort of plot might be useful for more complicated models. We might have similar models that we want to compare, or the same model fit to different subsets of data. 9.7.4.3 augment() Lastly, augment() creates a data frame with information about each observation. obs_fit &lt;- augment(fit) %&gt;% glimpse() Rows: 826 Columns: 8 $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ .fitted &lt;dbl&gt; 0.0883255, 0.4337440, 0.4769213, 0.4428026, 0.4870526… $ .resid &lt;dbl&gt; 0.002583595, -0.070107594, 0.068533303, 0.011742918, … $ .std.resid &lt;dbl&gt; 0.03756019, -1.01841671, 0.99571954, 0.17058860, 0.84… $ .hat &lt;dbl&gt; 0.003121865, 0.001546470, 0.001890853, 0.001608751, 0… $ .sigma &lt;dbl&gt; 0.06893487, 0.06889153, 0.06889344, 0.06893371, 0.068… $ .cooksd &lt;dbl&gt; 2.209010e-06, 8.032205e-04, 9.391259e-04, 2.344542e-0… Again, there are several columns we don’t understand (yet, see ?augment.lm for the definitions), so let’s select() the rows we know. obs_fit &lt;- augment(fit) %&gt;% select(portfolio_share:.resid) %&gt;% glimpse() Rows: 826 Columns: 4 $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ .fitted &lt;dbl&gt; 0.0883255, 0.4337440, 0.4769213, 0.4428026, 0.4870526… $ .resid &lt;dbl&gt; 0.002583595, -0.070107594, 0.068533303, 0.011742918, … The couple of variables are the \\(x\\) and \\(y\\) from the original dataset used to fit the model. .fitted is the fitted or predicted value that we denote as \\(\\hat{y}\\) .resid is the residual or the difference between the predicted and observed value. Now we have an important data set for assessing the fit of the model. Does it describe the data well? Poorly? We can use augment()ed dataset to create a plot of the fitted/predicted values versus the residuals. ggplot(obs_fit, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;) Exercise 9.13 Using the democracy-life dataset from here, fit the regression model \\(\\text{GDP per captia} = \\alpha + \\beta \\times \\text{DEMOC}\\). Use glance(), tidy(), and augment() to obtain data frames with the information of the fit. Create a fitted versus residual plot and use to to make a judgement about the fit of the model to the data. Review FPP, pp. 187-192 for more about plotting the fitted values verus the residuals. Solution dem_life &lt;- read_csv(&quot;https://raw.githubusercontent.com/pos5737/democracy-life/master/data/democracy-life.csv&quot;) %&gt;% glimpse() Rows: 151 Columns: 8 $ year &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016,… $ country_name &lt;chr&gt; &quot;United States&quot;, &quot;Canada&quot;, &quot;Cuba&quot;, &quot;Dominican Republi… $ wb &lt;chr&gt; &quot;USA&quot;, &quot;CAN&quot;, &quot;CUB&quot;, &quot;DOM&quot;, &quot;JAM&quot;, &quot;TTO&quot;, &quot;MEX&quot;, &quot;GTM… $ cown &lt;dbl&gt; 2, 20, 40, 42, 51, 52, 70, 90, 91, 92, 93, 94, 95, 10… $ life_expectancy &lt;dbl&gt; 78.53902, 82.14224, 78.60700, 73.47100, 74.17500, 73.… $ gdp_per_capita &lt;dbl&gt; 52534.365, 50263.834, 6550.274, 7084.627, 4753.705, 1… $ democ &lt;dbl&gt; 8, 10, 0, 8, 9, 10, 8, 9, 7, 8, 7, 10, 9, 7, 8, 6, 6,… $ autoc &lt;dbl&gt; 0, 0, 7, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,… fit &lt;- lm(gdp_per_capita ~ democ, data = dem_life) glance(fit) # A tibble: 1 x 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.0946 0.0886 18826. 15.6 1.22e-4 1 -1700. 3405. 3414. # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; tidy(fit) # A tibble: 2 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3869. 2984. 1.30 0.197 2 democ 1659. 420. 3.95 0.000122 obs_df &lt;- augment(fit) %&gt;% glimpse() Rows: 151 Columns: 8 $ gdp_per_capita &lt;dbl&gt; 52534.365, 50263.834, 6550.274, 7084.627, 4753.705, 15… $ democ &lt;dbl&gt; 8, 10, 0, 8, 9, 10, 8, 9, 7, 8, 7, 10, 9, 7, 8, 6, 6, … $ .fitted &lt;dbl&gt; 17138.532, 20455.803, 3869.446, 17138.532, 18797.167, … $ .resid &lt;dbl&gt; 35395.834, 29808.031, 2680.829, -10053.905, -14043.462… $ .std.resid &lt;dbl&gt; 1.8881716, 1.5947603, 0.1442260, -0.5363201, -0.750048… $ .hat &lt;dbl&gt; 0.008435309, 0.014230460, 0.025121117, 0.008435309, 0.… $ .sigma &lt;dbl&gt; 18661.80, 18727.25, 18887.83, 18870.91, 18853.46, 1888… $ .cooksd &lt;dbl&gt; 0.0151646668, 0.0183571132, 0.0002680066, 0.0012234835… ggplot(obs_df, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;) 9.8 Standard Errors and p-Values Almost all software reports p-values or standard errors by default. At this point, you should ignore these. In order to use these, you must be able to: Define a p-value. It’s the probability of…? Define a sampling distribution. Describe how randomness noise makes it’s way into your fitted coefficients. We’ll take a lot of care with (1) and (2). For most applications, it isn’t at all immediately clear how randomness enters the data (if at all). That said, regression is a powerful tool for description. Use it often. 9.9 A Warning When we simply describe a single set of measurements with a histogram or an average, then we intuitively remain in the world of description. Indeed, making a causal claim requires a comparing factual and counterfactual scenarios. In the case of a single histogram or average we only have one scenario and there is no comparison. When we have descriptions of multiple sets of measurements, say the average life expectancy for democracies and the average life expectancy for autocracies, we an easily interpret one scenario as factual and the other as counterfactual. On its face, though, both scenarios are factual. We can comfortably say that democracies have healthier populations than autocracies without claiming that the regime type causes this difference. But it is… oh. so. tempting. Regression models, by design, describe an outcome across a range of scenarios. Indeed, a regression model describes how the average value of \\(y\\) changes as \\(x\\) varies. The temptation to treat these neatly arranged scenarios as factual and counterfactual grows even stronger. But unless one makes a strong argument otherwise, statistical models describe the factual world. With few exceptions, statistical data analysis describes the outcomes of real social processes and not the processes themselves. It is therefore important to attend to the descriptive accuracy of statistical models, and to refrain from reifying them. —Fox (2008, p.3) Note that some methodologists claim that their statistical models can obtain estimates of the causal effects. These models might actually succeed on occasion. However, the researcher should carefully avoid seeing counterfactual worlds from regression models. Usually, credible causal inferences come from careful design in the data collection stage, not from complicated conditioning at the modeling stage. Exercise 9.14 Read Berk’s \" What You Can and Can’t Properly Do with Regression\" [pdf]. This is an easy reading and nicely wraps up our discussion of description and previews our future discussion of random sampling. 9.10 Review Exercises Exercise 9.15 Use devtools::install_github(\"pos5737/pos5737data\") to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737data\"). Use glimpse(anscombe) to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. Fit a regression model on each “dataset” in the anscombe dataset. To only use a subset of the dataset, you can filter() the dataset before supplying the data to lm() or use can supply the subset argument to lm(). In this case, just supplying subset = dataset == \"I\", for example, is probably easiest. Fit the regression to all four datasets and put the intercept, slope, RMS of the residuals, and number of observations for each regression in a little table. Interpret the results. For each of the four regression fits, create a scatterplot of the fitted values versus the residuals. Describe any inadequacies. Exercise 9.16 Use regression to test Clark and Golder’s (2006) theory. First, create scatterplots between ENEG and ENEP faceted by the electoral system with with the least-squares fit included in each. Then fit three separate regression models. Fit the model \\(\\text{ENEP}_i = \\alpha + \\beta \\text{ENEG}_i + r_i\\) for SMD systems, small-magnitude PR systems, and large-magnitude PR systems. Include the intercept, slope, and RMS of the residuals from each fit in a little table. Explain the results. For each regression, assess the fit using a scatterplot of the fitted values versus the residuals. Explain any inadequacies. Exercise 9.17 Use a regression model (fit in R) to assess the question in Exercise 9.1. Briefly discuss the strengths and weaknesses of each approach. Histogram Average and/or SD Scatterplot Correlation Coefficient Regression What’s the best approach (or combination of approaches)? Exercise 9.18 This continues Exercise 9.1. Get the economic-model CSV dataset from GitHub. In three separate regressions, use GDP, RDI, and unemployment to explain the incumbent’s margin of victory. Which measure of economic performance best describes incumbents’ vote shares? Using the best model of the three, which incumbents did much better than the model suggests? Which incumbents did much worse? Use tables and figures wisely to answer the questions above. "],
["references.html", "References", " References "]
]
