[
["index.html", "Concepts and Computation: An Introduction to Political Methodology Chapter 1 Overview", " Concepts and Computation: An Introduction to Political Methodology Carlisle Rainey 2020-08-20 Chapter 1 Overview "],
["tools.html", "Chapter 2 Tools 2.1 My Tools 2.2 Reading 2.3 Slack 2.4 GitHub 2.5 Ask a Question 2.6 Set Up Your Computer for Computing 2.7 R and RStudio 2.8 Text Editor 2.9 Git 2.10 LaTeX 2.11 R Markdown 2.12 GNU Make 2.13 Test a Complete Workflow 2.14 If It Goes Smoothly…", " Chapter 2 Tools Modern social science relies heavily on computational tools (i.e., software) to implement the ideas we will learn this semester. In my view, concepts (i.e., knowledge statistical theory) and computation (i.e., understanding how to use statistical software) are both critical. You cannot become a competent and capable empirical research without both. If you find a problem with this assignment, then please start a discussion in our Slack workspace, especially if the instructions don’t work or you find them confusing. This is important. This assignment can be particularly frustrating, so if the instructions are unclear, inaccurate, or incomplete, let me know so that I can update them and save others some frustration. This chapter walks you through installing a bunch of software, creating a couple of accounts, and making sure things work as expected. You don’t need to understand everything that’s happening. Unless you bring some outside know, you’ll sometimes have no ideas what’s happening. That’s expected and okay. You’ll learn more about these tools later in the semester. 2.1 My Tools Here are the tools I use (with macOS): RStudio for data management and manipulation (e.g., tidyverse), plotting (e.g., ggplot2), and modeling (e.g., rstan). LaTeX through TeXShop (for documents), BibDesk (for bibliographies), and LaTeXiT (for single equations). R Markdown for simple reports (e.g., peer reviews) git and GitHub through GitHub Desktop for version control. GNU Make for defining dependencies and reproducing projects. You can obtain the same properties more easily (but sacrifice a bit of power), but combining text and code in an R Markdown document. If you get an error following the steps below, please post to Slack so that I have a record of your difficultly, even if you figure it out relatively quickly. That helps me maintain high-quality materials that work for users with a range of abilities. Definitely seek help before you get frustrated. This is not a course in installing software–we just need to get it working for you. 2.2 Reading Read the following: Healy. 2014. Choosing Your Workflow Applications. [pdf]. Note that RStudio and GitHub Desktop are sufficient for data analysis, document preparation, and version control in this course. I do not use Emacs, which the author recommends. I recommend reading the following: Healy. 2018. The Plain Person’s Guide to Plain Text Social Science. [pdf] Bryan. 2018. “Excuse Me, Do You Have a Moment to Talk About Version Control?” [pdf] Rainey. “Bare Basics Introduction to GNU Make.” [web] 2.3 Slack Choose an e-mail address that you’d like to associate with your Slack account. I recommend using an e-mail account that you have control over (e.g., gmail), because you’ll eventually lose your FSU e-mail account. (It seems that Slack does not allow you to associate multiple e-mail addresses with the same account.) Just e-mail me (crainey@fsu.edu the address you prefer, and I’ll invite you. Register a Slack account. I’ll send you an invitation to join our workspace. Accept the invite and fill out your profile, including (i) a profile picture and (ii) the “What I do” box. Familiarize yourself with Slack. We use Slack as an alternative to e-mail and Canvas because (i) it’s better and (ii) it’s a tool you might encounter in industry. They have some simple introductory videos and an excellent help center. I might check my e-mail once per day. I am on Slack whenever I’m working. Consider getting the desktop and mobile apps. They allow you to fine-tune notifications. For example, you can turn off notifications from 8pm to 8am. Try out the #playground. I created the #playground channel to experiment. Go there and try out a few features that you want to read about. Try something fun (/giphy [emotion here]) and something more serious. Think about integrations. If you are a productivity ninja that uses services like Evernote and Todoist, you can integrate those with Slack. 2.4 GitHub Register a GitHub account. Read the advice from Jenny Bryan about choosing a username and then use your .edu e-mail address to sign up for an account on github.com. You can associate multiple e-mail accounts with GitHub, so feel free to add in the primary account that you have permanent control over. Apply for a Student Developer Pack. This gives you private repos for free. Follow me on GitHub. I’m carlislerainey. Follow [TA]. Once you follow me, I’ll add you to our POS 5737 or (POS 3713 - Research Intensive) private GitHub Organization. Browse some repos on GitHub. See an ongoing project of mine of mine, a finished project of mine, and Josh Alley’s cool dissertation project (that I made a small contribution to). Remember that most GitHub repos contain software, not research projects. For examples, see the Linux OS, my website, and the ggplot2 R package. 2.5 Ask a Question At some point in this assignment (not necessary now), find something that you don’t completely understand. Start a discussion in our Slack workspace and ask your question. Also, respond to someone else’s question. This should be an actual question (be aware of what you don’t understand) and an actual response to someone else’s question (share what you do understand). Maybe a question about the course? Syllabus? Software? 2.6 Set Up Your Computer for Computing To make things a little easier throughout the process, we want to do four things: Show hidden files. Show file extensions. Make it easy to open a terminal at a folder. Make your terminal look nice (on macOS). 2.6.1 Show Hidden Files Typically, files that start with ., like .gitignore, are hidden from the user’s view. Hidden files, like .DS_Store and .git, are hidden for a reason, but you’ll sometimes want to access them, especially .gitignore. On Windows, see the instructions here. On macOS, simply press command + shift + . to show these hidden files. This shows hidden files until you restart finder (e.g., log off, restart your computer, etc). If you don’t want to always see hidden files, this offers a convenient toggle. If you always want to see hidden files, then you can run defaults write com.apple.finder AppleShowAllFiles YES in a terminal. You can still use the toggle, but Finder now defaults to showing hidden files. 2.6.2 Show File Extensions For reasons I don’t understand, neither macOS nor Windows shows file extensions by default. (That’s the .docx bit of the Word file Essay for English 1101.docx.) On Windows, see the instructions here. On macOS, open Finder. Click Finder &gt; Preferences… Select Advanced tab and check the box to Show all filename extensions. 2.6.3 Enable the “New Terminal at Folder” on macOS On Windows 7/8/10, you don’t need to do anything. When you need a command line at a folder, simply hold down the shift key and right-click a folder. The context menu contains an entry, Open command window here. (Later, once you’ve installed Git for Windows, you’ll have the option to open a Git Bash terminal as well.) On macOS, make it easy to open a terminal in a specific directory by enabling New Terminal at Folder. Open Finder. Click Finder &gt; Services &gt; Services Preferences…. In the right box, navigate past Pictures, Messaging, and Development to Files and Folders. Check the box for New Terminal at Folder. Now right-click on a folder and you should see the option (near the bottom) to open a new terminal at that folder. 2.6.4 Customize Your Terminal Prompt on macOS This is for macOS only. The command prompt on macOS doesn’t look nice by default. You probably don’t have the file ~/.bash_profile, but you need to check. Open a terminal and run the command open -a TextEdit ~/.bash_profile. If you don’t have it, you’ll get a message saying so. If you already have it, just add the line export PS1=\"\\W &gt; \" to the file on a new, separate line. If you don’t have it, then you need to create it. Run echo 'export PS1=\"\\W &gt; \"' &gt; ~/.bash_profile. Open a new terminal. The prompt should look nicer. 2.7 R and RStudio Install (or Update) R. Choose the appropriate OS from CRAN and follow the instructions. On Windows, you get two versions of R: i386 and x64. This is normal. These are 32- and 64-bit versions We only use R indirectly through RStudio, so we never choose between the two. (But both work!) Install (or Update) RStudio. You may choose either the preview version or the latest stable version. I use the preview version. The lab computers have the latest stable version. The preview version has new features; the latest stable version is (I suppose) more robust. Choose a version, select your OS, and follow the instructions. If you already have R installed, update your packages if you already had R installed. Just open R or RStudio and run update.packages(ask = FALSE, checkBuilt = TRUE). Adjust One (Bad) Default. Click Tools &gt; Global Options…. Select General. Under Workspace, set Save workspace to .RData on exit: to Never. Uncheck the box for Restore .RData into workspace at startup. Install the tidyverse package. In the lower-right pane, click the Packages tab to show the Packages window. You see a list of available packages. Click the Install button at the top of the Packages window, type “tidyverse” in the middle box, and click Install. Make sure that tidyverse installs successfully by entering the command library(tidyverse) in the console in the lower-left pane. Comment: Other instructors might suggest the equivalent approach of entering the command install.packages(\"tidyverse\") in the console, but I recommend the point-and-click method. It’s easier and packages only need to be installed once per computer. Comment: R packages allow software developers to distribute additional functionality to users. For example, you’ll use the ggplot2 package to create graphs. Hadley Wickham wrote ggplot2 as part of his dissertation in the statistics department at Iowa State. That was version 0.0.7 and we’re now on version 3.0.0. I’m excited for the next release because of this annoying bug. Comment: When you run library(tidyverse), you get the output below, which is both expected and desirable. library(tidyverse) ## ── Attaching packages ────────────── ## ✓ ggplot2 3.3.1 ✓ purrr 0.3.4 ## ✓ tibble 3.0.1 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Run an R script. To start a new R script, open RStudio and click File &gt; New File &gt; R Script. This opens a blank R script in the upper-left pane. Copy-and-paste the code below into the R script. Use your mouse to highlight all eight lines (or press command + a) and then click Run (or press command + enter) at the upper-right of the script window. Make sure a scatterplot appears in the Plot tab of the lower-right pane. If so, you’ve got R and RStudio working. # simple example script # load packages library(tidyverse) # create two vectors of data x &lt;- c(2, 3, 5, 4, 1) # x &quot;gets&quot; a &quot;collection&quot; of 5 numbers y &lt;- c(4, 2, 1, 5, 6) # y &quot;gets&quot; a &quot;collection&quot; of 5 numbers # plot x and y qplot(x, y) 2.8 Text Editor If you already have a favorite, that’s fine. Use it. For Windows, I recommend downloading Sublime Text 3. Follow the instructions. For macOS, I recommend downloading Atom. Follow the instructions. 2.9 Git 2.9.1 Setup Git Install git. For macOS, open a terminal and run xcode-select --install. This installs Xcode command line tools, which includes git. (Note that you can also install all of Xcode, but all of Xcode is a much larger installation. You only need Xcode command line tools.) For Windows, download Git for Windows and follow the instructions. Install GitHub Desktop. Download GitHub Desktop and follow the instructions. Comment: There are three ways you can interact with git: (1) command line, (2) RStudio, and (3) a client like GitHub Desktop. RStudio gives you an easy, but limited, way to interact with git. The command line is complete, but complex. GitHub Desktop is the best of both worlds. Introduce yourself to git. (Only do this on your personal computer.) At some point, you’ll want to use git from the terminal/Git Bash. Go ahead and let git know who you are. In a terminal (macOS) or Git Bash (Windows), run git config --global user.name 'Jane Doe' and git config --global user.email 'jdoe@fsu.edu'–be sure to use the e-mail associated with your GitHub account. Run git config --global --list to verify the settings. Associate your text editor with git. For macOS, run git config --global core.editor \"atom --wait\" in a terminal. For Windows, run git config --global core.editor \"'c:/program files/sublime text 3/subl.exe' -w\" in Git Bash. Comment: When using git from the command line, it sometimes pops open a text editor. By default, git uses vim. Vim is a strange, scary place to wind up. If you find yourself in vim, you might never escape. Avoid that possibility entirely by associating a friendly text editor with git. 2.9.2 Use Git with GitHub There are three similarly-named things here: git: that’s the underlying version-control software. GitHub Desktop: that’s the visual interface that you use to interact with git. GitHub: that’s the website that allows you to easily browse repos’ histories and share work. 2.9.2.1 Start a Repo with GitHub Desktop Open GitHub Desktop. Click File, New Repository…. Name the repo test-github and set the local path to your computer’s desktop. Initialize the repo with a README. Click the blue Create repository button. This first repo is just for practice–you’ll delete it later. Click the button Publish repository at the top. Supply your GitHub username and password. Again, lick the button Publish repository. (Note that the local directory and the GitHub repo do not necessarily share their name, but I recommend you keep them the same.) Click the blue Publish repository button. Comment: In my work, I associate one repo with a single research project (i.e., usually one manuscript). See my repos. Throughout the semester, you’ll associate one single repo for all homework assignments (i.e., 1 total, not 15) and one repo for the research project. 2.9.2.2 Change-Review-Commit-Push When students first start to develop projects with git and GitHub, I recommend a change-review-commit-push model. Choose a manageable change you’d like to make to the project (like “Rewrite the introduction” or “Add code to make scatterplot”), then make, review, commit, and push the change. Git and GitHub are extremely complicated, but powerful tools. Don’t jump all in on git and GitHub immediately (and maybe not ever). Keep it simple for a while with change-review-commit-push. First, edit the file README.md locally. You should find a new folder test-github on the Desktop. Open the file README.md with Atom (macOS) or Sublime Text 3 (Windows). Make some change to the file and save it. Comment: GitHub treats the file README.md specially–it displays it on the repo’s main page. I have a detailed README.md for my latent-dissent project, for example. Because README.md is a Markdown file, you can use Markdown syntax to style the document. Second, review your change. Go back to GitHub Desktop, which shows you a list of changed files. This list should include README.md because you changed it. This list should only include README.md because it’s the only file you changed (it’s also the only file in the repo). If you click the entry for README.md, you can see the changes you made to that file. Third, commit your change. (Remember, a commit is like a snapshot of your files.) Once you’re happy with the changes, you want to commit those changes. Ideally, a commit is a three step process: Stage the files. Check the boxes next to the changed files you want to commit. Write a commit message. Type a short description of your change in the Summary box and a longer description in the Description box. This commit message reminds you what you did and why. I like Chris Beam’s discussion of a good commit message. Commit the change. Click the blue Commit to master button to commit the changes. Comment: Committing is like taking a snapshot of the directory. You already have one snapshot from when you initially created the repo on GitHub. When you made a change to README.md, you took another snapshot. This allows you to return to these exact files later, as needed. For an example, look at the commits for my recent paper in Political Analysis. Fourth, push these changes up to GitHub. Click Push origin in upper-right corner. This simply passes the update along to GitHub, which safely stores it for backup, browsing, and sharing. Comment: You’ve got three things going on now–keep them separate in your mind. You have the local files that you see on your computer. You have a local history of the project (each commit) stored in the file .git. You have a copy of the .git file hosted on GitHub for safe storage and others to access. Comment: If multiple users are pushing commits to the same repo on GitHub, then you need a workflow that pulls changes down from GitHub in addition to pushing them up. There are lots of options, but you need a strategy. It should involve pulling down and pushing up changes often. Lastly, confirm the local change propagated to the GitHub remote. In a web browser, navigate to your repo on GitHub. See that the changes you made to the README are there. 2.9.3 Do It Again Make another change. Make a local change and save it, then stage it, commit it, and push it. Maybe try adding a file. Delete. When you are done, delete your local directory and the GitHub repo. You can do this from GitHub Desktop by clicking Repository &gt; Remove. Check the box Also move to trash and click Remove. This stops GitHub Desktop from monitoring the local directory and moves the local directory to the trash. If you used a community computer, log out of GitHub Desktop when you are done. Be sure to both sign out of your GitHub.com account and remove your Name and Email from the Git tab. This keeps others from making changes as you (bad) and pushing to GitHub as you (worse). 2.9.4 Avoid typing your username and password every time. (Only do this on your personal computer!). Follow these instructions. 2.10 LaTeX 2.10.1 Install LaTeX For macOS, I recommend MacTeX. Download and follow the instructions. It includes the following: TeXShop: editor, used to write LaTeX files. BibDesk: reference manager, used to manage bibliography databases. LaTeXiT: used to create images of equations. For Windows, I recommend MiKTeX, TeXstudio, and JabRef. Download and follow the instructions. Hopefully, the installer asks you if you would like MiKTeX to install missing packages on-the-fly. Choose Yes rather than Ask Me. (See here for the details). In this setup, you have the following: TeXstudio: editor, used to write LaTeX files. JabRef: reference manager, used to manage bibliography databases. 2.10.2 Check LaTeX from RStudio Open RStudio. Click File &gt; New File &gt; Text File. Save as latex-test.tex. Create a new folder on the Desktop and save it there. Copy-and-paste the LaTeX code below into latex-test.tex. Click Compile PDF. Check that this creates the pdf you expect. Delete the pdf file. % a minimal latex document \\documentclass{article} \\begin{document} First document. If this compiles into a pdf, then LaTeX seems to work. \\end{document} 2.10.3 Check LaTeX from Your Editor Open latex-test.tex with your preferred editor. Right-click on latex-test.tex, select Open with, and choose your editor TeXShop (macOS) or TeXstudio (Windows). In TeXShop, click the Typeset button in the upper-left corner to compile into a pdf. In TeXstudio, click the green play button. Check that this creates the pdf you expect. Delete the .pdf file. At times, you might find it more convenient to develop .tex files from a dedicated editor like TeXShop for its features. Other times, you might choose to develop .tex files in RStudio to use a single interface. 2.11 R Markdown Open RStudio. Click File &gt; New File &gt; R Markdown. Install any suggested packages. Fill in the Title and Author boxes however you like. Make sure the HTML bubble is selected (the default). This initiates an R Markdown document that serves as a helpful template. Notice that the R Markdown file contains a mix of R code and Markdown syntax. Don’t sweat the details. Save the file as rmarkdown-test.Rmd to the Desktop. Click the Knit button to knit the .Rmd into an .html file, which you can view with a web browser. Check that you can also knit to a pdf (and maybe Word) document by clicking the tiny triangle next to the Knit button and then clicking Knit to PDF or Knit to Word. (If you don’t have Word, this won’t work. Don’t worry about it. You should be able to knit to pdf, because you have LaTeX working.) Sometimes, on Windows, R Markdown and MiKTeX will not work together properly. If R Markdown fails to generate the pdf, then let MiKTeX install missing LaTeX packages automatically. This might fix the problem. When you are sure that R Markdown works for you, delete rmarkdown-test.Rmd from the Desktop and all the associated files that knitr generated. 2.12 GNU Make For macOS, you already have Make installed (through Xcode command line tools). For Windows, download the latest Rtools and follow the instructions. IMPORTANT: Make sure to check to have the installer edit your PATH. To start a new RStudio project, click File &gt; New Project…. Click New Directory and then New Project. Name the directory test-make and create the project as a subdirectory of the desktop (use the Browse button). Click Create Project. To show whitespace characters on macOS, click RStudio &gt; Preferences…. Then select Code on the left and select Display at the top. Check the box for Show whitespace characters. On Windows click, Tools &gt; Global Options… &gt; Code &gt; Display. Check the box for Show whitespace characters. Click File, New File, and Text File. Click File &gt; Save. Save as Makefile. Notice that there is no extension. Copy-and-paste code below into the Makefile. Check that tabs (not spaces) precede @echo on lines 2 and 5. Comment: By default, RStudio replaces tabs with two spaces–this is the convention for R code. However, RStudio treats files named Makefile specially and doesn’t convert tabs to spaces. all: @echo Build all works! Build all usually makes sure that every file in the project is up-to-date! But for this special Makefile, it just makes this announcement. clean: @echo Clean works! Clean all usually deletes all output files so you can start fresh. But for this special Makefile, it just makes this announcement. 2.12.1 Make with RStudio Click Build &gt; Configure Build Tools…. Click Okay. To reload RStudio, simply right-click and select Reload. You now have a Build tab in the upper-right corner. Click the Build tab. Click Build All. You should get a message confirming what happened. Click More &gt; Clean All. You should get a message confirming what happened. 2.12.2 Make with Terminal In RStudio, click Tools &gt; Terminal &gt; New Terminal. Enter the command make. (That’s the equivalent of clicking Build All). Enter the command make clean. (That’s the equivalent of clicking More &gt; Clean All). 2.13 Test a Complete Workflow The steps below make up your graded homework for the week and allow you to see how we use R, LaTeX, GNU Make, and git to form a coherent, reliable, reproducible workflow. 2.13.1 Start New Project Open RStudio. Click File &gt; New Project. Then click New Directory and New Project. Check the boxes for Create git repository and Open in new session. Name the directory hw01. Create the project as a subdirectory of wherever you like (perhaps as a subdirectory of pos5737/homework/ for example). 2.13.2 Make Your First Commit Open hw01 in GitHub Desktop by clicking File &gt; Add local repository and selecting hw01. You’ve just initialized everything, so type “Initialize project” in the summary. Stage both .gitignore and hw01.Rproj by checking the boxes next to those files. Commit these changes by clicking the blue Commit to master button near the bottom. That’s your first commit. Yay! 2.13.3 Create a GitHub Repo Click the Publish to GitHub button in the upper-right corner. In the Name box, type hw01-jane-doe, replacing jane with your first name and doe with your last name. Check the box to Keep this code private. For organization, choose pos5737. (Note that if I haven’t added you to the pos5737 organization on GitHub, you won’t have this option yet. If that’s so, then please wait until I add you–perhaps send me a reminder on Slack.) Click the blue Publish Repository button. You’ve just created a repo on GitHub and pushed your local files there. You should be able to find your two files on GitHub now. Yay! 2.13.4 Write an R Script In RStudio, with the homework project open, click File &gt; New File &gt; R Script. Copy-and-paste the script below into the file and click the save button. Save the file as create-plot.R in hw01. File names must be exactly right. Run the code (in pieces if you want to see how it works). # create-plot.R: minimal R code to make a plot # load packages library(tidyverse) # create variables x and y x &lt;- c(1, 2, 4, 6, 3) y &lt;- c(6, 2, 3, 1, 4) # plot x and y qplot(x, y) # save plot in png format ggsave(&quot;plot.png&quot;, height = 3, width = 4) Remember that our workflow is change-review-commit-push. You just made a nice change, so open GitHub Desktop back up and review-commit-push. Include a nice commit message. 2.13.5 Write a LaTeX Document with a Figure In RStudio, with the homework project open, click File &gt; New File &gt; then Text File. Copy-and-paste the LaTeX document below into the file and click the save button. Save the file as doc.tex to hw01. % doc.tex: a code to include a plot \\documentclass{article} \\usepackage{graphicx} % useful for including graphics \\begin{document} You should see the figure below. % add the plot \\includegraphics[width = 4in]{plot.png} \\end{document} You can press the Compile PDF button if you like. If you ran the file create-plot.R in the previous step, it should compile into a pdf with a scatterplot. Remember that our workflow is change-review-commit-push. You just made a nice change, so open GitHub Desktop back up and review-commit-push. Include a nice commit message. 2.13.6 Write a Makefile (Optional, Advanced) In RStudio, with the homework project open, click File &gt; New File &gt; Text File. Copy-and-paste the Makefile below into the file and click the save button. Save the file as Makefile (without any extension). remember that Makefiles must use tabs, not spaces. When you paste the Makefile below into RStudio, you might have spaces creating the indents below, not tabs–you must change them to tabs. # Makefile # be careful to use tabs **not spaces**! # define &quot;all&quot;&quot; as the main pdf all: doc.pdf # compile .tex file to .pdf # depends on the tex file and the figure doc.pdf: doc.tex plot.png # first, generate the pdf pdflatex doc.tex # then, remove the auxillary files latex generates rm -f *.bbl *.log *.synctex.gz *.aux *.out # create .png plot # doesn&#39;t depend on any other files plot.png: create-plot.R # run R code Rscript create-plot.R # remove junk R plot, see https://github.com/tidyverse/ggplot2/issues/2787 rm -f Rplots.pdf # delete these files to clean clean: rm -f plot.png # deletes plot rm -f doc.pdf # deletes pdf document Click Build and Configure Build Tools…. Click Okay. Right-click in RStudio and select Reload. Click the Build All button. This is equivalent to running make in the terminal. If you want to delete all the created files, you can click More &gt; Clean All or run make clean in the terminal. (Note that Build All or make is just re-running all the code to (re)do our analysis.) Note that if you click Build All and none of the dependencies have changed, nothing happens! This is desireable because nothing needs to be re-run. If you click More &gt; Clean All, then it deleted all the created files and now Build All creates action because everything needs to be re-run. Remember that our workflow is change-review-commit-push. You just made a nice change, so open GitHub Desktop back up and review-commit-push. Include a nice commit message. 2.13.7 Do Something Interesting Do something interesting with the R script. Use Google to find something cool. Find an interesting R package and use a simple example from the help file. It doesn’t need to be fancy, just try something. Feel free to borrow heavily from someone else’s work you find online, just explain where you borrowed from and explain what’s happening using comments in the R code and prose in the LaTeX document (as best you understand it). I should be clear that I assume you know nothing about R code at this point. You should just search for some examples on Google, try it, and hope it works. You probably won’t understand what’s happening. I just want you to experiment a bit. Hint: Look through some of the geoms on the ggplot2 webpage (e.g., geom_point()). Try to run some of the example code there. Or try this site. If you break something beyond repair, you can (permanently) go back to your most recent commit. Close any open files in the project, open GitHub Desktop, right-click a file you’ve changed, and click Discard All Changes… (If it all goes to heck, then hop on Slack and I can help you out.) Once you’ve done something interesting, clean and rebuild a final time to make sure it works properly. That’s the change. Now just review-commit-push. Find your repo on GitHub and check that your files appear as you expect. Click the commits tab. Browse the previous versions of the project. 2.14 If It Goes Smoothly… If all goes smoothly, this assignment won’t take long. If you have extra time, I suggest you get started on the readings for next week’s homework assignment. "],
["location-and-scale.html", "Chapter 3 Location and Scale 3.1 The Intuition 3.2 The Usual Measures 3.3 Robust Alternatives 3.4 Computation in R", " Chapter 3 Location and Scale 3.1 The Intuition If we took a histogram and tried to describe it to someone else without showing it to them, the most most important pieces of information are usually the location and scale.1 We might describe the variable this way: “The values are about __________, give or take ________ or so.” We can think of the first blank as the location and the second blank as the scale. The location describes where the histogram is positioned along the left-right axis. The scale describes the width (or “spread” or “dispersion”) of the histogram. Inspect the histogram of a hypothethical variable to the right. Notice the location and the scale. If we had to describe these data, we might say that our variable is “about zero give or take one or so.” While this variable has a particular location (about zero), we can imagine shifting it left or right. The figure below shows some possible shifts. We could shift it way to the left, so that it’s “about -6” or a little bit to the right so that it’s “about two.” We can also imagine increasing the scale (more spread) or decreasing the scale (less spread). The figure below shows some possible changes in scale. In each case, the “give or take” number is changing. 3.2 The Usual Measures 3.2.1 The Average The most common measure of the location of a variable is the average.2 Suppose we have a variable (a list of numbers) \\(X = \\{x_1, x_2, ..., x_n\\}\\). \\[\\begin{equation} \\text{average} = \\dfrac{\\text{the sum of the list}}{\\text{the number of entries in the list}} = \\dfrac{\\sum_{i = 1}^n x_i}{n} \\nonumber \\end{equation}\\] The average is easy to compute and easy to work with mathematically.3 Unfortunately, the average doesn’t have an easy interpretation. The best interpretation, in my mind, is as the balance-point for the data. If we imagine the left-right axis as a teeter-totter and stack the data along the beam according to their values, then the average is the position of the fulcrum that would balance the data-filled beam. 3.2.2 The Standard Deviation The most common measure of scale is the standard deviation (SD). The intuition is subtle, so let’s look a a simple example. Rember, our goal is a \"give-or-take number. Suppose we have a list of numbers \\(X = \\{1, 2, 3, 4, 5\\}\\). The average of this list is 3, so we can compute the deviation from average for each value. \\[\\begin{equation} \\text{deviation from average} = d = \\text{value} - \\text{average} \\nonumber \\end{equation}\\] In this case, \\(d = \\{-2, -1, 0, 1, 2\\}\\). We want to use these deviations to find a give-or-take number. Here’s an initial idea. Just take the absolute values \\(|d| = \\{2, 1, 0, 1, 2\\}\\). These tell us how far each entry falls away from the average. Then we could average the absolute devations to find how far a typical entries falls away from the average of the list. In this case, we get 1.2. This is reasonable approach and we’ll refer to it as the average absolute devation or a.a.d. (It turns out that the a.a.d. isn’t a common quantity, so I don’t elevate it with an all-caps acronym.) The a.a.d. has one big problem–it uses an absolute value. This introduces some computational and mathematical difficulties.4 So let’s do something similar. Rather than take the absolute value, let’s square the deviations, take the average, and then undo the square at the end, so that \\(\\text{SD} = \\sqrt{\\text{avg}(d^2)}\\). Sometimes taking the (3) square root of (2) the average of (1) the squares is called the RMS. In this case, the RMS of the deviations from the average is the SD, so that \\[\\begin{equation} \\text{SD} = \\sqrt{\\text{avg}(d^2)} = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}} = \\text{RMS of deviations from average}. \\nonumber \\end{equation}\\] The SD moves smoothly as you move around the entries in the list. To calculate the SD, first make this little table, with the list of values, the deviations from the average, and the squares of the deviations. \\(X\\) \\(d\\) \\(d^2\\) 1 -2 4 2 -1 1 3 0 0 4 1 1 5 2 4 Then compute the average of the squares of the deviations, which in this case is 2. Then take the square root of that average, which in this case is about 1.4. Notice that 1.4 is about 1.2 (the a.a.d.). The SD is bounded (weakly) below by the a.a.s., but they’ll usually be close, so we can think of the SD as how far a typical point falls away from the average. 3.3 Robust Alternatives The average and the SD are mathematically nice. But they are not robust. Seemingly innocuous changes in the variable can lead to large changes in the average and SD.5 We can definite robustness more concretely: How many observations do I need to corrupt to make the summary arbitrarily large? Suppose the toy variable \\(X = \\{0.1, -0.6, 1.1, 1.3, 0.2\\}\\). If I replace the first entry (0.1) with 1, 5, 10, 50, and so on, what happens to the average and SD? The table below shows that we can easily manipulate the average and SD by changing only one data point. In this sense, the average and SD are fragile. Summary Average SD Actual Data Set 0.42 0.70 First entry of \\(X\\) replaced with 1 0.60 0.71 …with 5 1.40 1.92 …with 10 2.40 3.86 …with 50 10.40 19.81 …with 100 20.40 39.81 …with 500 100.40 199.80 …with 1,000 200.40 399.80 If corrupted data present a problem, then what do we mean by “corrupt”? There are (at least) three ways to imagine corrupting a measurement. First, perhaps we have a data entry error. While entering data in a spreadsheet, you entered the number 50,000 into the “hours spent watching the news per day” variable instead of the “income” variable. Second, perhaps our measurement procedure is noisy. Suppose we are coding Twitter posts by their support or opposition to President Trump. Our algorithm might interpret a sarcastic take as support when it actually presented intense opposition. Third, the substantive model might not apply to a particular observation. Take Clark and Golder’s project as an eplxame. They suggest that SMD systems should only have two parties. Indeed, this is a strong theoretical equilibirum. However, it might take several elections to reach this equilibrium. Parties might take several years to coordinate and consolodate. If we include a new democracy in the data set, then we might consider these data “corrupted” since the conceptual model doesn’t apply (yet). The average and SD respond to even a small amount of corrupt data. As an alternative to the average, we might use the median, which is more robust. The median is the/a number which splits the values in half, so that equal numbers of entries lie above and below the median. We have two common robust alternatives to the SD. The interquartile range (IQR) is the difference between the 25th and 75th quantiles. The median absolute deviation (MAD) is the median of the absolute values of the deviations from the median (almost the a.a.d., but using the medians in place of averages). It turns out that multiplying the MAD by 1.4826 makes it similar to the SD in many dataset, so it’s common to rescale it. To illustrate the robustness of each of our measures of location and scale, let’s imagine a variable with 10 observations \\(X = \\{-1.1, 1.5, -1, -0.1, -1.1, 0, -0.4, 0, 0.8, 0.4\\}\\). Let’s see how the measures change as we corrupt more and more of the observations. Summary % Corrupted Average SD Median IQR MAD Actual Data Set 0% -0.10 0.81 -0.05 1.15 0.96 First entry of \\(X\\) replaced with 100 10% 10.01 30.01 0.00 1.03 0.89 First two entries… 20% 19.86 40.07 0.00 1.03 0.89 First three entries… 30% 29.96 45.85 0.20 75.28 0.89 First four entries… 40% 39.97 49.02 0.60 100.00 2.00 First five entries… 50% 50.08 49.92 50.40 99.90 73.54 First six entries… 60% 60.08 48.89 100.00 99.50 0.00 This table illustrates that while the average and SD respond to any corruption, the median, IQR, and MAD remain reasonable summaries of the uncorrupted variable with 40%, 20%, and 30% of the data corrupted, respectively. T The percent of the data that one can corrupt before they can make the measure arbitrarily large is called the breakdown point. Here are the breakdown points for our measures: Measure Breakdown Point Average 0% SD 0% Median 50% IQR 25% MAD 50% As you can see, the median and the MAD are highly robust–they achieve the theoretical maximum breakdown point. 3.4 Computation in R We can easily calculate all these measures of location and scale in R.6 # create variable x = {1, 2, 3, 4, 5} x &lt;- 1:5 # compute measures of location and scale mean(x) # average ## [1] 3 sd(x) # SD; see sidenote ## [1] 1.581139 median(x) # median ## [1] 3 IQR(x) # IQR ## [1] 2 mad(x) # MAD, rescaled by 1.4826 ## [1] 1.4826 mad(x, constant = 1) # MAD, not rescaled ## [1] 1 The functions above work nicely for computing on whole variables. But in most cases, we are interested in comparing the summaries across groups. Take the nominate data set for example. # load packages library(tidyverse) # load nominate data df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% glimpse() ## Rows: 7,080 ## Columns: 7 ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;int&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, 10, 2… ## $ party &lt;fct&gt; Republican, Democrat, Democrat, Democrat, Democrat, Republic… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, William… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0.279,… For these data, we might want to know the average ideology for Republicans and Democrats. We could do it the hard way. # create a data frame with only republicans rep_df &lt;- df %&gt;% filter(party == &quot;Republican&quot;) # compute average mean(rep_df$ideology, na.rm = TRUE) ## [1] 0.4213385 But this is tedious, especially if we wanted to do it by party and Congress. To compute these summaries for lots of subsets of the data, we have the group_by()/summarize() workflow. group_by() defines several groups in the data frame. The first argument is the data frame to group (but we’ll %&gt;% it in). The remaining arguments are the grouping variables. You can think if the groups as a footnote at the bottom of the data set that just mentions the variables that define the groups of interest. Whenever we act (in the wrangling sense) on the data set and the action makes sense in the context of groups, the action will happen by group. After grouping, we use summarize() to create summaries for each group. The first argument is the data frame to summarize (but we’ll %&gt;% it in). The remaining arguments are the summarizes to compute. The names of the remaining arguments become variables in the resulting data frame. smry_df &lt;- df %&gt;% # group by party and congress group_by(party, congress) %&gt;% # compute all of our measures of location and scale summarize(average = mean(ideology, na.rm = TRUE), sd = sd(ideology, na.rm = TRUE), median = median(ideology, na.rm = TRUE), iqr = IQR(ideology, na.rm = TRUE), mad = mad(ideology, na.rm = TRUE), mad1 = mad(ideology, constant = 1, na.rm = TRUE)) %&gt;% # quick look at our work glimpse() ## `summarise()` regrouping output by &#39;party&#39; (override with `.groups` argument) ## Rows: 32 ## Columns: 8 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Democrat, … ## $ congress &lt;int&gt; 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, … ## $ average &lt;dbl&gt; -0.3092901, -0.3130075, -0.3142407, -0.3333065, -0.3615000, … ## $ sd &lt;dbl&gt; 0.1653092, 0.1664293, 0.1658089, 0.1609726, 0.1524251, 0.137… ## $ median &lt;dbl&gt; -0.3200, -0.3200, -0.3200, -0.3360, -0.3815, -0.3835, -0.384… ## $ iqr &lt;dbl&gt; 0.22750, 0.22600, 0.23025, 0.24000, 0.20550, 0.17850, 0.1730… ## $ mad &lt;dbl&gt; 0.1675338, 0.1719816, 0.1690164, 0.1793946, 0.1490013, 0.132… ## $ mad1 &lt;dbl&gt; 0.1130, 0.1160, 0.1140, 0.1210, 0.1005, 0.0895, 0.0870, 0.08… We can plot this measures to get a sense of how they change over time. Notice that mad (rescaled by multiplying by 1.4826) closely corresponds to the SD, but mad1 (not rescaled) is much smaller. # wrangle the data for plotting gg_df &lt;- smry_df %&gt;% pivot_longer(average:mad1, names_to = &quot;measure&quot;) %&gt;% mutate(measure_of = ifelse(measure %in% c(&quot;average&quot;, &quot;median&quot;), &quot;location&quot;, &quot;scale&quot;)) %&gt;% glimpse() ## Rows: 192 ## Columns: 5 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Democrat… ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101… ## $ measure &lt;chr&gt; &quot;average&quot;, &quot;sd&quot;, &quot;median&quot;, &quot;iqr&quot;, &quot;mad&quot;, &quot;mad1&quot;, &quot;average&quot;… ## $ value &lt;dbl&gt; -0.3092901, 0.1653092, -0.3200000, 0.2275000, 0.1675338, 0… ## $ measure_of &lt;chr&gt; &quot;location&quot;, &quot;scale&quot;, &quot;location&quot;, &quot;scale&quot;, &quot;scale&quot;, &quot;scale&quot;… # plot the measures of location and scale ggplot(gg_df, aes(x = congress, y = value, color = measure)) + geom_line() + facet_grid(cols = vars(party), rows = vars(measure_of), scales = &quot;free_y&quot;) I use these terms intentionally. Later, when we discuss random variables, the terms “location” and “scale” will return (with similar meanings). Indeed, we paramaterize many distributions according to their location and scale. For example, the normal distribution has a location parameter \\(\\mu\\) and a scale parameter \\(\\sigma\\).↩︎ Some people refer to the “average” as the “mean”. I prefer to avoid this because the “mean” might also refer to the expectation of a random variable. I use “average” and “expected value” to differentiate these two meanings.↩︎ The median, alternatively, is not easy to compute and quite difficult to work with mathematically.↩︎ Here’s the gist: If you take an entry and slide it up and down (i.e., make it larger or smaller), then the a.a.d. moves up and down as well. This is fine, except the a.a.s. doesn’t respond smoothly. The figure to the right shows what happens as we move the first entry on the list above around–notice the kink! The derivative of the a.a.d. isn’t define here (i.e., there are lots of tangents). This makes things hard mathematically.↩︎ The mathmatical ease and the substantive fragility are related.↩︎ For reasons I don’t want to deal with now, R uses the formula \\(SD = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n - 1}}\\) rather than \\(\\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}}\\). This means that R’s SD will be slightly larger than the SD with my formula. This difference will be tiny in data sets with a typical number of observations.↩︎ "],
["the-normal-model.html", "Chapter 4 The Normal Model 4.1 The Intuition 4.2 The Normal Curve(s) 4.3 The Empirical Rule 4.4 The Normal Approximation 4.5 Review Exercises", " Chapter 4 The Normal Model 4.1 The Intuition Last week, we used the average and SD to reduce and entire variable to two summaries. We use the average and SD to fill in the following sentence: “The values are about ________, give or take ________ or so.” This week, we add an additional assumption. This week, we also say that the histogram of the variable follows the normal curve. The normal curve is a bell-shaped curve with a particular equation. There are two varieties. There is a general, parameterized normal distibution that can move left and right (i.e., change location) and grow wider or taller (i.e., change scale) 4.2 The Normal Curve(s) There are two particular normal curves that we care about the normal curve, which has a location and scale parameter that we can specify: \\(f(x | \\mu, \\sigma) = \\phi(x | \\mu, \\sigma) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^\\frac{{ - \\left( {x - \\mu } \\right)^2 }}{2\\sigma ^2 }\\) the standard normal curve, with the location and scale parameters fixed: \\(f(x | \\mu = 0, \\sigma = 1) = \\phi(x | \\mu = 0, \\sigma = 1) = \\frac{1}{{\\sqrt {2\\pi } }}e^\\frac{{ - x ^2 }}{2}\\) These equations are complicated. Instead of memorizing them or working carefully through the math, just understand (for now) that the normal curve has an equation that exactly characterizes it. The figure below shows the standard normal curve (\\(\\mu = 0\\) and \\(\\sigma = 1\\)) and several other paramaterizations. 4.3 The Empirical Rule It turns out that many variable’s have a histogram that resembles the normal curve. Because of this, the normal curve can sometimes serve as an effective model for these variables. For example, NOMINATE ideology scores for Republicans in the 115th Congress roughly follow the normal curve. df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% filter(party == &quot;Democrat&quot;, congress == 115) ggplot(df, aes(x = ideology)) + geom_histogram() However, the ideology scores for both Republicans and Democrats together does not follow a normal curve. df &lt;- read_rds(&quot;data/nominate.rds&quot;) ggplot(df, aes(x = ideology)) + geom_histogram() The histograms of ENEP by electoral system and social heterogeneity deviate slightly from the normal curve. ## Rows: 1,161 ## Columns: 4 ## $ x &lt;dbl&gt; 1.23, 1.33, 1.43, 1.53, 1.63, 1.73, 1.83, 1.93, … ## $ density &lt;dbl&gt; 0.02616495, 0.02960818, 0.03338541, 0.03751070, … ## $ electoral_system &lt;fct&gt; Large-Magnitude PR, Large-Magnitude PR, Large-Ma… ## $ social_heterogeneity &lt;fct&gt; Bottom 3rd of ENEG, Bottom 3rd of ENEG, Bottom 3… If the variable seems to follow the normal curve, then we have the following rules: About 68% of the data (i.e., “most”) fall within 1 SD of the average. About 95% of the data (i.e., “almost all”) fall within 2 SDs of the average. We can evaluate this rule with the parties data above. Some of the nine hisgrams follow the normal curve quite well (e.g., lower-left). Other’s seem to meaningfully deviate from the normal curve (e.g., middle-left). The table below shows the actual percent of the variable that falls within one and two SDs of the average for each histogram. As you can see, for the lower-left panel (SMD, Top 3rd), the empircal rule of 68% and 95% matches the actual values of 74% and 98% fairly well. For the middle-left panel (SMD, Middle 3rd), the empirical rule matches the actual values of 87% and 93% less well. Across all histograms, it seems fair that the empirical rule works as a rough approximation, even for histograms that meaningfully deviate from the normal curve. Electoral System Social Heterogeneity within 1 SD within 2 SDs Single-Member District Bottom 3rd of ENEG 87% 96% Single-Member District Middle 3rd of ENEG 87% 93% Single-Member District Top 3rd of ENEG 74% 98% Small-Magnitude PR Bottom 3rd of ENEG 68% 97% Small-Magnitude PR Middle 3rd of ENEG 73% 96% Small-Magnitude PR Top 3rd of ENEG 76% 93% Large-Magnitude PR Bottom 3rd of ENEG 80% 98% Large-Magnitude PR Middle 3rd of ENEG 77% 96% Large-Magnitude PR Top 3rd of ENEG 65% 97% 4.4 The Normal Approximation If our normal model summarizes a histogram well, then we can use the model to estimate the percent of the observations that fall in a given range. There are two approaches: Just like we add up the area of the bars to compute percentages with a histogram, we add up the area under the normal curve to approximate percentages. Use a normal table from a textbook. Because the table is for the standard normal curve, we need to re-locate and re-scale the data to fit the standard normal curve. Use the pnorm() function in R. Because this function is parameterized with location and scale, we can simple re-locate and re-scale the curve to fit the data. 4.4.1 Normal Table Normal tables offer an antiquated method to use the normal distribution to approximate percentages. Because we cannot have a normal table for all possible locations and scales, we have one: the standard normal table, which works for a variable with an average of zero and an SD of one. This seems limiting, but it turns out that we can easily re-locate and re-scale any value to match the standard normal curve. We simply subtract the average and divide by the SD. We call this new value a z-score. \\(z\\text{-score} = \\dfrac{\\text{value} - \\text{average}}{\\text{SD}}\\) FSuppose we have the list \\(X = \\{1, 2, 3, 4, 5\\}\\). Then the average is 3, and the SD is about 1.26. We can compute the zscore for the first entry 1 as \\(\\frac{1 - 3}{1.25} \\approx -1.58\\). Similarly, we can convert the entire list to z-scores and get \\(Z = \\{1.59, -0.79, 0.00, 0.79, 1.59\\}\\). If you compute the average and SD of the list \\(Z\\), you will find zero and one, respectively. We can then use a normal table to compute areas under the normal curve between (or above or below) these values of \\(z\\). There are two types of normal tables. Some tables report the percent (or proportion) of the normal curve below a particular value \\(z\\). Other tables report the percent (or proportion) of the normal curve between a particular value \\(z\\) and \\(-z\\). (The normal table on p. A-104 of FPP works this way.) Either table works, but you must know what type of table you are working with. Depending on the question, one type might offer a more direct solution. Here’s a small normal table for a few values of \\(z\\) that uses both approaches. z % less than z % between -z and z Status 0.00 50% 0% 0.10 54% 8% 0.20 58% 16% 0.30 62% 24% 0.40 66% 31% 0.50 69% 38% 0.75 77% 55% 1.00 84% 68% Important 1.50 93% 87% 1.64 95% 90% Important 1.96 98% 95% Important 2.00 98% 95% Important 3.00 100% 100% In order to use the table to find the area between any two values, you need to use the following three rules in combination. The normal table gives the area (i) below \\(z\\) or (ii) between \\(-z\\) and \\(z\\). The area under the entire normal curve is 1 or 100%. The normal curve is symetric, so that the area to the right of \\(z\\) equals the area to the left of \\(-z\\). 4.4.2 pnorm() The pnorm() function in R return the area under the normal curve less than \\(z\\). By default, it uses the standard normal curve, but you can specify a mean and sd if you prefer to re-locate and/or re-scale the curve to fit your values. # area under the std. normal curve less than 1 pnorm(1) ## [1] 0.8413447 # area under the a normal curve (with average of 1 and SD of 4) less than 1 pnorm(1, mean = 1, sd = 4) ## [1] 0.5 # area between -1.64 and 1.64 pnorm(1.64) - pnorm(-1.64) ## [1] 0.8989948 4.4.3 Exactly Percentages To actually compute percentages, we can create a function that works just like pnorm(), but it returns the percent of the data that fall below a particular value. The most convenient method is to create an \"empirical cumulative distribution function*. This function is somewhat confusing. The ecdf() function does not return the proportion below its argument. Instead, it creates a function that returns the percent below its argument. If we have a numeric vector x, then ecdf(x) is a function! Let that settle in… both ecdf and ecdf(x) are function. The function ecdf (I’m dropping the () for clarity) is a function that creates a function, and ecdf(x)() (I’m including the (), as usual, for clarity) is a function that returns the percent below. df &lt;- read_rds(&quot;data/nominate.rds&quot;) %&gt;% filter(party == &quot;Democrat&quot;, congress == 115) # normal approximation for % of Democrats less than -0.05 avg &lt;- mean(df$ideology) sd &lt;- sd(df$ideology) pnorm(-0.5, mean = avg, sd = sd) ## [1] 0.1731597 # exact % of Democrats less than -0.05 ecdf(df$ideology)(-0.5) ## [1] 0.1479592 We can also plot the ECDF with ggplot2. 4.5 Review Exercises The plot below show the histograms for the ideology of legislators in the U.S. House by party. We can compute the average and SD by party. Party Average SD Democrat -0.39 0.12 Republican 0.49 0.15 The table below lists some of the leaders of each party and their ideology score. For each leader, use our three approaches to compute the percent of the party that is “more extreme” than their leader: inspect the histogram, use the normal approximation, and use R to compute the answer exactly. Name Party Position Ideology Score Inspect Histogram Normal Approximation Actual RYAN, Paul D. Republican Speaker of the House 0.56 MCCARTHY, Kevin Republican Majority Leader 0.46 SCALISE, Steve Republican Majority Whip 0.56 McMORRIS RODGERS, Cathy Republican Conference Chair 0.43 PELOSI, Nancy Democrat Minority Leader -0.49 HOYER, Steny Hamilton Democrat Minority Whip -0.38 CLYBURN, James Enos Democrat Assistant Democratic Leader -0.46 LEWIS, John R. Democrat Senior Chief Deputy Minority Whip -0.59 "],
["the-x-y-space.html", "Chapter 5 The X-Y Space 5.1 Points 5.2 Lines", " Chapter 5 The X-Y Space 5.1 Points The scatterplot has two key aesthetics: the horizontal and vertical location of points (and lines). We refer to the horizontal location as “x” and the vertical location as “y.” We sometimes refer to this two-dimmensional space (of horizontal-vertical or x-y locations) as the “Cartisian coordinate system.” The table below contains five observations. Each observation has values for variables x and y. (In the context of a data analysis, we typically think of x as the key explanatory variable and y as the outcome varaible.) Observation x y #1 1 1 #2 4 4 #3 -4 -3 #4 2 -2 #5 -2 4 The plot below shows the location of each point in the x-y space. Exercise 5.1 Recreate the x-y space below on a sheet of paper. Add the following points (0, 0), (1, 2), (-3, 4), (2, -3), and (-4, -2). Hint The first number in the (x, y) pair represents the “x” or horizontal location. The second number represents the “y” or vertical location. The location of the second point is… Solution 5.2 Lines We can also draw lines in the x-y space. Remember, the formula for a line is \\(y = mx + b\\). Here, \\(y\\) and \\(x\\) represent variables (i.e., locations in the x-y space), \\(m\\) represents the slope of the line, and \\(b\\) represents the intercept. Consider the following four examples: Example Equation Intercept Slope Example 1 \\(y = x + 0\\) 0 1.0 Example 2 \\(y = -2x + 2\\) 2 -2.0 Example 3 \\(y = 0.5x - 1\\) -1 0.5 Example 4 \\(y = 3x - 2\\) -2 3.0 5.2.1 The Intercept The intercept \\(b\\) tells us where the line crosses the vertical slice of the space where \\(x = 0\\). Exercise 5.2 For the lines below, identify the intercept visually, if possible. Hint All the lines look like 45-degree lines (or 315-degree) because the ranges of the axes are rescaled. This makes the problem a little trickier. For each line, identify where the line cross the slide of the space where \\(x = 0\\). When \\(x = 0\\), then \\(y = m \\times 0 + b = b\\). Remember that \\(b\\) represents the intercept. Solution The intercepts are 0, -2, 1, 1.5. You cannot see the third intercept visually, because the slice of the space where \\(x = 0\\) is not included in the plot. 5.2.2 Slope The slope \\(m\\) tells us how fast the line rises or falls as we move from left-to-right. If \\(m\\) is positive, then the line rises. If \\(m\\) is negative, then the line falls. If \\(m\\) is zero, then the line neither rises nor falls (stays constant at the same height). As \\(m\\) gets larger in magnitude, the line rises or falls faster. The best way to think about slope is as the “rise over run.” \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Take Example 2 from the table above or \\(y = -2x + 2\\). Consider two scenaarios: one where \\(x = 0\\) and another where \\(x = 3\\). When \\(x = 0\\), we know that \\(y = 2\\) because the intercept \\(b\\) equals 2. When \\(x = 3\\), we have “run” 3 units to the right (i.e., \\(\\text{run} = \\text{2nd value} - \\text{1st value} = 3 - 0 = 3\\)) and \\(y = -2 \\times 3 + 2 = -6 + 2 = -4\\). When we run 3 units, we rise \\(-4 - 2 = -6\\) units (or fall 6 units). The table below summarizes our work. Scenario x y Scenario 1 0 2 Scenario 2 3 -4 \\(\\text{run} = x \\text{ in Scenario 2} - x \\text{ in Scenario 1} = 3 - 0 = 3\\) \\(\\text{rise} = y \\text{ in Scenario 2} - y \\text{ in Scenario 1} = -4 - 2 = -6\\) \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}} = \\dfrac{-6}{3} = -2\\) These calculations match the slope we find by inspecting the original equation \\(y = -2x + 2\\). The figure below shows the rise-over-run logic for each of the four example equations. Exercise 5.3 Add the lines \\(y = 2x - 1\\) and \\(y = -0.5x + 1\\) to the figured you sketched in Exercise 5.1. Hint Remember that a line is completely defined by only two points. Use this rule to draw the line. Choose two values of \\(x\\) and find the corresponding value of \\(y\\). Ideally, choose two values of \\(x\\) that are separated by some distance (so long as the resulting x-y pairs remain on our plot). Let’s try the first line using \\(x = -1\\) and \\(x = 2\\). For the first equation, we have \\(y = 2 \\times -1 -1 = -2 - 1 = -3\\) and \\(y = 2 \\times 2 -1 = 4 - 1 = 3\\), respectively. Then our two points are (-1, -3) and (2, 3), respectively. Just add lightly add these two points to the plot and draw a line that goes through both. Solution Exercise 5.4 What is the slope and intercept of the line below? Hint To find the intercept, find where the line crosses the vertical slide at \\(x = 0\\). This gives the intercept directly. To find the slope, simple choose two points along the line and find the rise (i.e., the vertical distance between the two points) and the run (i.e., the horizontal distance between the two points). The slope is the rise over the run or \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Solution The intercept is -1 and the slope is 1.5, so the equation for the line is \\(y = 1.5x - 1\\). Exercise 5.5 What is the slope and intercept of the line below? Solution The intercept is 1 and the slope is -0.5, so the equation for the line is \\(y = -0.5x + 1\\). "],
["the-scatterplot.html", "Chapter 6 The Scatterplot 6.1 geom_point() 6.2 Example: gapminder 6.3 Example: voteincome 6.4 Resources", " Chapter 6 The Scatterplot The scatterplot is the most powerful tool in statistics. The following comes as close to any rote procedure that I would recommend following: Always plot your data using a scatterplot. For some combinations of unordered, qualitative variables with a large number of categories, the scatterplot might not offer useful information. However, the plot itself will not mislead the researcher. Therefore, the scatterplot offers a safe, likely useful starting point for almost all data analysis. As an example, here’s Sarah’s data for the research project. She cares about the difference in ideology self-reports across different study designs. Although this isn’t an ideal application for a scatterplot (i.e., two fine-grained measures of x and y), the scatterplot is (1) at least somewhat helpful and (2) certainly not harmful. 6.1 geom_point() To create scatterplots, we simply use geom_point() as the geometry combined with our same approach to data and aesthetics. Here’s a simple example with hypothetical data. # create a hypothetical dataset with tribble() df &lt;- tribble( ~x, ~ y, 1, 1, 2, 2, 3, 6, 1, 3, 2.5, 5) %&gt;% glimpse() Rows: 5 Columns: 2 $ x &lt;dbl&gt; 1.0, 2.0, 3.0, 1.0, 2.5 $ y &lt;dbl&gt; 1, 2, 6, 3, 5 ggplot(df, aes(x = x, y = y)) + geom_point() Here’s a more realistic example. gamson_df &lt;- read_rds(&quot;data/gamson.rds&quot;) %&gt;% glimpse() Rows: 826 Columns: 2 $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Because the data are so dense, especially in the lower-left corner of the plot, we might use alpha transparency to make the density easier to see. ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point(alpha = 0.3) 6.2 Example: gapminder For a dataset with more variables, we can represent a few other variables using aesthetics other than location in space. For this example, we use country-level data from the gapminder package. # load gapminder dataset from gapminder package data(gapminder, package = &quot;gapminder&quot;) glimpse(gapminder) Rows: 1,704 Columns: 6 $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,… ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) Because GDP per capita is skewed so heavily to the right, we might transform the x-axis from a linear scale (the default) to a log (base-10) scale. ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + scale_x_log10() Because countries are evolving over time, we can connect the subsequent points using geom_path(). Note that geom_path() connects points as they are arranged in the dataset, so make sure your dataset is arranged properly. Because we want one path per country, we should include the aesthetic aes(group = country) as an argument to geom_path(). # arrange data by year gapminder2 &lt;- gapminder %&gt;% arrange(year) ggplot(gapminder2, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + geom_path(aes(group = country)) + scale_x_log10() This is a little hard to see, so let’s clean it up a bit. ggplot(gapminder2, aes(x = gdpPercap, y = lifeExp, size = pop, color = year)) + geom_path(aes(group = country), size = 0.5, alpha = 0.2) + geom_point(alpha = 0.3) + scale_x_log10() + facet_wrap(vars(continent)) 6.3 Example: voteincome [This block is currently not working because the Zelig package is out of commission at the moment.] # load voteincome data from Zelig package # data(voteincome, package = &quot;Zelig&quot;) # # glimpse(voteincome) # # ggplot(voteincome, aes(x = education, # y = income)) + # geom_point() Notice three things: The variable education is not under control. To see the codings, use help(voteincome, package = \"Zelig\"). Ideally, this variable (i) use qualitative labels rather than numeric placeholders and (ii) be a factor with reasonably ordered levels. There’s substantial over-plotting. Dozens of points are right on top of each other, so we cannot tell how many points are at each coordiate. Let’s fix the first issue for education, so you can see how. (income has many more levels, so let’s just get on with the plotting). # voteincome2 &lt;- voteincome %&gt;% # mutate(education = fct_recode(as.character(education), # &quot;Less than High School Education&quot; = &quot;1&quot;, # &quot;High School Education&quot; = &quot;2&quot;, # &quot;College Education&quot; = &quot;3&quot;, # &quot;More than a College Education&quot; = &quot;4&quot;)) %&gt;% # glimpse() Now let’s deal with the overplotting. In general, we have two strategies for dealing with overplotting. alpha transparency jittering First, let’s try to adjust the alpha transparency. # ggplot(voteincome2, aes(x = education, # y = income)) + # geom_point(alpha = 0.2) This helps, but only a little. We can we wher we have many points, where we have just a few, and where we have none. But overall, we still don’t have a good sense of the density at each coordinate. Let’s try jittering. To jitter the data, we add a small amount of noise to each point. We add enough noise to separate it from the other points, but not so much noise to distort the position along in the space. # ggplot(voteincome2, aes(x = education, # y = income)) + # geom_point(position = &quot;jitter&quot;) Exercise 6.1 Write an R script that uses the parties dataset to create a scatterplot that allows you to evaluate Clark and Golder’s (2006) claim: The number of political parties increases as social heterogeity increases, but only under permissive electoral rules. Hint Perhaps use the following aesthetics: x = eneg and y = enep. Create individual facets for each electoral_system. Solution # load packages library(tidyverse) # load data parties_df &lt;- read_rds(&quot;data/parties.rds&quot;) # make scatterplot ggplot(parties_df, aes(x = eneg, y = enep)) + geom_point() + facet_wrap(vars(electoral_system)) Exercise 6.2 Go to the Dataverse repository for Barrilleaux and Rainey (2014) and download the dataset politics_and_need.csv. Plot the variable percent_uninsured (the percent of each state’s population without health insurance) along the horizontal axis and the variable percent_favorable_aca (the percent of each state with a favorable attitude toward Obamacare) along the vertical axis. Interpret and speculate about any pattern. I encourage you to represent other variables with other aesthetics. Exercise 6.3 Continuing the exercise above, label each point with the state’s two-letter abbreviation. Experiment with the following strategies. geom_text() instead of geom_point() geom_label() instead of geom_point() geom_text_repel() in the ggrepel package in addition to geom_point() geom_label_repel() in the ggrepel package in addition to geom_point() Hint: Review the help files (e.g., ?geom_text()) and the contained examples to understand how to use each geom. The variable state_abbr contains the two-letter abbreviation, so you’ll need to include the aesthetic label = state_abbr in the aes() function. 6.4 Resources Tufte. 2001. The Visual Display of Quantitative Information. Healy. 2018. Data Visualization: A Practical Introduction. [html] Wickham. ggplot2: Elegant Graphics for Data Analysis. [html for in-progress 3rd e.] RStudio’s ggplot2 cheat sheet [GitHub] The help file for geom_point() has some examples. The help file for geom_path() might be helpful, especially if you have the same country observed for multiple years and want to connect the subsequent points. The help file for geom_text() might be helpful, especially if you have only a few observations and your readers know something about some of them. "],
["correlation-coefficient.html", "Chapter 7 Correlation Coefficient 7.1 Intuition 7.2 Computing 7.3 Interpreting 7.4 Example: Clark and Golder (2006) 7.5 Example: Feeling Thermometers", " Chapter 7 Correlation Coefficient We’ve discussed several ways to reduce data–to summarize the key features of many observations using a single (or a few) numbers. A histogram visually shows the density in chosen bins. The average tells us the location of a set of observations. Remember the seesaw analogy. The SD tells us the scale (or spread or disperson) of a set of observations. We can describe a list of numbers as being “about [the average] give or take [the SD].” The correlation coefficient allows us to describe the relationship between two variables. Before, we compared variables by comparing their histograms, averages, or SDs. The correlation coefficient is our first summary that compares two variables directly (rather than summarizing just one). 7.1 Intuition The correlation coefficient measures how well two variables “go together.” “Go together” means “as one goes up, the other goes up [or down].” “Go together” has linearity built into the meaning. The correlation coefficient does not describe curved relationships. The figure below shows some scatterplots and how well I might say these variables go together. However, I am firmly opposed to any rules that link particular correlation coefficients to strength of relationship. Imagine the following studies: A study comparing two measures of the same concept. A study comparing the effect of a dose of vitamin D in the first hour after birth on lifespan. A “weak” or “small” correlation in the first study would be impossibly large in the second. The interpretation of the strength of a relationship must be made by a substantive expert in a particular substantive context. I use two guidelines to interpret a correlation coefficient: 0.9 seems a lot stronger than 0.7, but 0.4 seems barely stronger than 0.2. Around 0.4 [-0.4], the a correlation becomes “easily noticeable” without studying the plot carefully. For smaller datasets, this threshold increases toward 1 [-1]; for larger datasets, the threshold shrinks toward 0. Exercise 7.1 Guess the correlation coefficient for each scatterplot below. Solution dataset r Dataset 1 -0.60 Dataset 2 0.45 Dataset 3 0.90 Dataset 4 0.45 Dataset 5 0.55 Dataset 6 0.55 Dataset 7 0.10 Dataset 8 0.85 Dataset 9 0.85 Dataset 10 0.35 Dataset 11 0.60 Dataset 12 0.80 7.2 Computing Suppose we have the dataset below. x y 1 10 3 15 2 12 4 13 5 18 7.2.1 By Hand We can compute the correlation coefficient \\(r\\) as follows: \\(r = \\text{average of} \\left[ (x \\text{ in standard units}) \\times (y \\text{ in standard units}) \\right]\\) Using \\(\\overline(x)\\) to represent the average of \\(x\\) and \\(n\\) to represent the number of observations (5, in this case), we have \\(r = \\dfrac{\\frac{(x - \\overline{x})}{\\sqrt{\\frac{(x - \\overline{x})^2}{n}}} \\times \\frac{(y - \\overline{y})}{\\sqrt{\\frac{(y - \\overline{y})^2}{n}}}}{n}\\). We can implement this formula by creating the little table below and then averaging the final column of products. x y x in SUs y in SUs product 1 10 -1.41 -1.32 1.87 3 15 0.00 0.51 0.00 2 12 -0.71 -0.59 0.41 4 13 0.71 -0.22 -0.16 5 18 1.41 1.61 2.28 The average of the final column is 0.88. 7.2.2 With R In R, we can compute the corrlation between x and y using cor(x, y). Note that dropping missing values is more complicated for pairs of data. If you want to drop missing values from the calculations, then cor(x, y, use = pairwise.complete.obs\") is a good choice. We can use the code below to find the correlation in the example above. x &lt;- c(1, 3, 2, 4, 5) y &lt;- c(10, 15, 12, 13, 18) cor(x, y) [1] 0.8814089 Exercise 7.2 Compute the correlation coefficient between each combination of the four variables below. Check your work with R. x y z 2 8 7 4 0 3 5 5 5 6 3 6 4 6 6 3 5 3 7.3 Interpreting In general, a correlation coefficient is NOT particularly useful. I introduce it for two reasons: Other people use it. We use it to obtain more useful quantities. However, the correlation coefficient \\(r\\) has a concrete interpretation: If \\(x\\) is one SD larger, then \\(y\\) is \\(r\\) SDs larger on average. We might also say that “a one SD increase in \\(x\\) leads to an \\(r\\) SD increase in \\(y\\) on average,” but we must take care that “leads to” describes a pattern in the data and does not describe a causal relationship. 7.4 Example: Clark and Golder (2006) For a substantive example, consider Clark and Golder’s data. # load parties dataset parties_df &lt;- read_rds(&quot;data/parties.rds&quot;) # compute correlation between enep and eneg for each electoral system cor_df &lt;- parties_df %&gt;% group_by(electoral_system) %&gt;% summarize(cor = cor(enep, eneg)) electoral_system cor Single-Member District 0.04 Small-Magnitude PR 0.45 Large-Magnitude PR -0.02 ggplot(parties_df, aes(x = eneg, y = enep)) + geom_point(alpha = 0.5) + facet_wrap(vars(electoral_system)) + geom_label(data = cor_df, aes(x = Inf, y = Inf, label = paste0(&quot;cor = &quot;, round(cor, 2))), hjust = 1.1, vjust = 1.1) + theme_bw() As Clark and Golder expect, we get a correlation coefficient near zero in SMD systems. But contrary to their expectation, we also get a correlation coefficient near zero in large-magnitude PR systems. Exercise 7.3 Interpret the correlation for small-magnitude PR systems above by filling in the following blanks: A one SD increase in ENEG leads to a _____ SD increase in ENEP, on average. A _____ unit increase in ENEG leads to a _____ unit increase in ENEP, on average. Hint How many units is one SD for ENEG? What about for ENEP? Going from SDs to the original units is like going from feet to yards: you just need to know how many feet are in a yard (or how many SDs are in each original unit). 7.5 Example: Feeling Thermometers Below, I compute the correlation between feelings toward the Democratic and Republican parties. It makes sense that this correlation should be negative. As respondents’ feelings toward the Democratic party grow warmer, their feelings toward the Republican party should grow cooler. We might also expect this correlation to be stronger among more educated respondents and change over time. The example below uses the therms dataset in the pos5737data package available on GitHub. # get pos5737data (if updated) devtools::install_github(&quot;pos5737/pos5737data&quot;) # load data data(therms, package = &quot;pos5737data&quot;) # quick look glimpse(therms) Rows: 38,100 Columns: 4 $ year &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1… $ ft_democratic_party &lt;dbl&gt; 80, 50, 40, 60, 85, 50, 70, NA, 60, NA, NA, 70, 8… $ ft_republican_party &lt;dbl&gt; 50, 50, 60, 60, 60, 50, 40, NA, 60, NA, NA, 40, 8… $ education &lt;fct&gt; High School, 8th Grade or Less, High School, High… # compute correlation between the two feelinging thermometers for # each year and education level smry_df &lt;- therms %&gt;% # drop observations where education is missing drop_na(education) %&gt;% # compute correlation for each year-education subset group_by(year, education) %&gt;% summarize(cor = cor(x = ft_democratic_party, y = ft_republican_party, use = &quot;pairwise.complete.obs&quot;)) %&gt;% # complete dataset by right-joining a dataset that has all years and all education levels combinations right_join(crossing(year = unique(therms$year), education = unique(therms$education))) %&gt;% # add a variable for presidential elections--if the year is evenly divisible by 4 mutate(election_type = ifelse(test = year %% 4 == 0, yes = &quot;Presidential Election&quot;, no = &quot;Congressional Election&quot;)) %&gt;% glimpse() Rows: 102 Columns: 4 Groups: year [17] $ year &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1980, 1980, 1980, 1… $ education &lt;fct&gt; 8th Grade or Less, Some High School, High School, High … $ cor &lt;dbl&gt; -0.219932553, -0.153398674, -0.154494861, -0.059715780,… $ election_type &lt;chr&gt; &quot;Congressional Election&quot;, &quot;Congressional Election&quot;, &quot;Co… # plot correlations ggplot(smry_df, aes(x = year, y = cor, color = election_type)) + geom_point() + geom_line() + facet_wrap(vars(education)) Exercise 7.4 Read the excerpt from Clark, Golder, and Golder on pp. 477-478. Download the gamson dataset from the data page. Compute the correlation coefficient \\(r\\) between seat and portfolio shares and create a scatterplot of the two. Comment briefly. Solution # load data gamson_df &lt;- read_rds(&quot;data/gamson.rds&quot;) # compute correlation coefficient cor(x = gamson_df$seat_share, gamson_df$portfolio_share) [1] 0.9423176 # create scatterplot ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Exercise 7.5 Use devtools::install_github(\"pos5737/pos5737data\") to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737data\"). Use glimpse(anscombe) to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. Use a group_by() and summarize() workflow to compute a correlation coefficient for each of the four datasets. How do they compare? What do they suggest about the strength of the relationship between \\(x\\) and \\(y\\)? Create a scatterplot of \\(x\\) and \\(y\\) with separate panels for each dataset. How do they compare? How would you describe the strength of the relationship between \\(x\\) and \\(y\\) in each panel? Would you say that the correlation coefficient offered a good summary of each dataset? "],
["regression.html", "Chapter 8 Regression 8.1 Review 8.2 The Equation 8.3 The Conditional Average 8.4 The Best Line 8.5 The RMS of the Residuals 8.6 \\(R^2\\) 8.7 Fitting Regression Models 8.8 Standard Errors and p-Values 8.9 A Warning 8.10 Review Exercises", " Chapter 8 Regression 8.1 Review So far, we’ve made comparison between sets of measurements by… Comparing the histograms of the sets of measurements (e.g., the ideologies of representatives from the 100th and 115th Congresses). Comparing the average or SD of sets of the measurements (e.g., the ideologies of representatives from the 100th and 115th Congresses). Computing the correlation coefficient between two sets of measurements (e.g., the portfolio share and the seat share for coalition governments) Regression allows us to extend the correlation coefficient–a measure of how well two variables “go together”–into a more intuitive, informative quantity. Regression allows us to answer the question “What is the average of \\(y\\)?” for many different values of \\(x\\). Rather than compare a small number of scenarios (e.g., the 100th and 115th Congresses), the regression line allows us to compare the average of \\(y\\) as \\(x\\) varies continuously. In short, we describe the average value of \\(y\\) as a linear function of \\(x\\), so that \\(\\text{the average of } Y = mX + b\\). Before we hop into regression, let’s review where we’ve been. Let’s do it with an exercise. Exercise 8.1 One of the simplest democratic mechanims goes like this: If things are going well, vote for the incumbent. If things are going poorly, vote against the incumbent. Normatively, we want to see voters punish incumbents for bad outcomes. Let’s see if incumbents get fewer votes when things go poorly. The dataset below shows incumbent presidents’ margins of victory (in the popular vote) and the percent change in the real disposable income from Q2 in the year before the election to Q2 in the year of the election. Do the following analyses with a pencil-and-paper. (You can use a computer for things like addition and multiplication if that makes it easier, but show your work on the paper.) Use the percent change in the RDI to break incumbents into “good performers” and “bad performers”. You may use more than two categories if you want. Draw a histogram of the margin of victory for the good performers and another for the bad performers. Compare and interpret. Compute the average and SD for each category. Compare and interpret. Compute the correlation between the percent change in the RDI and the margin of victory. Interpret. Now replicate the pencil-and-paper work with R. You can find the data here. Overall, do voters punish incumbents for bad outcomes? Year Incumbent % Change in RDI Margin of Victory 2016 H. Clinton 0.89 2.23 2012 Obama 2.83 3.93 2008 McCain 1.30 -7.38 2004 GWB 2.67 2.49 2000 Gore 4.13 0.54 1996 B. Clinton 2.21 9.47 1992 GHWB 2.93 -6.91 1988 GHWB 4.80 7.80 1984 Reagan 6.66 18.34 1980 Carter -1.08 -10.61 1976 Ford 0.10 -2.10 1972 Nixon 2.10 23.57 1968 Humphrey 4.04 -0.81 1964 Johnson 6.09 22.69 1960 Nixon 0.31 -0.17 1956 Ike 3.23 15.50 1952 Stevenson 0.44 -10.90 1948 Truman 4.49 4.74 Two warnings: Social scientists use the term “regression” imprecisely. In one case, they might use “regression” to refer to a broad class of models. In another case, they might use it to refer to the particular model discussed below. So watch out for inconsistent meanings, but the context should make the meaning clear. Methodologists often motivate regression from a random sampling perspective. This is not necessary but it’s common. In my view, it’s unfortunate because regression is a useful tool with datasets that are not random samples and it unnecessarily complicated the results. However, in the random sampling framework, one can motivate the methods below quite elegantly. 8.2 The Equation Let’s start by describing a scatterplot using a line. Indeed, we can think of the regression equation as an equation for a scatterplot. First, let’s agree that we won’t encounter a scatterplot where all the points \\((x_i, y_i)\\) fall exactly along a line. As such, we need a notation that allows us to distinguish the line from the observed values. We commonly refer to the values along the line as the “fitted values” (or “predicted values” or “predictions” and the observations themselves as the “observed values” or “observations.” We use \\(y_i\\) to denote the \\(i\\)th observation of \\(y\\) and use \\(\\hat{y}_i\\) to denote the fitted value (usually given \\(x_i\\)). We write the equation for the line as \\(\\hat{y} = \\alpha + \\beta x\\) and the fitted values as \\(\\hat{y}_i = \\alpha + \\beta x_i\\). We refer to the intercept \\(\\alpha\\) and the slope \\(\\beta\\) as coefficients. We refer to the difference between the observed value \\(y_i\\) and the fitted value \\(\\hat{y}_i = \\alpha + \\beta x_i\\) as the residual \\(r_i = y_i - \\hat{y}_i\\). Thus, for any \\(\\alpha\\) and \\(\\beta\\), we can write \\(y_i = \\alpha + \\beta x_i + r_i\\) for the observations \\(i = \\{1, 2, ..., n\\}\\). Notice that we can break each \\(y_i\\) into two pieces components the linear function of \\(x_i\\): \\(\\alpha + \\beta x_i\\) the residual \\(r_i\\). In short, we can describe any scatterplot using the model \\(y_i = \\alpha + \\beta x_i + r_i\\). The black points show the individual observations \\((x_i, y_i)\\), The green line shows the equation \\(\\hat{y} = \\alpha + \\beta x\\). The purple star shows the prediction \\(\\hat{y}_i\\) for \\(x = x_i\\). The orange vertical line shows the residual \\(r_i = y_i - \\hat{y}_i\\). Using this generic approach, we can describe any scatterplot using any line. Of course, the line above isn’t a very good line. How can we go about finding a good line? Exercise 8.2 Before we talk about a good line, let’s talk about a good point. Suppose you have a dataset \\(y = \\{y_1, y_2, ... , y_n\\}\\) and you want to predict these observations with a single point \\(\\theta\\). Use calculus to find the \\(\\theta\\) that minimizes the r.m.s. of the residuals \\(r_i = y_i - \\theta\\) or that minimizes \\(f(\\theta) = \\sqrt{\\dfrac{\\sum_{i = 1}^n(y_i - \\theta)^2}{n}}\\). Hint 1. Realize that the \\(\\theta\\) that minimizes \\(f(\\theta) = \\sqrt{\\dfrac{\\displaystyle \\sum_{i = 1}^n (y_i - \\theta)^2}{n}}\\) also minimizes \\(g(\\theta) = \\dfrac{\\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2}{n}\\). We know this because the square root function is monotonically increase (for positive values, which this must always be) and preserves the order of observations. In other words, the \\(\\theta\\) that produces the smallest RMS of the deviations also produces the smallest MS of the deviations. 1. Realize that the \\(\\theta\\) that minimizes \\(g(\\theta) = \\dfrac{\\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2}{n}\\) also minimizes \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2\\). Removing a constant just shifts the curve up or down, but it does not change the \\(\\theta\\) that minimizes the curve. So work with \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2\\). 1. To make things easier, expand \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i - \\theta)^2\\) to \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n(y_i^2 - 2\\theta y_i + \\theta^2)\\). 1. Distribute the summation operator to obtain \\(h(\\theta) = \\displaystyle \\sum_{i = 1}^n y_i^2 - 2 \\theta \\sum_{i = 1} y_i + n\\theta^2\\). 1. Now take the derivative of \\(h(\\theta)\\) w.r.t. \\(\\theta\\), set that derivative equal to zero, and solve for \\(\\theta\\). The result should be familiar. Exercise 8.3 See how other authors conceptualize the regression model. Read Lewis-Beck (1980), pp. 9-13 (up to “Least Squares Principle”) Read Wooldridge (2013), pp. 22-26 (Section 2.1). Do q. 1, p. 60. 8.3 The Conditional Average Have a look at the scatterplot below. What’s the portfolio share of a party in a coalition government with a seat share of 25%? Your eyes probably immediately begin examining a vertical strip above 25%. You probably estimate the average is a little more than 25%… call it 27%. You can see that the SD is about 10% because you’d need to go out about 10 percentage points above and below the average to grab about 2/3rds of the data. Now you’re informed by the data and ready to answer the question. Q: What’s the portfolio share of a party in a coalition government with a seat share of 25%? A: It’s about 28% give or take 10 percentage points or so. Notice that if we break the data into many small windows, we can visually create an average (and an SD) for each. Freedman, Pisani, and Purves (2008) refer to this as a “graph of averages.” Fox (2008) calls this “naive nonparametric regression.” It’s a conceptual tool to help us understand regression. For some datasets, these averages will fall roughly along a line. In that case, we can described the average value of \\(y\\) for each value of \\(x\\)–that is, the conditional average of \\(y\\)–with a line. Here’s the takeaway: a “good” line is the conditional average. Exercise 8.4 Other authors use this “graph of averages”. Read FPP, ch. 10. Set A, p. 161: 1-4. Set B, p. 163: 1, 3. Set C, p. 167: 1-3. Set D, p. 174: 1, 2. Set E, p. 175: 1, 2. Section 6, p. 176: 1-3, 5-7, 10. Read Read Fox (2008), pp. 17-21. (Optional: Section 2.3 describes how to create a smoothed average in a more principled manner.) 8.4 The Best Line So far, we have to results: The average is the point that minimizes the RMS of the deviations. We want a line that captures the conditional average. Just as the average minimizes the RMS of the deviations, perhaps we should choose the line that minimizes the RMS of the residuals… that’s exactly what we do. We want the pair of coefficients \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimizes the RMS of the residuals or \\(\\DeclareMathOperator*{\\argmin}{arg\\,min}\\) \\[\\begin{equation} (\\hat{\\alpha}, \\hat{\\beta}) = \\displaystyle \\argmin_{( \\alpha, \\, \\beta ) \\, \\in \\, \\mathbb{R}^2} \\sqrt{\\frac{r_i^2}{n}} \\end{equation}\\] Let’s explore three methods to find the coefficients that minimize the RMS of the residuals. grid search numerical optimization, to get additional intuition and preview more advanced methods analytical optimization 8.4.1 Grid Search Because we’re looking for the pair \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimize the sum of the RMS residuals, we could simply check lots of different values. In some applications, this is a reasonable optimization routine. (It is not reasonable in the context of regression, where we have much faster and accuracy tools.) The figure below shows the result of a grid search. In the top-left panel, we see the line and the residuals for each intercept-slope pair. The size of the points indicates the residual squared. Notice that some lines make big errors and other lines make small errors. In the top-right panel, we see a histogram of the squared residuals. In the lower-left panel, each point in the grid shows a intercept-slope pair. The label, color, and size indicate the RMS of the residuals. As we move around the grid, the RMS changes–we’re looking for the smallest. In the lower-right panel, the three lines show the evolution of the intercept, slope, and RMS of the residuals, respectively. Again, we’re looking for the pair that produces the smallest RMS of the residuals. For this search, the intercept -0.36 and the slope 0.89 produce the smallest RMS of the residuals (of the combinations we considered). 8.4.2 Numerical Optimization Remember that we simply need to minimize the function \\(f(\\alpha, \\beta) = \\displaystyle \\sqrt{\\frac{\\sum_{i = 1}^n r_i^2}{n}} = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{ \\sum_{i = 1}^n [y_i - (\\alpha + \\beta x_i)]^2}{n}}\\), shown below. Hill-climbing algorithms, such as Newton-Raphson, find the optimum numerically by investigating the shape of \\(f\\) at its current location, taking a step uphill, and then repeating. When no step leads uphill, the algorithm has found the optimum. Under meaningful restrictions (e.g., no local optima), these algorithms find the global optimum. First, let’s add the data from the grid search example above using tribble(). df &lt;- tribble( ~x, ~y, -1, -0.2, 1.1, -0.2, -1.1, -2.9, -0.2, -0.2, -0.5, -1.4, 1.9, 2.2, 1.2, 0.9, 0.1, -0.5, 0.7, -0.4, -1.3, -1.8, 2.3, 2.5, 0.4, -0.9, 1, 2, -0.9, -0.1, -0.1, 1.4, -1.2, -1.3, 0.9, 1, 0, -0.6, -0.6, -0.6, -0.3, -2.2, 1.4, 1.2, 1.8, 1.2, 1.6, -0.2, 2.2, 1.4, -0.7, -0.7, 0.6, -0.4, 0.8, 0.6 ) Now let’s create a function that takes the parameters (to be optimized over) as the first argument. f &lt;- function(par, data) { alpha &lt;- par[1] beta &lt;- par[2] y_hat &lt;- alpha + beta*data$x r &lt;- data$y - y_hat rms &lt;- sqrt(mean(r^2)) return(rms) } Now we can optimize this function f() using optim(). The default method is \"Nelder-Mead\", which works similarly to the Newton-Raphson algorithm you might have seen before. results &lt;- optim( par = c(0, 0), # initial slope and intercept fn = f, # function to optimize data = df # dataset for f() ) results$par [1] -0.3595557 0.9414289 results$value [1] 0.8448945 The Nelder-Mead optimization routine finds that intercept of -0.36 and a slope of 0.94 result in the smallest RMS of the residuals 0.84. This somewhat agrees with the results from the coarse grid search. If the grid search were more fine-grained, we could easily obtain solutions that agree to two decimal places. 8.4.3 Analytical Optimization In the case of finding the line that minimizes the RMS of the residuals, we have an easy analytical solution. We don’t need a grid search, and we don’t need to optimize numerically. 8.4.3.1 Scalar Form Remember that we simply need to minimize the function \\(f(\\alpha, \\beta) = \\displaystyle \\sqrt{\\frac{\\sum_{i = 1}^n [y_i - (\\alpha + \\beta x_i)]^2}{n}}\\). This is equivalent to minimizing \\(h(\\alpha, \\beta) = \\sum_{i = 1}^n(y_i - \\alpha - \\beta x_i)^2\\). We sometimes refer to this quantity as the SSR or “sum of squared residuals.” To minimize \\(h(\\alpha, \\beta)\\), remember that we need to solve for \\(\\frac{\\partial h}{\\partial \\alpha} = 0\\) and \\(\\frac{\\partial h}{\\partial \\beta} = 0\\) (i.e., the first-order conditions). Using the chain rule, we have the partial derivatives \\(\\frac{\\partial h}{\\partial \\alpha} = \\sum_{i = 1}^n [2 \\times (y_i - \\alpha - \\beta x_i) \\times (-1)] = -2 \\sum_{i = 1}^n(y_i - \\alpha + \\beta x_i)\\) and \\(\\frac{\\partial h}{\\partial \\beta} = \\sum_{i = 1}^n 2 \\times (y_i - \\alpha - \\beta x_i) \\times (-x_i) = -2 \\sum_{i = 1}^n(y_i - \\alpha - \\beta x_i)x_i\\) and the two first-order conditions \\(-2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i) = 0\\) and \\(-2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i)x_i = 0\\) 8.4.3.1.1 The 1st First-Order Condition \\[\\begin{align} -2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i) &amp;= 0 \\\\ \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i) &amp;= 0 \\text{ (divide both sizes by $-2$)} \\\\ \\sum_{i = 1}^n y_i - \\sum_{i = 1}^n \\hat{\\alpha} - \\sum_{i = 1}^n \\hat{\\beta} x_i &amp;= 0 \\text{ (distribute the sum)} \\\\ \\sum_{i = 1}^n y_i - n \\hat{\\alpha} - \\hat{\\beta}\\sum_{i = 1}^n x_i &amp;= 0 \\text{ (move constant $\\beta$ in front and realize that $\\sum_{i = 1}^n \\hat{\\alpha} = n\\hat{\\alpha}$)} \\\\ \\sum_{i = 1}^n y_i &amp; = n \\hat{\\alpha} + \\hat{\\beta}\\sum_{i = 1}^n x_i \\text{ (rearrange)} \\\\ \\frac{\\sum_{i = 1}^n y_i}{n} &amp; = \\hat{\\alpha} + \\hat{\\beta} \\frac{\\sum_{i = 1}^n x_i}{n} \\text{ (divide both sides by $n$)} \\\\ \\overline{y} &amp; = \\hat{\\alpha} + \\hat{\\beta} \\overline{x} \\text{ (recognize the average of $y$ and of $x$)} \\\\ \\end{align}\\] Theorem 8.1 The 1st first-order condition implies that the regression line \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) equals \\(\\overline{y}\\) when \\(x = \\overline{x}\\). Thus, the regression line must go through the point \\((\\overline{x}, \\overline{y})\\) or “the point of averages”. The figure below shows a regression line that goes through the point of averages. Theorem 8.2 We can rearrange the identity \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) from Theorem 8.1 to obtain the identity \\(\\hat{\\alpha} = \\overline{y} - \\hat{\\beta}\\overline{x}\\) 8.4.3.1.2 The 2nd First-Order Condition Sometimes, when writing proofs, you obtain a result that’s not particularly interesting, but true and useful later. We refer to these results as “lemmas.” We’ll need the following Lemmas in the subsequent steps. Lemma 8.1 \\(\\sum_{i = i}^n y_i = n\\overline{y}\\). Exercise 8.5 Prove Lemma 8.1. A personal perspective on proofs: In my experience, proofs are not intuitive. Sometimes I have a sneaking suspicion about a result, but sometimes that sneaking suspicion is wildly wrong. When I investigate a the suspicion analytically, the path to the result is unclear. A maze analogy works quite well. To obtain the result, just move things around, sometimes getting further from the result. Eventually, you just happen upon the correct sequence of movements. Lemma 8.2 \\(\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y} = \\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\). Exercise 8.6 Prove Lemma 8.2. Lemma 8.3 \\(\\sum_{i = 1}^n x_i ^2 - n \\overline{x}^2 = \\sum_{i = 1}^n (x_i - \\overline{x})^2\\). Exercise 8.7 Prove Lemma 8.3. \\[\\begin{align} -2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)x_i &amp;= 0 \\\\ \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)x_i &amp;= 0 \\text{ (divide both sides by -2)} \\\\ \\sum_{i = 1}^n(y_i x_i - \\hat{\\alpha}x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (distribute the $x_i$)} \\end{align}\\] Now we can use use Theorem 8.2 and replace \\(\\hat{\\alpha}\\) with \\(\\overline{y} - \\hat{\\beta}\\overline{x}\\). \\[\\begin{align} \\sum_{i = 1}^n(y_i x_i - (\\overline{y} - \\hat{\\beta}\\overline{x})x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (use the identity $\\hat{\\alpha} = \\overline{y} - \\hat{\\beta}\\overline{x}$)} \\\\ \\sum_{i = 1}^n( x_i y_i - \\overline{y} x_i + \\hat{\\beta}\\overline{x} x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (expand the middle term)} \\\\ \\sum_{i = 1}^n x_i y_i - \\overline{y} \\sum_{i = 1}^n x_i + \\hat{\\beta}\\overline{x} \\sum_{i = 1}^n x_i - \\hat{\\beta} \\sum_{i = 1}^n x_i^2 &amp;= 0 \\text{ (distribute the sum)} \\end{align}\\] Now we can use Lemma 8.1 to replace \\(\\sum_{i = i}^n y_i\\) with \\(n\\overline{y}\\). \\[\\begin{align} \\sum_{i = 1}^n x_i y_i - n \\overline{y} \\overline{x} + \\hat{\\beta}n \\overline{x}^2 - \\hat{\\beta} \\sum_{i = 1}^n x_i^2 &amp;= 0 \\text{ (use the identity $\\sum_{i = i}^n y_i = n\\overline{y}$)}\\\\ \\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y} &amp;= \\hat{\\beta} \\left(\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2 \\right) \\text{ (rearrange)}\\\\ \\hat{\\beta} &amp;=\\dfrac{\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2} \\text{ (rearrange)}\\\\ \\end{align}\\] This final result is correct, but unfamiliar. In order to make sense of this solution, we need to connect this identity to previous results. Now we can use Lemmas 8.2 and 8.3 to replace the numerator \\(\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}\\) with the more familiar expression \\(\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\) and replace the denominator \\(\\sum_{i = 1}^n x_i ^2 - n \\overline{x}^2\\) with the more familiar expression \\(\\sum_{i = 1}^n (x_i - \\overline{x})^2\\). \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2} \\\\ \\end{align}\\] Denote the SD of \\(x\\) as \\(\\text{SD}_x\\) and the the SD of \\(y\\) as \\(\\text{SD}_y\\). Multiply the top and bottom by \\(\\frac{1}{n \\times \\text{SD}_x^2 \\times \\text{SD}_y}\\) and rearrange strategically. \\[\\begin{align} \\hat{\\beta} &amp;=\\frac{\\frac{\\sum_{i = 1}^n \\left(\\frac{x_i - \\overline{x}}{\\text{SD}_x} \\right)\\left(\\frac{y_i - \\overline{y}}{\\text{SD}_y} \\right)}{n} \\times \\frac{1}{\\text{SD}_x}}{ \\frac{1}{n \\times \\text{SD}_x^2 \\times \\text{SD}_y} \\times \\sum_{i = 1}^n (x_i - \\overline{x})^2} \\\\ \\end{align}\\] Now we recognize that the left term \\(\\dfrac{\\sum_{i = 1}^n \\left(\\frac{x_i - \\overline{x}}{\\text{SD}_x} \\right)\\left(\\frac{y_i - \\overline{y}}{\\text{SD}_y} \\right)}{n}\\) in the numerator is simply the correlation coefficient \\(r\\) between \\(x\\) and \\(y\\). \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{r \\times \\frac{1}{\\text{SD}_x}}{\\frac{1}{\\text{SD}_x^2 \\times \\text{SD}_y}\\sum_{i = 1}^n \\frac{(x_i - \\overline{x})^2}{n}} \\\\ \\end{align}\\] Now we recognize that \\(\\sum_{i = 1}^n \\frac{(x_i - \\overline{x})^2}{n}\\) is almost the \\(\\text{SD}_x\\). Conveniently, it’s \\(\\text{SD}_x^2\\), which allows us to cancel those two terms. \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{r \\times \\frac{1}{\\text{SD}_x}}{\\frac{1}{\\text{SD}_y}} \\\\ &amp; r \\times \\frac{\\text{SD}_y}{\\text{SD}_x} \\end{align}\\] This final result clearly connects \\(\\hat{\\beta}\\) to previous results. Theorem 8.3 \\(\\hat{\\beta} = r \\times \\dfrac{\\text{SD of } y}{\\text{SD of }x} = \\dfrac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2} = \\dfrac{\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2}\\). In summary, we can obtain the smallest RMS of the residuals with results from Theorems 8.2 and 8.3. \\[\\begin{align} \\hat{\\beta} &amp;= r \\times \\dfrac{\\text{SD of } y}{\\text{SD of }x} \\\\ \\hat{\\alpha} &amp;= \\overline{y} - \\hat{\\beta}\\overline{x} \\end{align}\\] Exercise 8.8 Other authors develop this least-squares approach using slightly different language and notation. Read FPP, ch. 12. Set A, p. 207: 1, 2, 3, 4. Set B, p. 210: 1, 2. Section 4, p. 213: 1-5, 8. Read Fox (2008), pp. 77-86. Do p. 96, q. 5.2. Read Wooldridge (2013), pp. 27-35. Do pp. 60-61, q. 3 (i, ii, and iii) and q. 6. Read DeGroot and Schervish (2012), pp. 689-692. Exercise 8.9 [HARD] Do Fox (2008), p. 97, q. 5.4. 8.4.3.2 Matrix Form In some cases, a matrix approach might help analytically or numerically compared to the scalar approach. Rather than writing the model as \\(y_i = \\alpha + \\beta x_i + r_i\\), we can write the model in an equivalent matrix form \\[\\begin{align} y &amp;= X\\beta + r \\\\ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_1\\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix} \\times \\begin{bmatrix} \\beta_1 \\\\ \\beta_2\\\\ \\end{bmatrix} + \\begin{bmatrix} r_1\\\\ r_2\\\\ \\vdots\\\\ r_n \\end{bmatrix} . \\end{align}\\] In this case, our intercept \\(\\alpha\\) and slope \\(\\beta\\) are combined into a single vector \\(\\beta = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2\\\\ \\end{bmatrix}\\), where \\(\\beta_1\\) represents the intercept and \\(\\beta_2\\) represents the slope. Exercise 8.10 Show that the scalar representation \\(y_i = \\alpha + \\beta x_i + r_i\\) and the matrix formulation \\(y = X\\beta + r\\) are equivalent. Hint Do the matrix multiplication \\(X\\beta\\) and show that \\(y_i = \\beta_1 + \\beta_2x + r_i\\). Theorem 8.4 In matrix form we can combine Theorems 8.2 and 8.3 and compute the slope and intercept as \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). Proof See Fox (2008), pp. 192-193 8.5 The RMS of the Residuals Just like the SD offers a give-or-take number around the average, the RMS of the residuals offers a give-or-take number around the regression line. Indeed, the SD is the RMS of the deviations from the average and the RMS of the residuals is the RMS of the deviations from the regression line. The RMS of the residuals tells us how far typical points fall from the regression line. Sometimes the RMS of the residuals is called the “RMS error (of the regression),” the “standard error of the regression,” or denoted as \\(\\hat{\\sigma}\\). We can compute the RMS of the regression by computing each residual and then taking the root-mean-square. But we can also use the much simpler formula \\(\\sqrt{1 - r^2} \\times \\text{ SD of }y\\). This formula makes sense because \\(y\\) has an SD, but \\(x\\) explains some of that variation. As \\(r\\) increases, \\(x\\) explains more and more of the variation. As \\(x\\) explains more variation, then the RMS of the residuals shrinks away from SD of \\(y\\) toward zero. It turns out that the SD of \\(y\\) shrinks toward zero by a factor of \\(\\sqrt{1 - r^2}\\). Exercise 8.11 Read FPP, ch. 11. Do the following exercises. Set A, p. 184: 1-4, 6, 7. Set B, p. 187: 1, 2. Set C, p. 189: 1-3. Set D, p. 193: 1, 2, 4-6. Set E, p. 197: 1, 2. Section 6, p. 198: 1, 2, 4, 6, 7,10, 12. 8.6 \\(R^2\\) Some authors use the quantity \\(R^2\\) to assess the fit of the regression model. I prefer the RMS of the residuals because it’s on the same scale as \\(y\\). Also, \\(R^2\\) computes the what fraction of the variance of \\(y\\), which is the SD squared, is explained by \\(x\\). I have a hard time making sense of variances, because they are not on the original scale. However, \\(R^2\\) is a common quantity, so do the following exercises. Exercise 8.12 Read Lewis-Beck (1980), pp. 20-25. Read Wooldridge (2013), pp. 36-39. Do pp. 60-61, q. 3 (part iv) 8.6.1 Adequacy of a Line In some cases, a line can describe the average value \\(y\\) quite well. In other cases, a line describes the data poorly. Remember, the regression line describes the average value of \\(y\\) for different values of \\(x\\). In the figure below, the left panel shows a dataset in which a line does not (and cannot) adequately describe the average values of \\(y\\) for describe low, middle, and high values (at least ad the same time). The right panel shows a data in which a line can adequately describe how the average value of \\(y\\) changes with \\(x\\). We can see that when \\(x \\approx -2\\), then \\(y \\approx 0\\). Similarly, when \\(x \\approx 0\\), then \\(y \\approx 4\\). A line can describe the average value of \\(y\\) for varying values of \\(x\\) when the average of \\(y\\) changes linearly with \\(x\\). A line does a great job of describing the relationship between seat shares and portfolio shares in government coalitions. A line poorly describes the relationship between Polity IV’s DEMOC measure and GDP per capita. When we have variable that’s skewed heavily to the right, we can sometimes more easily describe the log of the variable. For this dataset, the line poorly describes the average logged GDP per capita for the various democracy scores. 8.7 Fitting Regression Models To fit a regression model in R, we can use the following approach: Use lm() to fit the model. Use coef(), arm::display(), texreg::screenreg(), or summary() to quickly inspect the slope and intercept. Use glance(), tidy(), and augment() functions in the broom package to process the fit more thoroughly. 8.7.1 geom_smooth() In the context of ggplot, we can show the fitted line with geom_smooth(). gamson &lt;- read_rds(&quot;data/gamson.rds&quot;) ggplot(gamson, aes(x = seat_share, y = portfolio_share)) + geom_point() + geom_smooth() By default, geom_smooth() fits a smoothed curve rather than a straight line. There’s nothing wrong with a smoothed curve—sometimes it’s preferable to a straight line. But we don’t understand how to fit a smoothed curve. To us the least-squares fit, we supply the argument method = \"lm\" to geom_smooth(). geom_smooth() also includes the uncertainty around the line by default. Notice the grey band around the line, especially in the top-right. We don’t have a clear since of how uncertainty enters the fit, nor do we understand a standard error, so we should not include the uncertainty in the plot (at least for now). To remove the grey band, we supply the argument se = FALSE to geom_smooth(). The line \\(y = x\\) is theoretically relevant–that’s the line that indicates a perfectly proportional portfolio distribution. To include it, we can use geom_abline(). ggplot(gamson, aes(x = seat_share, y = portfolio_share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;) 8.7.2 lm() The lm() function takes two key arguments. The first argument is a formula, which is a special type of object in R. It has a left-hand side and a right-hand side, separated by a ~. You put the name of the outcome variable \\(y\\) on the LHS and the name of the explanatory variable \\(x\\) on the RHS. The second argument is the dataset. fit &lt;- lm(portfolio_share ~ seat_share, data = gamson) 8.7.3 Quick Look at the Fit We have several ways to look at the fit. Experiment with coef(), arm::display(), texreg::screenreg(), and summary() to see the differences. For now, we only understand the slope and intercept, so coef() works perfectly. coef(fit) (Intercept) seat_share 0.06913558 0.79158398 The coef() function outputs a numeric vector with named entries. The intercept is named (Intercept) and the slope is named after its associated variable. 8.7.4 Post-Processing The intercept and slope have a nice, intuitive interpretation so it’s tempting to just examine those values and call it quits. That’s a mistake, and it’s an especially bad habit to carry into richer models. The broom package offers three useful tools to explore the fit in more detail. The core of broom contains three functions: glance(), tidy(), and augment(). broom works nicely in our usual workflow because it produces a data frame containing information about the fit. These functions work for a wide range of models, so you can use this workflow as models become richer. 8.7.4.1 glance() glance() produces a one-row data frame that contains summaries of the model fit. fit_summary &lt;- glance(fit) %&gt;% glimpse() Rows: 1 Columns: 12 $ r.squared &lt;dbl&gt; 0.8879625 $ adj.r.squared &lt;dbl&gt; 0.8878265 $ sigma &lt;dbl&gt; 0.06889308 $ statistic &lt;dbl&gt; 6530.681 $ p.value &lt;dbl&gt; 0 $ df &lt;dbl&gt; 1 $ logLik &lt;dbl&gt; 1038.673 $ AIC &lt;dbl&gt; -2071.346 $ BIC &lt;dbl&gt; -2057.196 $ deviance &lt;dbl&gt; 3.910916 $ df.residual &lt;int&gt; 824 $ nobs &lt;int&gt; 826 We don’t understand many of these (yet, see ?glance.lm for the definitions), so let’s select() only those we understand. fit_summary &lt;- glance(fit) %&gt;% select(r.squared, sigma) %&gt;% # this used to have nobs, but not any more glimpse() Rows: 1 Columns: 2 $ r.squared &lt;dbl&gt; 0.8879625 $ sigma &lt;dbl&gt; 0.06889308 r.squared is the \\(R^2\\) statistic. sigma is the RMS of the residuals. nobs is the number of observations (see note above). 8.7.4.2 tidy() tidy() produces a usually several-row data frame that contains summaries of the “components” of the model. The “components” are usually the main focus of the model. In the case of an lm() fit, it’s the intercept and slope(s). fit_components &lt;- tidy(fit) %&gt;% glimpse() Rows: 2 Columns: 5 $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;seat_share&quot; $ estimate &lt;dbl&gt; 0.06913558, 0.79158398 $ std.error &lt;dbl&gt; 0.004037815, 0.009795300 $ statistic &lt;dbl&gt; 17.12203, 80.81263 $ p.value &lt;dbl&gt; 1.865252e-56, 0.000000e+00 You can see that tidy gives us one row per coefficient (one for intercept and one for slope). Again, there are several columns we don’t understand (yet, see ?tidy.lm for the definitions), so let’s select() the rows we know. fit_components &lt;- tidy(fit) %&gt;% select(term, estimate) %&gt;% glimpse() Rows: 2 Columns: 2 $ term &lt;chr&gt; &quot;(Intercept)&quot;, &quot;seat_share&quot; $ estimate &lt;dbl&gt; 0.06913558, 0.79158398 term contains the name of the coefficient. estimate contains the estimate for that coefficient. This gives us the same information as coef(fit), except in a data frame rather than a named vector. The data frame is more convenient for computing and plotting. For example, we can plot the coefficients. ggplot(fit_components, aes(x = estimate, y = term)) + geom_point() You can see that this sort of plot might be useful for more complicated models. We might have similar models that we want to compare, or the same model fit to different subsets of data. 8.7.4.3 augment() Lastly, augment() creates a data frame with information about each observation. obs_fit &lt;- augment(fit) %&gt;% glimpse() Rows: 826 Columns: 8 $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ .fitted &lt;dbl&gt; 0.0883255, 0.4337440, 0.4769213, 0.4428026, 0.4870526… $ .resid &lt;dbl&gt; 0.002583595, -0.070107594, 0.068533303, 0.011742918, … $ .std.resid &lt;dbl&gt; 0.03756019, -1.01841671, 0.99571954, 0.17058860, 0.84… $ .hat &lt;dbl&gt; 0.003121865, 0.001546470, 0.001890853, 0.001608751, 0… $ .sigma &lt;dbl&gt; 0.06893487, 0.06889153, 0.06889344, 0.06893371, 0.068… $ .cooksd &lt;dbl&gt; 2.209010e-06, 8.032205e-04, 9.391259e-04, 2.344542e-0… Again, there are several columns we don’t understand (yet, see ?augment.lm for the definitions), so let’s select() the rows we know. obs_fit &lt;- augment(fit) %&gt;% select(portfolio_share:.resid) %&gt;% glimpse() Rows: 826 Columns: 4 $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ .fitted &lt;dbl&gt; 0.0883255, 0.4337440, 0.4769213, 0.4428026, 0.4870526… $ .resid &lt;dbl&gt; 0.002583595, -0.070107594, 0.068533303, 0.011742918, … The couple of variables are the \\(x\\) and \\(y\\) from the original dataset used to fit the model. .fitted is the fitted or predicted value that we denote as \\(\\hat{y}\\) .resid is the residual or the difference between the predicted and observed value. Now we have an important data set for assessing the fit of the model. Does it describe the data well? Poorly? We can use augment()ed dataset to create a plot of the fitted/predicted values versus the residuals. ggplot(obs_fit, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;) Exercise 8.13 Using the democracy-life dataset from here, fit the regression model \\(\\text{GDP per captia} = \\alpha + \\beta \\times \\text{DEMOC}\\). Use glance(), tidy(), and augment() to obtain data frames with the information of the fit. Create a fitted versus residual plot and use to to make a judgement about the fit of the model to the data. Review FPP, pp. 187-192 for more about plotting the fitted values verus the residuals. Solution dem_life &lt;- read_csv(&quot;https://raw.githubusercontent.com/pos5737/democracy-life/master/data/democracy-life.csv&quot;) %&gt;% glimpse() Rows: 151 Columns: 8 $ year &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016,… $ country_name &lt;chr&gt; &quot;United States&quot;, &quot;Canada&quot;, &quot;Cuba&quot;, &quot;Dominican Republi… $ wb &lt;chr&gt; &quot;USA&quot;, &quot;CAN&quot;, &quot;CUB&quot;, &quot;DOM&quot;, &quot;JAM&quot;, &quot;TTO&quot;, &quot;MEX&quot;, &quot;GTM… $ cown &lt;dbl&gt; 2, 20, 40, 42, 51, 52, 70, 90, 91, 92, 93, 94, 95, 10… $ life_expectancy &lt;dbl&gt; 78.53902, 82.14224, 78.60700, 73.47100, 74.17500, 73.… $ gdp_per_capita &lt;dbl&gt; 52534.365, 50263.834, 6550.274, 7084.627, 4753.705, 1… $ democ &lt;dbl&gt; 8, 10, 0, 8, 9, 10, 8, 9, 7, 8, 7, 10, 9, 7, 8, 6, 6,… $ autoc &lt;dbl&gt; 0, 0, 7, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,… fit &lt;- lm(gdp_per_capita ~ democ, data = dem_life) glance(fit) # A tibble: 1 x 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.0946 0.0886 18826. 15.6 1.22e-4 1 -1700. 3405. 3414. # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; tidy(fit) # A tibble: 2 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 3869. 2984. 1.30 0.197 2 democ 1659. 420. 3.95 0.000122 obs_df &lt;- augment(fit) %&gt;% glimpse() Rows: 151 Columns: 8 $ gdp_per_capita &lt;dbl&gt; 52534.365, 50263.834, 6550.274, 7084.627, 4753.705, 15… $ democ &lt;dbl&gt; 8, 10, 0, 8, 9, 10, 8, 9, 7, 8, 7, 10, 9, 7, 8, 6, 6, … $ .fitted &lt;dbl&gt; 17138.532, 20455.803, 3869.446, 17138.532, 18797.167, … $ .resid &lt;dbl&gt; 35395.834, 29808.031, 2680.829, -10053.905, -14043.462… $ .std.resid &lt;dbl&gt; 1.8881716, 1.5947603, 0.1442260, -0.5363201, -0.750048… $ .hat &lt;dbl&gt; 0.008435309, 0.014230460, 0.025121117, 0.008435309, 0.… $ .sigma &lt;dbl&gt; 18661.80, 18727.25, 18887.83, 18870.91, 18853.46, 1888… $ .cooksd &lt;dbl&gt; 0.0151646668, 0.0183571132, 0.0002680066, 0.0012234835… ggplot(obs_df, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;) 8.8 Standard Errors and p-Values Almost all software reports p-values or standard errors by default. At this point, you should ignore these. In order to use these, you must be able to: Define a p-value. It’s the probability of…? Define a sampling distribution. Describe how randomness noise makes it’s way into your fitted coefficients. We’ll take a lot of care with (1) and (2). For most applications, it isn’t at all immediately clear how randomness enters the data (if at all). That said, regression is a powerful tool for description. Use it often. 8.9 A Warning When we simply describe a single set of measurements with a histogram or an average, then we intuitively remain in the world of description. Indeed, making a causal claim requires a comparing factual and counterfactual scenarios. In the case of a single histogram or average we only have one scenario and there is no comparison. When we have descriptions of multiple sets of measurements, say the average life expectancy for democracies and the average life expectancy for autocracies, we an easily interpret one scenario as factual and the other as counterfactual. On its face, though, both scenarios are factual. We can comfortably say that democracies have healthier populations than autocracies without claiming that the regime type causes this difference. But it is… oh. so. tempting. Regression models, by design, describe an outcome across a range of scenarios. Indeed, a regression model describes how the average value of \\(y\\) changes as \\(x\\) varies. The temptation to treat these neatly arranged scenarios as factual and counterfactual grows even stronger. But unless one makes a strong argument otherwise, statistical models describe the factual world. With few exceptions, statistical data analysis describes the outcomes of real social processes and not the processes themselves. It is therefore important to attend to the descriptive accuracy of statistical models, and to refrain from reifying them. —Fox (2008, p.3) Note that some methodologists claim that their statistical models can obtain estimates of the causal effects. These models might actually succeed on occasion. However, the researcher should carefully avoid seeing counterfactual worlds from regression models. Usually, credible causal inferences come from careful design in the data collection stage, not from complicated conditioning at the modeling stage. Exercise 8.14 Read Berk’s \" What You Can and Can’t Properly Do with Regression\" [pdf]. This is an easy reading and nicely wraps up our discussion of description and previews our future discussion of random sampling. 8.10 Review Exercises Exercise 8.15 Use devtools::install_github(\"pos5737/pos5737data\") to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737data\"). Use glimpse(anscombe) to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. Fit a regression model on each “dataset” in the anscombe dataset. To only use a subset of the dataset, you can filter() the dataset before supplying the data to lm() or use can supply the subset argument to lm(). In this case, just supplying subset = dataset == \"I\", for example, is probably easiest. Fit the regression to all four datasets and put the intercept, slope, RMS of the residuals, and number of observations for each regression in a little table. Interpret the results. For each of the four regression fits, create a scatterplot of the fitted values versus the residuals. Describe any inadequacies. Exercise 8.16 Use regression to test Clark and Golder’s (2006) theory. First, create scatterplots between ENEG and ENEP faceted by the electoral system with with the least-squares fit included in each. Then fit three separate regression models. Fit the model \\(\\text{ENEP}_i = \\alpha + \\beta \\text{ENEG}_i + r_i\\) for SMD systems, small-magnitude PR systems, and large-magnitude PR systems. Include the intercept, slope, and RMS of the residuals from each fit in a little table. Explain the results. For each regression, assess the fit using a scatterplot of the fitted values versus the residuals. Explain any inadequacies. Exercise 8.17 Use a regression model (fit in R) to assess the question in Exercise 8.1. Briefly discuss the strengths and weaknesses of each approach. Histogram Average and/or SD Scatterplot Correlation Coefficient Regression What’s the best approach (or combination of approaches)? Exercise 8.18 This continues Exercise 8.1. Get the economic-model CSV dataset from GitHub. In three separate regressions, use GDP, RDI, and unemployment to explain the incumbent’s margin of victory. Which measure of economic performance best describes incumbents’ vote shares? Using the best model of the three, which incumbents did much better than the model suggests? Which incumbents did much worse? Use tables and figures wisely to answer the questions above. "],
["references.html", "References", " References "]
]
